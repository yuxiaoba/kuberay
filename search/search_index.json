{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":""},{"location":"#kuberay","title":"KubeRay","text":"<p>KubeRay is a powerful, open-source Kubernetes operator that simplifies the deployment and management of Ray applications on Kubernetes. It offers several key components:</p> <p>KubeRay core: This is the official, fully-maintained component of KubeRay that provides three custom resource definitions, RayCluster, RayJob, and RayService. These resources are designed to help you run a wide range of workloads with ease.</p> <ul> <li> <p>RayCluster: KubeRay fully manages the lifecycle of RayCluster, including cluster creation/deletion, autoscaling, and ensuring fault tolerance.</p> </li> <li> <p>RayJob: With RayJob, KubeRay automatically creates a RayCluster and submits a job when the cluster is ready. You can also configure RayJob to automatically delete the RayCluster once the job finishes.</p> </li> <li> <p>RayService: RayService is made up of two parts: a RayCluster and a Ray Serve deployment graph. RayService offers zero-downtime upgrades for RayCluster and high availability.</p> </li> </ul> <p>Community-managed components (optional): Some components are maintained by the KubeRay community.</p> <ul> <li> <p>KubeRay APIServer: It provides a layer of simplified configuration for KubeRay resources. The KubeRay API server is used internally by some organizations to back user interfaces for KubeRay resource management.</p> </li> <li> <p>KubeRay Python client: This Python client library provides APIs to handle RayCluster from your Python application.</p> </li> <li> <p>KubeRay CLI: KubeRay CLI provides the ability to manage KubeRay resources through command-line interface.</p> </li> </ul>"},{"location":"#kuberay-ecosystem","title":"KubeRay ecosystem","text":"<ul> <li>AWS Application Load Balancer</li> <li>Nginx</li> <li>Prometheus and Grafana </li> <li>Volcano</li> <li>MCAD</li> <li>Kubeflow</li> </ul>"},{"location":"#security","title":"Security","text":"<p>If you discover a potential security issue in this project, or think you may have discovered a security issue, we ask that you notify KubeRay Security via our Slack Channel. Please do not create a public GitHub issue.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the Apache-2.0 License.</p>"},{"location":"#the-ray-docs","title":"The Ray docs","text":"<p>You can find even more information on deployments of Ray on Kubernetes at the official Ray docs.</p>"},{"location":"troubleshooting/","title":"Troubleshooting handbook","text":""},{"location":"troubleshooting/#introduction","title":"Introduction","text":"<p>This page will give you some guild on troubleshooting for some situations when you deploy and use the kuberay.</p>"},{"location":"troubleshooting/#ray-version-compatibility","title":"Ray Version Compatibility","text":""},{"location":"troubleshooting/#problem","title":"Problem","text":"<p>For every running ray cluster, when we try to connect with the client, we must be careful about the python and ray version we used. There are several  issues report failures related to version imcompatibility: #148, #21549. Therefore there is a reminder for troubleshooting when come up with that situation.</p>"},{"location":"troubleshooting/#error-cases","title":"Error cases","text":"<p>In the ray client initialization, there are several checks will be executed in ray-project/ray/util/client/__init__.py:L115-L137.</p> <p>Common cases would be like:</p> <pre><code>...\n\nRuntimeError: Python minor versions differ between client and server: client is 3.8.10, server is 3.7.7\n</code></pre> <p>or:</p> <pre><code>...\nRuntimeError: Client Ray installation incompatible with server: client is 2021-05-20, server is 2021-12-07\n</code></pre> <p>Some cases may not be so clear:</p> <pre><code>ConnectionAbortedError: Initialization failure from server:\nTraceback (most recent call last):\n...\n'AttributeError: ''JobConfig'' object has no attribute ''_parsed_runtime_env''\n</code></pre> <pre><code>Traceback (most recent call last):\n...\nRuntimeError: Version mismatch: The cluster was started with:\n    Ray: 1.9.0\n    Python: 3.7.7\nThis process on node NODE_ADDRESS was started with:\n    Ray: 1.10.0\n    Python: 3.7.7\n</code></pre>"},{"location":"troubleshooting/#solution","title":"Solution","text":"<p>In above cases, you will need to check if the client ray version is compatible with the images version in the ray cluster's configuration. </p> <p>For example, when you deployed <code>kuberay/ray-operator/config/samples/ray-cluster.mini.yaml</code>, you need to be aware that <code>spec.rayVersion</code> and images version is the same with your expect ray release and same with your ray client version.</p> <p>NOTE:</p> <p>In ray code, the version check will only go through major and minor version, so the python and ray image's minor version match is enough. Also the ray upstream community provide different python version support from 3.6 to 3.9, you can choose the image to match your python version.</p>"},{"location":"best-practice/worker-head-reconnection/","title":"Explanation and Best Practice for workers-head Reconnection","text":""},{"location":"best-practice/worker-head-reconnection/#problem","title":"Problem","text":"<p>For a <code>RayCluster</code> with a head and several workers, if a worker is crashed, it will be relaunched immediately and re-join the same cluster quickly; however, when the head is crashed, it will run into the issue #104 that all worker nodes are lost from the head for a long period of time. </p>"},{"location":"best-practice/worker-head-reconnection/#explanation","title":"Explanation","text":"<p>note It was an issue that only happened with old version In the Kuberay version under 0.3.0, we recommand you try the latest version  </p> <p>When the head pod was deleted, it will be recreated with a new IP by KubeRay controller\uff0cand the GCS server address is changed accordingly. The Raylets of all workers will try to get GCS address from Redis in <code>ReconnectGcsServer</code>, but the redis_clients always use the previous head IP, so they will always fail to get new GCS address. The Raylets will not exit until max retries are reached. There are two configurations determining this long delay:</p> <pre><code>/// The interval at which the gcs rpc client will check if gcs rpc server is ready.\nRAY_CONFIG(int64_t, ping_gcs_rpc_server_interval_milliseconds, 1000)\n\n/// Maximum number of times to retry ping gcs rpc server when gcs server restarts.\nRAY_CONFIG(int32_t, ping_gcs_rpc_server_max_retries, 600)\n\nhttps://github.com/ray-project/ray/blob/98be9fb5e08befbd6cac3ffbcaa477c5117b0eef/src/ray/gcs/gcs_client/gcs_client.cc#L294-L295\n</code></pre> <p>It retries 600 times and each interval is 1s, resulting in total 600s timeout, i.e. 10 min. So immediately after 10-min wait for retries, each client exits and gets restarted while connecting to the new head IP. This issue exists in stable ray versions under 1.9.1. This has been reduced to 60s in recent commit under Kuberay 0.3.0.</p>"},{"location":"best-practice/worker-head-reconnection/#solution","title":"Solution","text":"<p>We recommend using the latest version of KubeRay. After version 0.5.0, the GCS Fault-Tolerance feature is now in beta and can help resolve this reconnection issue.</p>"},{"location":"best-practice/worker-head-reconnection/#best-practice","title":"Best Practice","text":"<p>For older version (Kuberay &lt;=0.4.0, ray &lt;=2.1.0). To reduce the chances of a lost worker-head connection, there are two other options:</p> <ul> <li> <p>Make head more stable: when creating the cluster, allocate sufficient amount of resources on head pod such that it tends to be stable and not easy to crash. You can also set {\"num-cpus\": \"0\"} in \"rayStartParams\" of \"headGroupSpec\" such that Ray scheduler will skip the head node when scheduling workloads. This also helps to maintain the stability of the head. </p> </li> <li> <p>Make reconnection shorter: for version &lt;= 1.9.1, you can set this head param --system-config='{\"ping_gcs_rpc_server_max_retries\": 20}' to reduce the delay from 600s down to 20s before workers reconnect to the new head. </p> </li> </ul>"},{"location":"components/apiserver/","title":"KubeRay APIServer","text":"<p>The KubeRay APIServer provides gRPC and HTTP APIs to manage KubeRay resources.</p> <p>Note</p> <pre><code>The KubeRay APIServer is an optional component. It provides a layer of simplified\nconfiguration for KubeRay resources. The KubeRay API server is used internally\nby some organizations to back user interfaces for KubeRay resource management.\n\nThe KubeRay APIServer is community-managed and is not officially endorsed by the\nRay maintainers. At this time, the only officially supported methods for\nmanaging KubeRay resources are\n\n- Direct management of KubeRay custom resources via kubectl, kustomize, and Kubernetes language clients.\n- Helm charts.\n\nKubeRay APIServer maintainer contacts (GitHub handles):\n@Jeffwan @scarlet25151\n</code></pre>"},{"location":"components/apiserver/#installation","title":"Installation","text":""},{"location":"components/apiserver/#helm","title":"Helm","text":"<p>Make sure the version of Helm is v3+. Currently, existing CI tests are based on Helm v3.4.1 and v3.9.4.</p> <pre><code>helm version\n</code></pre>"},{"location":"components/apiserver/#install-kuberay-apiserver","title":"Install KubeRay APIServer","text":"<ul> <li> <p>Install a stable version via Helm repository (only supports KubeRay v0.4.0+)   <pre><code>helm repo add kuberay https://ray-project.github.io/kuberay-helm/\n\n# Install KubeRay APIServer v0.5.0.\nhelm install kuberay-apiserver kuberay/kuberay-apiserver --version 0.5.0\n\n# Check the KubeRay APIServer Pod in `default` namespace\nkubectl get pods\n# NAME                                 READY   STATUS    RESTARTS   AGE\n# kuberay-apiserver-67b46b88bf-m7dzg   1/1     Running   0          6s\n</code></pre></p> </li> <li> <p>Install the nightly version   <pre><code># Step1: Clone KubeRay repository\n\n# Step2: Move to `helm-chart/kuberay-apiserver`\n\n# Step3: Install KubeRay APIServer\nhelm install kuberay-apiserver .\n</code></pre></p> </li> </ul>"},{"location":"components/apiserver/#list-the-chart","title":"List the chart","text":"<p>To list the <code>my-release</code> deployment:</p> <pre><code>helm ls\n# NAME                    NAMESPACE       REVISION        UPDATED                                 STATUS  CHART                    APP VERSION\n# kuberay-apiserver       default         1               2023-02-07 09:28:15.510869781 -0500 EST deployedkuberay-apiserver-0.5.0\n</code></pre>"},{"location":"components/apiserver/#uninstall-the-chart","title":"Uninstall the Chart","text":"<pre><code># Uninstall the `kuberay-apiserver` release\nhelm uninstall kuberay-apiserver\n\n# The KubeRay APIServer Pod should be removed.\nkubectl get pods\n# No resources found in default namespace.\n</code></pre>"},{"location":"components/apiserver/#usage","title":"Usage","text":"<p>After the deployment we may use the <code>{{baseUrl}}</code> to access the</p> <ul> <li> <p>(default) for nodeport access, we provide the default http port <code>31888</code> for connection and you can connect it using.</p> </li> <li> <p>for ingress access, you will need to create your own ingress</p> </li> </ul> <p>The requests parameters detail can be seen in KubeRay swagger, here we only present some basic example:</p>"},{"location":"components/apiserver/#setup-end-to-end-test","title":"Setup end-to-end test","text":"<ol> <li>(Optional) You may use your local kind cluster or minikube</li> </ol> <pre><code>cat &lt;&lt;EOF | kind create cluster --name ray-test --config -\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n  - role: control-plane\n    kubeadmConfigPatches:\n      - |\n        kind: InitConfiguration\n        nodeRegistration:\n          kubeletExtraArgs:\n            node-labels: \"ingress-ready=true\"\n    extraPortMappings:\n      - containerPort: 30379\n        hostPort: 6379\n        listenAddress: \"0.0.0.0\"\n        protocol: tcp\n      - containerPort: 30265\n        hostPort: 8265\n        listenAddress: \"0.0.0.0\"\n        protocol: tcp\n      - containerPort: 30001\n        hostPort: 10001\n        listenAddress: \"0.0.0.0\"\n        protocol: tcp\n      - containerPort: 8000\n        hostPort: 8000\n        listenAddress: \"0.0.0.0\"\n      - containerPort: 31888\n        hostPort: 31888\n        listenAddress: \"0.0.0.0\"\n  - role: worker\n  - role: worker\nEOF\n</code></pre> <ol> <li>Deploy the KubeRay APIServer within the same cluster of KubeRay operator</li> </ol> <pre><code>helm repo add kuberay https://ray-project.github.io/kuberay-helm/\nhelm -n ray-system install kuberay-apiserver kuberay/kuberay-apiserver\n</code></pre> <ol> <li>The APIServer expose service using <code>NodePort</code> by default. You can test access by your host and port, the default port is set to <code>31888</code>.</li> </ol> <pre><code>curl localhost:31888\n{\"code\":5, \"message\":\"Not Found\"}\n</code></pre> <ol> <li>You can create <code>RayCluster</code>, <code>RayJobs</code> or <code>RayService</code> by dialing the endpoints. The following is a simple example for creating the <code>RayService</code> object, follow swagger support to get the complete definitions of APIs.</li> </ol> <p><pre><code>curl -X POST 'localhost:31888/apis/v1alpha2/namespaces/ray-system/compute_templates' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"name\": \"default-template\",\n  \"namespace\": \"ray-system\",\n  \"cpu\": 2,\n  \"memory\": 4\n}'\n\ncurl -X POST 'localhost:31888/apis/v1alpha2/namespaces/ray-system/services' \\\n--header 'Content-Type: application/json' \\\n--data '{\n  \"name\": \"user-test-1\",\n  \"namespace\": \"ray-system\",\n  \"user\": \"user\",\n  \"serveDeploymentGraphSpec\": {\n      \"importPath\": \"fruit.deployment_graph\",\n      \"runtimeEnv\": \"working_dir: \\\"https://github.com/ray-project/test_dag/archive/c620251044717ace0a4c19d766d43c5099af8a77.zip\\\"\\n\",\n      \"serveConfigs\": [\n      {\n        \"deploymentName\": \"OrangeStand\",\n        \"replicas\": 1,\n        \"userConfig\": \"price: 2\",\n        \"actorOptions\": {\n          \"cpusPerActor\": 0.1\n        }\n      },\n      {\n        \"deploymentName\": \"PearStand\",\n        \"replicas\": 1,\n        \"userConfig\": \"price: 1\",\n        \"actorOptions\": {\n          \"cpusPerActor\": 0.1\n        }\n      },\n      {\n        \"deploymentName\": \"FruitMarket\",\n        \"replicas\": 1,\n        \"actorOptions\": {\n          \"cpusPerActor\": 0.1\n        }\n      },{\n        \"deploymentName\": \"DAGDriver\",\n        \"replicas\": 1,\n        \"routePrefix\": \"/\",\n        \"actorOptions\": {\n          \"cpusPerActor\": 0.1\n        }\n      }]\n  },\n  \"clusterSpec\": {\n    \"headGroupSpec\": {\n      \"computeTemplate\": \"default-template\",\n      \"image\": \"rayproject/ray:2.5.0\",\n      \"serviceType\": \"NodePort\",\n      \"rayStartParams\": {\n            \"dashboard-host\": \"0.0.0.0\",\n            \"metrics-export-port\": \"8080\"\n        },\n       \"volumes\": []\n    },\n    \"workerGroupSpec\": [\n      {\n        \"groupName\": \"small-wg\",\n        \"computeTemplate\": \"default-template\",\n        \"image\": \"rayproject/ray:2.5.0\",\n        \"replicas\": 1,\n        \"minReplicas\": 0,\n        \"maxReplicas\": 5,\n        \"rayStartParams\": {\n                \"node-ip-address\": \"$MY_POD_IP\"\n            }\n      }\n    ]\n  }\n}'\n</code></pre> The Ray resource will then be created in your Kubernetes cluster.</p>"},{"location":"components/apiserver/#full-definition-of-payload","title":"Full definition of payload","text":""},{"location":"components/apiserver/#compute-template","title":"Compute Template","text":"<p>For the purpose to simplify the setting of resource, we abstract the resource of the pods template resource to the <code>compute template</code> for usage, you can define the resource in the <code>compute template</code> and then choose the appropriate template for your <code>head</code> and <code>workergroup</code> when you are creating the real objects of <code>RayCluster</code>, <code>RayJobs</code> or <code>RayService</code>.</p>"},{"location":"components/apiserver/#create-compute-templates-in-a-given-namespace","title":"Create compute templates in a given namespace","text":"<pre><code>POST {{baseUrl}}/apis/v1alpha2/namespaces/&lt;namespace&gt;/compute_templates\n</code></pre> <pre><code>{\n\"name\": \"default-template\",\n\"namespace\": \"&lt;namespace&gt;\",\n\"cpu\": 2,\n\"memory\": 4,\n\"gpu\": 1,\n\"gpuAccelerator\": \"Tesla-V100\"\n}\n</code></pre>"},{"location":"components/apiserver/#list-all-compute-templates-in-a-given-namespace","title":"List all compute templates in a given namespace","text":"<pre><code>GET {{baseUrl}}/apis/v1alpha2/namespaces/&lt;namespace&gt;/compute_templates\n</code></pre> <pre><code>{\n\"compute_templates\": [\n{\n\"name\": \"default-template\",\n\"namespace\": \"&lt;namespace&gt;\",\n\"cpu\": 2,\n\"memory\": 4,\n\"gpu\": 1,\n\"gpu_accelerator\": \"Tesla-V100\"\n}\n]\n}\n</code></pre>"},{"location":"components/apiserver/#list-all-compute-templates-in-all-namespaces","title":"List all compute templates in all namespaces","text":"<pre><code>GET {{baseUrl}}/apis/v1alpha2/compute_templates\n</code></pre>"},{"location":"components/apiserver/#get-compute-template-by-name","title":"Get compute template by name","text":"<pre><code>GET {{baseUrl}}/apis/v1alpha2/namespaces/&lt;namespace&gt;/compute_templates/&lt;compute_template_name&gt;\n</code></pre>"},{"location":"components/apiserver/#delete-compute-template-by-name","title":"Delete compute template by name","text":"<pre><code>DELETE {{baseUrl}}/apis/v1alpha2/namespaces/&lt;namespace&gt;/compute_templates/&lt;compute_template_name&gt;\n</code></pre>"},{"location":"components/apiserver/#clusters","title":"Clusters","text":""},{"location":"components/apiserver/#create-cluster-in-a-given-namespace","title":"Create cluster in a given namespace","text":"<pre><code>POST {{baseUrl}}/apis/v1alpha2/namespaces/&lt;namespace&gt;/clusters\n</code></pre> <p>payload <pre><code>{\n\"name\": \"test-cluster\",\n\"namespace\": \"&lt;namespace&gt;\",\n\"user\": \"jiaxin.shan\",\n\"version\": \"1.9.2\",\n\"environment\": \"DEV\",\n\"clusterSpec\": {\n\"headGroupSpec\": {\n\"computeTemplate\": \"head-template\",\n\"image\": \"ray.io/ray:1.9.2\",\n\"serviceType\": \"NodePort\",\n\"rayStartParams\": {}\n},\n\"workerGroupSpec\": [\n{\n\"groupName\": \"small-wg\",\n\"computeTemplate\": \"worker-template\",\n\"image\": \"ray.io/ray:1.9.2\",\n\"replicas\": 2,\n\"minReplicas\": 0,\n\"maxReplicas\": 5,\n\"rayStartParams\": {}\n}\n]\n}\n}\n</code></pre></p>"},{"location":"components/apiserver/#list-all-clusters-in-a-given-namespace","title":"List all clusters in a given namespace","text":"<pre><code>GET {{baseUrl}}/apis/v1alpha2/namespaces/&lt;namespace&gt;/clusters\n</code></pre> <pre><code>{\n\"clusters\": [\n{\n\"name\": \"test-cluster\",\n\"namespace\": \"&lt;namespace&gt;\",\n\"user\": \"jiaxin.shan\",\n\"version\": \"1.9.2\",\n\"environment\": \"DEV\",\n\"cluster_spec\": {\n\"head_group_spec\": {\n\"compute_template\": \"head-template\",\n\"image\": \"rayproject/ray:1.9.2\",\n\"service_type\": \"NodePort\",\n\"ray_start_params\": {\n\"dashboard-host\": \"0.0.0.0\",\n\"node-ip-address\": \"$MY_POD_IP\",\n\"port\": \"6379\"\n}\n},\n\"worker_group_spec\": [\n{\n\"group_name\": \"small-wg\",\n\"compute_template\": \"worker-template\",\n\"image\": \"rayproject/ray:1.9.2\",\n\"replicas\": 2,\n\"min_replicas\": 0,\n\"max_replicas\": 5,\n\"ray_start_params\": {\n\"node-ip-address\": \"$MY_POD_IP\",\n}\n}\n]\n},\n\"created_at\": \"2022-03-13T15:13:09Z\",\n\"deleted_at\": null\n},\n]\n}\n</code></pre>"},{"location":"components/apiserver/#list-all-clusters-in-all-namespaces","title":"List all clusters in all namespaces","text":"<pre><code>GET {{baseUrl}}/apis/v1alpha2/clusters\n</code></pre>"},{"location":"components/apiserver/#get-cluster-by-its-name-and-namespace","title":"Get cluster by its name and namespace","text":"<pre><code>GET {{baseUrl}}/apis/v1alpha2/namespaces/&lt;namespace&gt;/clusters/&lt;cluster_name&gt;\n</code></pre>"},{"location":"components/apiserver/#delete-cluster-by-its-name-and-namespace","title":"Delete cluster by its name and namespace","text":"<pre><code>DELETE {{baseUrl}}/apis/v1alpha2/namespaces/&lt;namespace&gt;/clusters/&lt;cluster_name&gt;\n</code></pre>"},{"location":"components/apiserver/#rayjob","title":"RayJob","text":""},{"location":"components/apiserver/#create-ray-job-in-a-given-namespace","title":"Create ray job in a given namespace","text":"<pre><code>POST {{baseUrl}}/apis/v1alpha2/namespaces/&lt;namespace&gt;/jobs\n</code></pre> <p>payload <pre><code>{\n\"name\": \"string\",\n\"namespace\": \"string\",\n\"user\": \"string\",\n\"entrypoint\": \"string\",\n\"metadata\": {\n\"additionalProp1\": \"string\",\n\"additionalProp2\": \"string\",\n\"additionalProp3\": \"string\"\n},\n\"runtimeEnv\": \"string\",\n\"jobId\": \"string\",\n\"shutdownAfterJobFinishes\": true,\n\"clusterSelector\": {\n\"additionalProp1\": \"string\",\n\"additionalProp2\": \"string\",\n\"additionalProp3\": \"string\"\n},\n\"clusterSpec\": {\n\"headGroupSpec\": {\n\"computeTemplate\": \"string\",\n\"image\": \"string\",\n\"serviceType\": \"string\",\n\"rayStartParams\": {\n\"additionalProp1\": \"string\",\n\"additionalProp2\": \"string\",\n\"additionalProp3\": \"string\"\n},\n\"volumes\": [\n{\n\"mountPath\": \"string\",\n\"volumeType\": \"PERSISTENT_VOLUME_CLAIM\",\n\"name\": \"string\",\n\"source\": \"string\",\n\"readOnly\": true,\n\"hostPathType\": \"DIRECTORY\",\n\"mountPropagationMode\": \"NONE\"\n}\n]\n},\n\"workerGroupSpec\": [\n{\n\"groupName\": \"string\",\n\"computeTemplate\": \"string\",\n\"image\": \"string\",\n\"replicas\": 0,\n\"minReplicas\": 0,\n\"maxReplicas\": 0,\n\"rayStartParams\": {\n\"additionalProp1\": \"string\",\n\"additionalProp2\": \"string\",\n\"additionalProp3\": \"string\"\n},\n\"volumes\": [\n{\n\"mountPath\": \"string\",\n\"volumeType\": \"PERSISTENT_VOLUME_CLAIM\",\n\"name\": \"string\",\n\"source\": \"string\",\n\"readOnly\": true,\n\"hostPathType\": \"DIRECTORY\",\n\"mountPropagationMode\": \"NONE\"\n}\n]\n}\n]\n},\n\"ttlSecondsAfterFinished\": 0,\n\"createdAt\": \"2022-08-19T21:20:30.494Z\",\n\"deleteAt\": \"2022-08-19T21:20:30.494Z\",\n\"jobStatus\": \"string\",\n\"jobDeploymentStatus\": \"string\",\n\"message\": \"string\"\n}\n</code></pre></p>"},{"location":"components/apiserver/#list-all-jobs-in-a-given-namespace","title":"List all jobs in a given namespace","text":"<pre><code>GET {{baseUrl}}/apis/v1alpha2/namespaces/&lt;namespace&gt;/jobs\n</code></pre> <p>Response <pre><code>{\n\"jobs\": [\n{\n\"name\": \"string\",\n\"namespace\": \"string\",\n\"user\": \"string\",\n\"entrypoint\": \"string\",\n\"metadata\": {\n\"additionalProp1\": \"string\",\n\"additionalProp2\": \"string\",\n\"additionalProp3\": \"string\"\n},\n\"runtimeEnv\": \"string\",\n\"jobId\": \"string\",\n\"shutdownAfterJobFinishes\": true,\n\"clusterSelector\": {\n\"additionalProp1\": \"string\",\n\"additionalProp2\": \"string\",\n\"additionalProp3\": \"string\"\n},\n\"clusterSpec\": {\n\"headGroupSpec\": {\n\"computeTemplate\": \"string\",\n\"image\": \"string\",\n\"serviceType\": \"string\",\n\"rayStartParams\": {\n\"additionalProp1\": \"string\",\n\"additionalProp2\": \"string\",\n\"additionalProp3\": \"string\"\n},\n\"volumes\": [\n{\n\"mountPath\": \"string\",\n\"volumeType\": \"PERSISTENT_VOLUME_CLAIM\",\n\"name\": \"string\",\n\"source\": \"string\",\n\"readOnly\": true,\n\"hostPathType\": \"DIRECTORY\",\n\"mountPropagationMode\": \"NONE\"\n}\n]\n},\n\"workerGroupSpec\": [\n{\n\"groupName\": \"string\",\n\"computeTemplate\": \"string\",\n\"image\": \"string\",\n\"replicas\": 0,\n\"minReplicas\": 0,\n\"maxReplicas\": 0,\n\"rayStartParams\": {\n\"additionalProp1\": \"string\",\n\"additionalProp2\": \"string\",\n\"additionalProp3\": \"string\"\n},\n\"volumes\": [\n{\n\"mountPath\": \"string\",\n\"volumeType\": \"PERSISTENT_VOLUME_CLAIM\",\n\"name\": \"string\",\n\"source\": \"string\",\n\"readOnly\": true,\n\"hostPathType\": \"DIRECTORY\",\n\"mountPropagationMode\": \"NONE\"\n}\n]\n}\n]\n},\n\"ttlSecondsAfterFinished\": 0,\n\"createdAt\": \"2022-08-19T21:31:24.352Z\",\n\"deleteAt\": \"2022-08-19T21:31:24.352Z\",\n\"jobStatus\": \"string\",\n\"jobDeploymentStatus\": \"string\",\n\"message\": \"string\"\n}\n]\n}\n</code></pre></p>"},{"location":"components/apiserver/#list-all-jobs-in-all-namespaces","title":"List all jobs in all namespaces","text":"<pre><code>GET {{baseUrl}}/apis/v1alpha2/jobs\n</code></pre>"},{"location":"components/apiserver/#get-job-by-its-name-and-namespace","title":"Get job by its name and namespace","text":"<pre><code>GET {{baseUrl}}/apis/v1alpha2/namespaces/&lt;namespace&gt;/jobs/&lt;job_name&gt;\n</code></pre>"},{"location":"components/apiserver/#delete-job-by-its-name-and-namespace","title":"Delete job by its name and namespace","text":"<pre><code>DELETE {{baseUrl}}/apis/v1alpha2/namespaces/&lt;namespace&gt;/jobs/&lt;job_name&gt;\n</code></pre>"},{"location":"components/apiserver/#rayservice","title":"RayService","text":""},{"location":"components/apiserver/#create-ray-service-in-a-given-namespace","title":"Create ray service in a given namespace","text":"<pre><code>POST {{baseUrl}}/apis/v1alpha2/namespaces/&lt;namespace&gt;/services\n</code></pre> <p>payload</p> <pre><code>{\n\"name\": \"string\",\n\"namespace\": \"string\",\n\"user\": \"string\",\n\"serveDeploymentGraphSpec\": {\n\"importPath\": \"string\",\n\"runtimeEnv\": \"string\",\n\"serveConfigs\": [\n{\n\"deploymentName\": \"string\",\n\"replicas\": 0,\n\"routePrefix\": \"string\",\n\"maxConcurrentQueries\": 0,\n\"userConfig\": \"string\",\n\"autoscalingConfig\": \"string\",\n\"actorOptions\": {\n\"runtimeEnv\": \"string\",\n\"cpus\": 0,\n\"gpu\": 0,\n\"memory\": 0,\n\"objectStoreMemory\": 0,\n\"resource\": \"string\",\n\"accceleratorType\": \"string\"\n}\n}\n]\n},\n\"clusterSpec\": {\n\"headGroupSpec\": {\n\"computeTemplate\": \"string\",\n\"image\": \"string\",\n\"serviceType\": \"string\",\n\"rayStartParams\": {\n\"additionalProp1\": \"string\",\n\"additionalProp2\": \"string\",\n\"additionalProp3\": \"string\"\n},\n\"volumes\": [\n{\n\"mountPath\": \"string\",\n\"volumeType\": \"PERSISTENT_VOLUME_CLAIM\",\n\"name\": \"string\",\n\"source\": \"string\",\n\"readOnly\": true,\n\"hostPathType\": \"DIRECTORY\",\n\"mountPropagationMode\": \"NONE\"\n}\n]\n},\n\"workerGroupSpec\": [\n{\n\"groupName\": \"string\",\n\"computeTemplate\": \"string\",\n\"image\": \"string\",\n\"replicas\": 0,\n\"minReplicas\": 0,\n\"maxReplicas\": 0,\n\"rayStartParams\": {\n\"additionalProp1\": \"string\",\n\"additionalProp2\": \"string\",\n\"additionalProp3\": \"string\"\n},\n\"volumes\": [\n{\n\"mountPath\": \"string\",\n\"volumeType\": \"PERSISTENT_VOLUME_CLAIM\",\n\"name\": \"string\",\n\"source\": \"string\",\n\"readOnly\": true,\n\"hostPathType\": \"DIRECTORY\",\n\"mountPropagationMode\": \"NONE\"\n}\n]\n}\n]\n},\n\"rayServiceStatus\": {\n\"applicationStatus\": \"string\",\n\"applicationMessage\": \"string\",\n\"serveDeploymentStatus\": [\n{\n\"deploymentName\": \"string\",\n\"status\": \"string\",\n\"message\": \"string\"\n}\n],\n\"rayServiceEvent\": [\n{\n\"id\": \"string\",\n\"name\": \"string\",\n\"createdAt\": \"2022-08-19T21:30:01.097Z\",\n\"firstTimestamp\": \"2022-08-19T21:30:01.097Z\",\n\"lastTimestamp\": \"2022-08-19T21:30:01.097Z\",\n\"reason\": \"string\",\n\"message\": \"string\",\n\"type\": \"string\",\n\"count\": 0\n}\n],\n\"rayClusterName\": \"string\",\n\"rayClusterState\": \"string\",\n\"serviceEndpoint\": {\n\"additionalProp1\": \"string\",\n\"additionalProp2\": \"string\",\n\"additionalProp3\": \"string\"\n}\n},\n\"createdAt\": \"2022-08-19T21:30:01.097Z\",\n\"deleteAt\": \"2022-08-19T21:30:01.097Z\"\n}\n</code></pre>"},{"location":"components/apiserver/#list-all-services-in-a-given-namespace","title":"List all services in a given namespace","text":"<pre><code>GET {{baseUrl}}/apis/v1alpha2/namespaces/&lt;namespace&gt;/services\n</code></pre> <p>Response <pre><code>  \"name\": \"string\",\n\"namespace\": \"string\",\n\"user\": \"string\",\n\"serveDeploymentGraphSpec\": {\n\"importPath\": \"string\",\n\"runtimeEnv\": \"string\",\n\"serveConfigs\": [\n{\n\"deploymentName\": \"string\",\n\"replicas\": 0,\n\"routePrefix\": \"string\",\n\"maxConcurrentQueries\": 0,\n\"userConfig\": \"string\",\n\"autoscalingConfig\": \"string\",\n\"actorOptions\": {\n\"runtimeEnv\": \"string\",\n\"cpus\": 0,\n\"gpu\": 0,\n\"memory\": 0,\n\"objectStoreMemory\": 0,\n\"resource\": \"string\",\n\"accceleratorType\": \"string\"\n}\n}\n]\n},\n\"clusterSpec\": {\n\"headGroupSpec\": {\n\"computeTemplate\": \"string\",\n\"image\": \"string\",\n\"serviceType\": \"string\",\n\"rayStartParams\": {\n\"additionalProp1\": \"string\",\n\"additionalProp2\": \"string\",\n\"additionalProp3\": \"string\"\n},\n\"volumes\": [\n{\n\"mountPath\": \"string\",\n\"volumeType\": \"PERSISTENT_VOLUME_CLAIM\",\n\"name\": \"string\",\n\"source\": \"string\",\n\"readOnly\": true,\n\"hostPathType\": \"DIRECTORY\",\n\"mountPropagationMode\": \"NONE\"\n}\n]\n},\n\"workerGroupSpec\": [\n{\n\"groupName\": \"string\",\n\"computeTemplate\": \"string\",\n\"image\": \"string\",\n\"replicas\": 0,\n\"minReplicas\": 0,\n\"maxReplicas\": 0,\n\"rayStartParams\": {\n\"additionalProp1\": \"string\",\n\"additionalProp2\": \"string\",\n\"additionalProp3\": \"string\"\n},\n\"volumes\": [\n{\n\"mountPath\": \"string\",\n\"volumeType\": \"PERSISTENT_VOLUME_CLAIM\",\n\"name\": \"string\",\n\"source\": \"string\",\n\"readOnly\": true,\n\"hostPathType\": \"DIRECTORY\",\n\"mountPropagationMode\": \"NONE\"\n}\n]\n}\n]\n},\n\"rayServiceStatus\": {\n\"applicationStatus\": \"string\",\n\"applicationMessage\": \"string\",\n\"serveDeploymentStatus\": [\n{\n\"deploymentName\": \"string\",\n\"status\": \"string\",\n\"message\": \"string\"\n}\n],\n\"rayServiceEvent\": [\n{\n\"id\": \"string\",\n\"name\": \"string\",\n\"createdAt\": \"2022-08-19T21:33:15.485Z\",\n\"firstTimestamp\": \"2022-08-19T21:33:15.485Z\",\n\"lastTimestamp\": \"2022-08-19T21:33:15.485Z\",\n\"reason\": \"string\",\n\"message\": \"string\",\n\"type\": \"string\",\n\"count\": 0\n}\n],\n\"rayClusterName\": \"string\",\n\"rayClusterState\": \"string\",\n\"serviceEndpoint\": {\n\"additionalProp1\": \"string\",\n\"additionalProp2\": \"string\",\n\"additionalProp3\": \"string\"\n}\n},\n\"createdAt\": \"2022-08-19T21:33:15.485Z\",\n\"deleteAt\": \"2022-08-19T21:33:15.485Z\"\n}\n</code></pre></p>"},{"location":"components/apiserver/#list-all-services-in-all-namespaces","title":"List all services in all namespaces","text":"<pre><code>GET {{baseUrl}}/apis/v1alpha2/services\n</code></pre>"},{"location":"components/apiserver/#get-service-by-its-name-and-namespace","title":"Get service by its name and namespace","text":"<pre><code>GET {{baseUrl}}/apis/v1alpha2/namespaces/&lt;namespace&gt;/services/&lt;service_name&gt;\n</code></pre>"},{"location":"components/apiserver/#delete-service-by-its-name-and-namespace","title":"Delete service by its name and namespace","text":"<pre><code>DELETE {{baseUrl}}/apis/v1alpha2/namespaces/&lt;namespace&gt;/services/&lt;service_name&gt;\n</code></pre>"},{"location":"components/apiserver/#swagger-support","title":"Swagger Support","text":"<ol> <li>Download Swagger UI from Swagger-UI. In this case, we use <code>swagger-ui-3.51.2.tar.gz</code></li> <li>Unzip package and copy <code>dist</code> folder to <code>third_party</code> folder</li> <li>Use <code>go-bindata</code> to generate go code from static files.</li> </ol> <pre><code>mkdir third_party\ntar -zvxf ~/Downloads/swagger-ui-3.51.2.tar.gz /tmp\nmv /tmp/swagger-ui-3.51.2/dist  third_party/swagger-ui\n\ncd apiserver/\ngo-bindata --nocompress --pkg swagger -o pkg/swagger/datafile.go ./third_party/swagger-ui/...\n</code></pre>"},{"location":"components/cli/","title":"KubeRay CLI","text":"<p>KubeRay CLI provides the ability to manage kuberay resources (ray clusters, compute templates etc) through command line interface.</p> <p>Note</p> <p>The KubeRay CLI is an optional interface backed by the KubeRay API server. It provides a layer of simplified configuration for KubeRay resources.</p> <p>The KubeRay CLI is community-managed and is not officially endorsed by the Ray maintainers. At this time, the only officially supported methods for managing KubeRay resources are</p> <ul> <li>Direct management of KubeRay custom resources via kubectl, kustomize, and Kubernetes language clients.</li> <li>Helm charts.</li> </ul> <p>KubeRay CLI maintainer contacts (GitHub handles): @Jeffwan @scarlet25151</p>"},{"location":"components/cli/#installation","title":"Installation","text":"<p>Please check release page and download the binaries. </p>"},{"location":"components/cli/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kuberay operator needs to be running.</li> <li>Kuberay apiserver needs to be running and accessible.</li> </ul>"},{"location":"components/cli/#development","title":"Development","text":"<ul> <li>Kuberay CLI uses Cobra framework for the CLI application.</li> <li>Kuberay CLI depends on kuberay apiserver to manage these resources by sending grpc requests to the kuberay apiserver.</li> </ul> <p>You can build kuberay binary following this way.</p> <pre><code>cd kuberay/cli\ngo build -o kuberay -a main.go\n</code></pre>"},{"location":"components/cli/#usage","title":"Usage","text":""},{"location":"components/cli/#configure-kuberay-apiserver-endpoint","title":"Configure kuberay apiserver endpoint","text":"<ul> <li>Default kuberay apiserver endpoint: <code>127.0.0.1:8887</code>.</li> <li>If kuberay apiserver is not run locally, this must be set in order to manage ray clusters and ray compute templates.</li> </ul>"},{"location":"components/cli/#read-current-kuberay-apiserver-endpoint","title":"Read current kuberay apiserver endpoint","text":"<p><code>./kuberay config get endpoint</code></p>"},{"location":"components/cli/#reset-kuberay-apiserver-endpoint-to-default-1270018887","title":"Reset kuberay apiserver endpoint to default (<code>127.0.0.1:8887</code>)","text":"<p><code>./kuberay config reset endpoint</code></p>"},{"location":"components/cli/#set-kuberay-apiserver-endpoint","title":"Set kuberay apiserver endpoint","text":"<p><code>./kuberay config set endpoint &lt;kuberay apiserver endpoint&gt;</code></p>"},{"location":"components/cli/#manage-ray-clusters","title":"Manage Ray Clusters","text":""},{"location":"components/cli/#create-a-ray-cluster","title":"Create a Ray Cluster","text":"<pre><code>Usage:\nkuberay cluster create [flags]\n\nFlags:\n      --environment string               environment of the cluster (valid values: DEV, TESTING, STAGING, PRODUCTION) (default \"DEV\")\n      --head-compute-template string     compute template name for ray head\n      --head-image string                ray head image\n      --head-service-type string         ray head service type (ClusterIP, NodePort, LoadBalancer) (default \"ClusterIP\")\n      --name string                      name of the cluster\n  -n, --namespace string                 kubernetes namespace where the cluster will be\n      --user string                      SSO username of ray cluster creator\n      --version string                   version of the ray cluster (default \"1.9.0\")\n      --worker-compute-template string   compute template name of worker in the first worker group\n      --worker-group-name string         first worker group name\n      --worker-image string              image of worker in the first worker group\n      --worker-replicas uint32           pod replicas of workers in the first worker group (default 1)\n</code></pre> <p>Known Limitation: Currently only one worker compute template is supported during creation. </p>"},{"location":"components/cli/#get-a-ray-cluster","title":"Get a Ray Cluster","text":"<p><code>./kuberay cluster get -n &lt;namespace&gt; &lt;cluster name&gt;</code></p>"},{"location":"components/cli/#list-ray-clusters","title":"List Ray Clusters","text":"<p><code>./kuberay cluster -n &lt;namespace&gt; list</code></p>"},{"location":"components/cli/#delete-a-ray-cluster","title":"Delete a Ray Cluster","text":"<p><code>./kuberay cluster delete -n &lt;namespace&gt; &lt;cluster name&gt;</code></p>"},{"location":"components/cli/#manage-ray-compute-template","title":"Manage Ray Compute Template","text":""},{"location":"components/cli/#create-a-compute-template","title":"Create a Compute Template","text":"<pre><code>Usage:\n  kuberay template compute create [flags]\n\nFlags:\n      --cpu uint32               ray pod CPU (default 1)\n      --gpu uint32               ray head GPU\n      --gpu-accelerator string   GPU Accelerator type\n      --memory uint32            ray pod memory in GB (default 1)\n      --name string              name of the compute template\n  -n, --namespace string         kubernetes namespace where the compute template will be stored\n</code></pre>"},{"location":"components/cli/#get-a-ray-compute-template","title":"Get a Ray Compute Template","text":"<p><code>./kuberay template compute get -n &lt;namespace&gt; &lt;compute template name&gt;</code></p>"},{"location":"components/cli/#list-ray-compute-templates","title":"List Ray Compute Templates","text":"<p><code>./kuberay template compute list -n &lt;namespace&gt;</code></p>"},{"location":"components/cli/#delete-a-ray-compute-template","title":"Delete a Ray Compute Template","text":"<p><code>./kuberay template compute delete -n &lt;namespace&gt; &lt;compute template name&gt;</code></p>"},{"location":"components/cli/#end-to-end-example","title":"End to end example","text":"<p>Configure the endpoints</p> <pre><code>kubectl port-forward svc/kuberay-apiserver-service 8887:8887 -n ray-system\n./kuberay config set endpoint 127.0.0.1:8887\n</code></pre> <p>Create compute templates</p> <pre><code>./kuberay template compute create -n &lt;namespace&gt; --cpu 2 --memory 4 --name \"worker-template\"\n./kuberay template compute create -n &lt;namespace&gt; --cpu 1 --memory 2 --name \"head-template\"\n</code></pre> <p>List compute templates created</p> <pre><code>./kuberay template compute list\n</code></pre> <p>Create the cluster</p> <pre><code>./kuberay cluster create -n &lt;namespace&gt; --name test-cluster --user jiaxin.shan \\\n--head-compute-template head-template \\\n--head-image rayproject/ray:1.9.2 \\\n--worker-group-name small-wg \\\n--worker-compute-template worker-template \\\n--worker-image rayproject/ray:1.9.2\n</code></pre> <p>List the clusters</p> <pre><code>./kuberay cluster list\n</code></pre>"},{"location":"components/operator/","title":"Ray Kubernetes Operator","text":"<p>The KubeRay Operator makes deploying and managing Ray clusters on top of Kubernetes painless. Clusters are defined as a custom RayCluster resource and managed by a fault-tolerant Ray controller. The KubeRay Operator automates Ray cluster lifecycle management, autoscaling, and other critical functions.</p> <p></p> <p>Below are some of the main features of the KubeRay operator:</p> <ul> <li>Management of first-class RayClusters via a custom resource.</li> <li>Support for heterogenous worker types in a single Ray cluster.</li> <li>Optional Ray Autoscaler integration; autoscaling based on Ray application semantics.</li> <li>Use of Kubernetes <code>PodTemplates</code> to configure Ray pods.</li> <li>Use of <code>ScaleStrategy</code> to remove specific Ray worker pods.</li> <li>Automatated management of critical configuration, such as required <code>environment variables</code>, the <code>ray start</code> entrypoint, and a <code>dev/shm</code> volume mount for Ray's shared memory.</li> <li>Built-in monitoring via Prometheus.</li> <li>Each <code>RayCluster</code>'s Status is updated based on the state of running Ray pods.</li> <li>Kubernetes Events concerning <code>RayCluster</code> instances are emitted to aid observability.</li> </ul>"},{"location":"components/operator/#overview","title":"Overview","text":"<p>When deployed, the KubeRay Operator will watch for K8s events (Create/Delete/Update) for <code>RayCluster</code> resources. The KubeRay Operator can create a Ray cluster (Ray head pod + multiple Ray worker pods), delete a Ray cluster, or update the Ray cluster by adding or removing worker pods.</p>"},{"location":"components/operator/#ray-cluster-creation","title":"Ray cluster creation","text":"<p>Once a <code>RayCluster</code> resource is created, the operator will configure and create the Ray head pod and the Ray worker pods specified in the <code>raycluster</code> manifest as shown below.</p> <p></p>"},{"location":"components/operator/#ray-cluster-update","title":"Ray cluster update","text":"<p>You can update the number of replicas in a worker group, and specify which exact replica to remove by updating the RayCluster resource manifest:</p> <p></p> <p>Note</p> <p>While updating <code>replicas</code> and <code>workersToDeleteUpdate</code> is supported, updating other fields in RayCluster manifests is not supported. In particular, updating Ray head pod and Ray worker pod configuration is not supported. To update pod configuration, delete the RayCluster, edit its configuration and then re-create the cluster. In other words, use <code>kubectl delete</code> and <code>kubectl create</code> to update a RayCluster's pod configuration, rather than <code>kubectl apply</code>. Support for in-place updates of pod configuration is tracked in KubeRay issue #527.</p>"},{"location":"components/operator/#deploy-the-operator","title":"Deploy the operator","text":"<pre><code>kubectl create -k \"github.com/ray-project/kuberay/ray-operator/config/default?ref=v0.5.0&amp;timeout=90s\"\n</code></pre> <p>Check that the controller is running. <pre><code>$ kubectl get deployments -n ray-system\nNAME           READY   UP-TO-DATE   AVAILABLE   AGE\nray-operator   1/1     1            1           40s\n\n$ kubectl get pods -n ray-system\nNAME                            READY   STATUS    RESTARTS   AGE\nray-operator-75dbbf8587-5lrvn   1/1     Running   0          31s\n</code></pre></p> <p>Delete the operator. <pre><code>kubectl delete -k \"github.com/ray-project/kuberay/ray-operator/config/default\"\n</code></pre></p>"},{"location":"components/operator/#running-an-example-cluster","title":"Running an example cluster","text":"<p>We include a few example config files to deploy RayClusters:</p> Sample Description ray-cluster.mini.yaml Small example consisting of 1 head pod. ray-cluster.heterogeneous.yaml Example with heterogenous worker types. 1 head pod and 2 worker pods, each of which has a different resource quota. ray-cluster.complete.yaml Shows all available custom resource properties. ray-cluster.autoscaler.yaml Shows all available custom resource properties and demonstrates autoscaling. ray-cluster.complete.large.yaml Demonstrates resource configuration for production use-cases. ray-cluster.autoscaler.large.yaml Demonstrates resource configuration for autoscaling Ray clusters in production. <p>Note</p> <p>For production use-cases, make sure to allocate sufficient resources for your Ray pods; it usually makes sense to run one large Ray pod per Kubernetes node. We do not recommend allocating less than 8Gb memory for a Ray pod running in production. Always set limits for memory and CPU. When possible, set requests equal to limits. See the Ray documentation for further guidance. See ray-cluster.complete.large.yaml and ray-cluster.autoscaler.large.yaml for examples of RayCluster resource configurations suitable for production. The rest of the sample configs above are meant only for experimentation in local kind or minikube environments.</p> <p>The memory usage of the KubeRay Operator depends on the number of pods and Ray clusters being managed. Anecdotally, managing 500 Ray pods requires roughly 500MB memory. Monitor memory usage and adjust requests and limits as needed.</p> <p>We recommend running the following example in a kind or minikube environment with a resource capacity of at least 4CPU and 4Gb memory. Run the following commands from the root of your cloned kuberay repo. <pre><code># Clone the kuberay repo if you haven't already.\n$ git clone https://github.com/ray-project/kuberay\n# Enter the root of the repo\n$ cd kuberay/\n# If you haven't already done so, deploy the KubeRay operator.\n$ kubectl create -k ray-operator/config/default\n# Create a RayCluster and a ConfigMap with hello world Ray code.\n$ kubectl create -f ray-operator/config/samples/ray-cluster.heterogeneous.yaml\nconfigmap/ray-code created\nraycluster.ray.io/raycluster-heterogeneous created\n\n# List running clusters.\n$ kubectl get rayclusters\nNAME                AGE\nraycluster-heterogeneous   2m48s\n\n# The created cluster should include a head pod, worker pod, and a head service.\n# It may take a few minutes for the pods to enter Running status.\n# If you're on minikube or kind, a Pending status indicates that your local Kubernetes environment\n# may not have sufficient CPU or memory capacity -- try adjusting your Docker settings.\n$ kubectl get pods\nNAME                                                 READY   STATUS    RESTARTS   AGE\nraycluster-heterogeneous-head-9t28q                  1/1     Running   0          97s\nraycluster-heterogeneous-worker-medium-group-l9x9n   1/1     Running   0          97s\nraycluster-heterogeneous-worker-small-group-hldxz    1/1     Running   0          97s\nraycluster-heterogeneous-worker-small-group-tmgtq    1/1     Running   0          97s\nraycluster-heterogeneous-worker-small-group-zc5dh    1/1     Running   0          97s\n</code></pre></p> <pre><code>$ kubectl get services\nNAME                                TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                       AGE\nkubernetes                          ClusterIP   10.96.0.1      &lt;none&gt;        443/TCP                       22h\nraycluster-heterogeneous-head-svc   ClusterIP   10.96.47.129   &lt;none&gt;        6379/TCP,8265/TCP,10001/TCP   2m18s\n</code></pre> <pre><code># Check the logs of the head pod. (Substitute the name of your head pod in this step.)\n$ kubectl logs raycluster-heterogeneous-head-9t28q\n2022-09-21 13:21:57,505 INFO usage_lib.py:479 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected to be non-interactive. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.\n2022-09-21 13:21:57,505 INFO scripts.py:719 -- Local node IP: 10.244.0.144\n2022-09-21 13:22:00,513 SUCC scripts.py:756 -- --------------------\n2022-09-21 13:22:00,514 SUCC scripts.py:757 -- Ray runtime started.\n2022-09-21 13:22:00,514 SUCC scripts.py:758 -- --------------------\n2022-09-21 13:22:00,514 INFO scripts.py:760 -- Next steps\n2022-09-21 13:22:00,514 INFO scripts.py:761 -- To connect to this Ray runtime from another node, run\n2022-09-21 13:22:00,514 INFO scripts.py:766 --   ray start --address='10.244.0.144:6379'\n2022-09-21 13:22:00,514 INFO scripts.py:780 -- Alternatively, use the following Python code:\n2022-09-21 13:22:00,514 INFO scripts.py:782 -- import ray\n2022-09-21 13:22:00,514 INFO scripts.py:795 -- ray.init(address='auto', _node_ip_address='10.244.0.144')\n2022-09-21 13:22:00,515 INFO scripts.py:799 -- To connect to this Ray runtime from outside of the cluster, for example to\n2022-09-21 13:22:00,515 INFO scripts.py:803 -- connect to a remote cluster from your laptop directly, use the following\n2022-09-21 13:22:00,515 INFO scripts.py:806 -- Python code:\n2022-09-21 13:22:00,515 INFO scripts.py:808 -- import ray\n2022-09-21 13:22:00,515 INFO scripts.py:814 -- ray.init(address='ray://&lt;head_node_ip_address&gt;:10001')\n2022-09-21 13:22:00,515 INFO scripts.py:820 -- If connection fails, check your firewall settings and network configuration.\n2022-09-21 13:22:00,515 INFO scripts.py:826 -- To terminate the Ray runtime, run\n2022-09-21 13:22:00,515 INFO scripts.py:827 --   ray stop\n2022-09-21 13:22:00,515 INFO scripts.py:905 -- --block\n2022-09-21 13:22:00,515 INFO scripts.py:907 -- This command will now block forever until terminated by a signal.\n2022-09-21 13:22:00,515 INFO scripts.py:910 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.\n</code></pre> <p>Now, we can run the hello world Ray code mounted from the config map created above. <pre><code># Substitute the name of your head pod in this step.\n$ kubectl exec raycluster-heterogeneous-head-9t28q -- python /opt/sample_code.py\n2022-09-21 13:28:41,176 INFO worker.py:1224 -- Using address 127.0.0.1:6379 set in the environment variable RAY_ADDRESS\n2022-09-21 13:28:41,176 INFO worker.py:1333 -- Connecting to existing Ray cluster at address: 10.244.0.144:6379...\n2022-09-21 13:28:41,183 INFO worker.py:1515 -- Connected to Ray cluster. View the dashboard at http://10.244.0.144:8265\ntrying to connect to Ray!\nnow executing some code with Ray!\nRay Nodes:  {'10.244.0.145', '10.244.0.143', '10.244.0.146', '10.244.0.144', '10.244.0.147'}\nExecution time =  4.855740308761597\n</code></pre></p> <p>The output of the hello world Ray code shows 5 nodes in the Ray cluster. <pre><code>Ray Nodes:  {'10.244.0.145', '10.244.0.143', '10.244.0.146', '10.244.0.144', '10.244.0.147'}\n</code></pre></p> <pre><code># Delete the cluster.\n$ kubectl delete raycluster raycluster-heterogeneous\n</code></pre>"},{"location":"deploy/docker/","title":"Docker Images","text":""},{"location":"deploy/docker/#docker-images","title":"Docker images","text":"<p>Find the Docker images for various KubeRay components on Dockerhub.</p>"},{"location":"deploy/docker/#stable-versions","title":"Stable versions","text":"<p>For stable releases, use version tags (e.g. <code>kuberay/operator:v0.5.0</code>).</p>"},{"location":"deploy/docker/#master-commits","title":"Master commits","text":"<p>The first seven characters of the git SHA specify images built from specific commits (e.g. <code>kuberay/operator:944a042</code>).</p>"},{"location":"deploy/docker/#nightly-images","title":"Nightly images","text":"<p>The nightly tag specifies images built from the most recent master (e.g. <code>kuberay/operator:nightly</code>).</p>"},{"location":"deploy/helm-cluster/","title":"RayCluster","text":"<p>RayCluster is a custom resource definition (CRD). KubeRay operator will listen to the resource events about RayCluster and create related Kubernetes resources (e.g. Pod &amp; Service). Hence, KubeRay operator installation and CRD registration are required for this guide.</p>"},{"location":"deploy/helm-cluster/#prerequisites","title":"Prerequisites","text":"<p>See kuberay-operator/README.md for more details. * Helm * Install custom resource definition and KubeRay operator (covered by the following end-to-end example.)</p>"},{"location":"deploy/helm-cluster/#end-to-end-example","title":"End-to-end example","text":"<pre><code># Step 1: Create a KinD cluster \nkind create cluster\n\n# Step 2: Register a Helm chart repo\nhelm repo add kuberay https://ray-project.github.io/kuberay-helm/\n\n# Step 3: Install both CRDs and KubeRay operator v0.5.0.\nhelm install kuberay-operator kuberay/kuberay-operator --version 0.5.0\n\n# Step 4: Install a RayCluster custom resource\n# (For x86_64 users)\nhelm install raycluster kuberay/ray-cluster --version 0.5.0\n# (For arm64 users, e.g. Mac M1)\n# See here for all available arm64 images: https://hub.docker.com/r/rayproject/ray/tags?page=1&amp;name=aarch64\nhelm install raycluster kuberay/ray-cluster --version 0.5.0 --set image.tag=nightly-aarch64\n\n# Step 5: Verify the installation of KubeRay operator and RayCluster \nkubectl get pods\n# NAME                                          READY   STATUS    RESTARTS   AGE\n# kuberay-operator-6fcbb94f64-gkpc9             1/1     Running   0          89s\n# raycluster-kuberay-head-qp9f4                 1/1     Running   0          66s\n# raycluster-kuberay-worker-workergroup-2jckt   1/1     Running   0          66s\n\n# Step 6: Forward the port of Dashboard\nkubectl port-forward --address 0.0.0.0 svc/raycluster-kuberay-head-svc 8265:8265\n\n# Step 7: Check ${YOUR_IP}:8265 for the Dashboard (e.g. 127.0.0.1:8265)\n\n# Step 8: Log in to Ray head Pod and execute a job.\nkubectl exec -it ${RAYCLUSTER_HEAD_POD} -- bash\npython -c \"import ray; ray.init(); print(ray.cluster_resources())\" # (in Ray head Pod)\n\n# Step 9: Check ${YOUR_IP}:8265/#/job. The status of the job should be \"SUCCEEDED\".\n\n# Step 10: Uninstall RayCluster\nhelm uninstall raycluster\n\n# Step 11: Verify that RayCluster has been removed successfully\n# NAME                                READY   STATUS    RESTARTS   AGE\n# kuberay-operator-6fcbb94f64-gkpc9   1/1     Running   0          9m57s\n</code></pre>"},{"location":"deploy/helm/","title":"KubeRay Operator","text":"<p>This document provides instructions to install both CRDs (RayCluster, RayJob, RayService) and KubeRay operator with a Helm chart.</p>"},{"location":"deploy/helm/#helm","title":"Helm","text":"<p>Make sure the version of Helm is v3+. Currently, existing CI tests are based on Helm v3.4.1 and v3.9.4.</p> <pre><code>helm version\n</code></pre>"},{"location":"deploy/helm/#install-crds-and-kuberay-operator","title":"Install CRDs and KubeRay operator","text":"<ul> <li> <p>Install a stable version via Helm repository (only supports KubeRay v0.4.0+)   <pre><code>helm repo add kuberay https://ray-project.github.io/kuberay-helm/\n\n# Install both CRDs and KubeRay operator v0.5.0.\nhelm install kuberay-operator kuberay/kuberay-operator --version 0.5.0\n\n# Check the KubeRay operator Pod in `default` namespace\nkubectl get pods\n# NAME                                READY   STATUS    RESTARTS   AGE\n# kuberay-operator-6fcbb94f64-mbfnr   1/1     Running   0          17s\n</code></pre></p> </li> <li> <p>Install the nightly version   <pre><code># Step1: Clone KubeRay repository\n\n# Step2: Move to `helm-chart/kuberay-operator`\n\n# Step3: Install KubeRay operator\nhelm install kuberay-operator .\n</code></pre></p> </li> <li> <p>Install KubeRay operator without installing CRDs</p> </li> <li>In some cases, the installation of the CRDs and the installation of the operator may require different levels of admin permissions, so these two installations could be handled as different steps by different roles.</li> <li>Use Helm's built-in <code>--skip-crds</code> flag to install the operator only. See this document for more details.   <pre><code># Step 1: Install CRDs only (for cluster admin)\nkubectl create -k \"github.com/ray-project/kuberay/manifests/cluster-scope-resources?ref=v0.5.0&amp;timeout=90s\"\n\n# Step 2: Install KubeRay operator only. (for developer)\nhelm install kuberay-operator kuberay/kuberay-operator --version 0.5.0 --skip-crds\n</code></pre></li> </ul>"},{"location":"deploy/helm/#list-the-chart","title":"List the chart","text":"<p>To list the <code>my-release</code> deployment:</p> <pre><code>helm ls\n# NAME                    NAMESPACE       REVISION        UPDATED                                   STATUS          CHART                   # APP VERSION\n# kuberay-operator        default         1               2022-12-02 02:13:37.514445313 +0000 UTC   deployed        kuberay-operator-0.5.0  1.0\n</code></pre>"},{"location":"deploy/helm/#uninstall-the-chart","title":"Uninstall the Chart","text":"<pre><code># Uninstall the `kuberay-operator` release\nhelm uninstall kuberay-operator\n\n# The operator Pod should be removed.\nkubectl get pods\n# No resources found in default namespace.\n</code></pre>"},{"location":"deploy/helm/#working-with-argo-cd","title":"Working with Argo CD","text":"<p>If you are using Argo CD to manage the operator, you will encounter the issue which complains the CRDs too long. Same with this issue. The recommended solution is to split the operator into two Argo apps, such as:</p> <ul> <li>The first app just for installing the CRDs with <code>Replace=true</code> directly, snippet:</li> </ul> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\nname: ray-operator-crds\nspec:\nproject: default\nsource:\nrepoURL: https://github.com/ray-project/kuberay\ntargetRevision: v0.5.0\npath: helm-chart/kuberay-operator/crds\ndestination:\nserver: https://kubernetes.default.svc\nsyncPolicy:\nsyncOptions:\n- Replace=true\n...\n</code></pre> <ul> <li>The second app that installs the Helm chart with <code>skipCrds=true</code> (new feature in Argo CD 2.3.0), snippet:</li> </ul> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\nname: ray-operator\nspec:\nsource:\nrepoURL: https://github.com/ray-project/kuberay\ntargetRevision: v0.5.0\npath: helm-chart/kuberay-operator\nhelm:\nskipCrds: true\ndestination:\nserver: https://kubernetes.default.svc\nnamespace: ray-operator\nsyncPolicy:\nsyncOptions:\n- CreateNamespace=true\n...\n</code></pre>"},{"location":"deploy/installation/","title":"YAML","text":""},{"location":"deploy/installation/#installation","title":"Installation","text":"<p>Make sure your Kubernetes cluster and Kubectl are both at version at least 1.19.</p>"},{"location":"deploy/installation/#nightly-version","title":"Nightly version","text":"<pre><code>export KUBERAY_VERSION=master\n\n# Install CRDs\nkubectl create -k \"github.com/ray-project/kuberay/manifests/cluster-scope-resources?ref=${KUBERAY_VERSION}&amp;timeout=90s\"\n\n# Install KubeRay operator\nkubectl apply -k \"github.com/ray-project/kuberay/manifests/base?ref=${KUBERAY_VERSION}&amp;timeout=90s\"\n</code></pre>"},{"location":"deploy/installation/#stable-version","title":"Stable version","text":""},{"location":"deploy/installation/#method-1-install-charts-from-helm-repository","title":"Method 1: Install charts from Helm repository","text":"<pre><code>helm repo add kuberay https://ray-project.github.io/kuberay-helm/\n\n# Install both CRDs and KubeRay operator\nhelm install kuberay-operator kuberay/kuberay-operator\n</code></pre>"},{"location":"deploy/installation/#method-2-kustomize","title":"Method 2: Kustomize","text":"<pre><code># Install CRDs\nkubectl create -k \"github.com/ray-project/kuberay/manifests/cluster-scope-resources?ref=v0.5.0&amp;timeout=90s\"\n\n# Install KubeRay operator\nkubectl apply -k \"github.com/ray-project/kuberay/manifests/base?ref=v0.5.0\"\n</code></pre> <p>Observe that we must use <code>kubectl create</code> to install cluster-scoped resources. The corresponding <code>kubectl apply</code> command will not work. See KubeRay issue #271.</p>"},{"location":"deploy/installation/#single-namespace-version","title":"Single Namespace version","text":"<p>Users can use the following commands to deploy KubeRay operator in a specific namespace.</p> <pre><code>export KUBERAY_NAMESPACE=&lt;my-awesome-namespace&gt;\n\n# Install CRDs (Executed by cluster admin)\nkustomize build \"github.com/ray-project/kuberay/manifests/overlays/single-namespace-resources?ref=v0.5.0\" | envsubst | kubectl create -f -\n\n# Install KubeRay operator (Executed by user)\nkustomize build \"github.com/ray-project/kuberay/manifests/overlays/single-namespace?ref=v0.5.0\" | envsubst | kubectl apply -f -\n</code></pre>"},{"location":"design/protobuf-grpc-service/","title":"Support proto Core API and RESTful backend services","text":""},{"location":"design/protobuf-grpc-service/#motivation","title":"Motivation","text":"<p>There're few major blockers for users to use KubeRay Operator directly.</p> <ul> <li> <p>Current ray operator is only friendly to users who is familiar with Kubernetes operator pattern. For most data scientists, there's still a learning curve.</p> </li> <li> <p>Using kubectl requires sophisticated permission system. Some kubernetes clusters do not enable user level authentication. In some companies, devops use loose RBAC management and corp SSO system is not integrated with Kubernetes OIDC at all.</p> </li> </ul> <p>For the above reasons, it's worth it to build a generic abstraction on top of the RayCluster CRD. With the core API support, we can easily build backend services, cli, etc to bridge users without Kubernetes experience to KubeRay.</p>"},{"location":"design/protobuf-grpc-service/#goals","title":"Goals","text":"<ul> <li>The API definition should be flexible enough to support different kinds of clients (e.g. backend, cli etc).</li> <li>This backend service underneath should leverage generate clients to interact with existing RayCluster custom resources.</li> <li>New added components should be plugable to existing operator.</li> </ul>"},{"location":"design/protobuf-grpc-service/#proposal","title":"Proposal","text":""},{"location":"design/protobuf-grpc-service/#deployment-topology-and-interactive-flow","title":"Deployment topology and interactive flow","text":"<p>The new gRPC service would be a individual deployment of the KubeRay control plane and user can choose to install it optionally. It will create a service and exposes endpoint to users.</p> <pre><code>NAME                                                      READY   STATUS    RESTARTS      AGE\nkuberay-grpc-service-c8db9dc65-d4w5r                      1/1     Running   0             2d15h\nkuberay-operator-785476b948-fmlm7                         1/1     Running   0             3d\n</code></pre> <p>In issue #29, <code>RayCluster</code> CRD clientset has been generated and gRPC service can leverage it to operate Custom Resources.</p> <p>A simple flow would be like this. (Thanks @akanso for providing the flow) <pre><code>client --&gt; GRPC Server --&gt; [created Custom Resources] &lt;-- Ray Operator (reads CR and accordingly performs CRUD)\n</code></pre></p>"},{"location":"design/protobuf-grpc-service/#api-abstraction","title":"API abstraction","text":"<p>Protocol Buffers are a language-neutral, platform-neutral extensible mechanism for serializing structured data. Protoc also provides different community plugins to meet different needs.</p> <p>In order to better define resources at the API level, a few proto files will be defined. Technically, we can use similar data structure like <code>RayCluster</code> Kubernetes resource but this is probably not a good idea.</p> <ul> <li>Some of the Kubernetes API like <code>tolerance</code> and <code>node affinity</code> are too complicated to be converted to an API.</li> <li>We want to leave some flexibility to use database to store history data in the near future (for example, pagination, list options etc).</li> </ul> <p>To resolve these issues, we provide a simple API which can cover most common use-cases. </p> <p>For example, the protobuf definition of the <code>RayCluster</code>:</p> <pre><code>service ClusterService {\n// Creates a new Cluster.\nrpc CreateCluster(CreateClusterRequest) returns (Cluster) {\noption (google.api.http) = {\npost: \"/apis/v1alpha2/namespaces/{namespace}/clusters\"\nbody: \"cluster\"\n};\n}\n\n// Finds a specific Cluster by ID.\nrpc GetCluster(GetClusterRequest) returns (Cluster) {\noption (google.api.http) = {\nget: \"/apis/v1alpha2/namespaces/{namespace}/clusters/{name}\"\n};\n}\n\n// Finds all Clusters in a given namespace. Supports pagination, and sorting on certain fields.\nrpc ListCluster(ListClustersRequest) returns (ListClustersResponse) {\noption (google.api.http) = {\nget: \"/apis/v1alpha2/namespaces/{namespace}/clusters\"\n};\n}\n\n// Finds all Clusters in all namespaces. Supports pagination, and sorting on certain fields.\nrpc ListAllClusters(ListAllClustersRequest) returns (ListAllClustersResponse) {\noption (google.api.http) = {\nget: \"/apis/v1alpha2/clusters\"\n};\n}\n\n// Deletes an cluster without deleting the cluster's runs and jobs. To\n// avoid unexpected behaviors, delete an cluster's runs and jobs before\n// deleting the cluster.\nrpc DeleteCluster(DeleteClusterRequest) returns (google.protobuf.Empty) {\noption (google.api.http) = {\ndelete: \"/apis/v1alpha2/namespaces/{namespace}/clusters/{name}\"\n};\n}\n}\n\nmessage CreateClusterRequest {\n// The cluster to be created.\nCluster cluster = 1;\n// The namespace of the cluster to be created. \nstring namespace = 2;\n}\n\nmessage GetClusterRequest {\n// The name of the cluster to be retrieved.\nstring name = 1;\n// The namespace of the cluster to be retrieved.\nstring namespace = 2;\n}\n\nmessage ListClustersRequest {\n// The namespace of the clusters to be retrieved.\nstring namespace = 1;\n\n}\n\nmessage ListClustersResponse {\n// A list of clusters returned.\nrepeated Cluster clusters = 1;\n}\n\nmessage ListAllClustersRequest {}\n\nmessage ListAllClustersResponse {\n// A list of clusters returned.\nrepeated Cluster clusters = 1;\n}\n\nmessage DeleteClusterRequest {\n// The name of the cluster to be deleted.\nstring name = 1;\n// The namespace of the cluster to be deleted.\nstring namespace = 2;\n}\n\nmessage Cluster {\n// Required input field. Unique cluster name provided by user.\nstring name = 1;\n\n// Required input field. Cluster's namespace provided by user\nstring namespace = 2;\n\n// Required field. This field indicates the user who owns the cluster.\nstring user = 3;\n\n// Optional input field. Ray cluster version\nstring version = 4;\n\n// Optional field.\nenum Environment {\nDEV = 0;\nTESTING = 1;\nSTAGING = 2;\nPRODUCTION = 3;\n}\nEnvironment environment = 5;\n\n// Required field. This field indicates ray cluster configuration\nClusterSpec cluster_spec = 6;\n\n// Output. The time that the cluster created.\ngoogle.protobuf.Timestamp created_at = 7;\n\n// Output. The time that the cluster deleted.\ngoogle.protobuf.Timestamp deleted_at = 8;\n\n// Output. The status to show the cluster status.state\nstring cluster_state = 9;\n\n// Output. The list related to the cluster.\nrepeated ClusterEvent events = 10;\n\n// Output. The service endpoint of the cluster\nmap&lt;string, string&gt; service_endpoint = 11;\n\n// Optional input field. Container environment variables from user.\nmap&lt;string, string&gt; envs = 12;\n}\n\nmessage ClusterSpec {\n// The head group configuration\nHeadGroupSpec head_group_spec = 1;\n// The worker group configurations\nrepeated WorkerGroupSpec worker_group_spec = 2;\n}\n\nmessage Volume {\nstring mount_path = 1;\nenum VolumeType {\nPERSISTENT_VOLUME_CLAIM = 0;\nHOST_PATH = 1;\n}\nVolumeType volume_type = 2;\nstring name = 3;\nstring source = 4;\nbool read_only = 5;\n\n// If indicate hostpath, we need to let user indicate which type \n// they would like to use.\nenum HostPathType {\nDIRECTORY = 0;\nFILE = 1;\n}\nHostPathType host_path_type = 6;\n\nenum MountPropagationMode {\nNONE = 0;\nHOSTTOCONTAINER = 1;\nBIDIRECTIONAL = 2;\n}\nMountPropagationMode mount_propagation_mode = 7;\n}\n\nmessage HeadGroupSpec {\n// Optional. The computeTemplate of head node group\nstring compute_template = 1;\n// Optional field. This field will be used to retrieve right ray container\nstring image = 2;\n// Optional. The service type (ClusterIP, NodePort, Load balancer) of the head node\nstring service_type = 3;\n// Optional. The ray start params of head node group\nmap&lt;string, string&gt; ray_start_params = 4;\n// Optional. The volumes mount to head pod\nrepeated Volume volumes = 5;\n}\n\nmessage WorkerGroupSpec {\n// Required. Group name of the current worker group\nstring group_name = 1;\n// Optional. The computeTemplate of head node group\nstring compute_template = 2;\n// Optional field. This field will be used to retrieve right ray container\nstring image = 3;\n// Required. Desired replicas of the worker group \nint32 replicas = 4;\n// Optional. Min replicas of the worker group \nint32 min_replicas = 5;\n// Optional. Max replicas of the worker group \nint32 max_replicas = 6;\n// Optional. The ray start parames of worker node group\nmap&lt;string, string&gt; ray_start_params = 7;\n// Optional. The volumes mount to worker pods\nrepeated Volume volumes = 8;\n}\n\nmessage ClusterEvent {\n// Output. Unique Event Id.\nstring id = 1;\n\n// Output. Human readable name for event.\nstring name = 2;\n\n// Output. The creation time of the event. \ngoogle.protobuf.Timestamp created_at = 3;\n\n// Output. The last time the event occur.\ngoogle.protobuf.Timestamp first_timestamp = 4;\n\n// Output. The first time the event occur\ngoogle.protobuf.Timestamp last_timestamp = 5;\n\n// Output. The reason for the transition into the object's current status.\nstring reason = 6;\n\n// Output. A human-readable description of the status of this operation.\nstring message = 7;\n\n// Output. Type of this event (Normal, Warning), new types could be added in the future\nstring type = 8;\n\n// Output. The number of times this event has occurred.\nint32 count = 9;\n}\n</code></pre>"},{"location":"design/protobuf-grpc-service/#support-multiple-clients","title":"Support multiple clients","text":"<p>Since we may have different clients to interactive with our services, we will generate gateway RESTful APIs and OpenAPI Spec at the same time.</p> <p></p> <p><code>.proto</code> define core api, grpc and gateway services. go_client and swagger can be generated easily for further usage.</p>"},{"location":"design/protobuf-grpc-service/#grpc-services","title":"gRPC services","text":"<p>The GRPC protocol provides an extremely efficient way of cross-service communication for distributed applications. The public toolkit includes instruments to generate client and server code-bases for many languages allowing the developer to use the most optimal language for the task.</p> <p>The service will implement gPRC server as following graph shows.</p> <ul> <li>A <code>ResourceManager</code> will be used to abstract the implementation of CRUD operators.</li> <li>ClientManager manages kubernetes clients which can operate Kubernetes native resource and custom resources like RayCluster.</li> <li><code>RayClusterClient</code> comes from code generator of CRD. issue#29</li> </ul> <p></p>"},{"location":"design/protobuf-grpc-service/#implementation-history","title":"Implementation History","text":"<ul> <li>2021-11-25: inital proposal accepted.</li> <li>2022-12-01: new protobuf definition released.</li> </ul> <p>Note: we should update doc when there's a large update.</p>"},{"location":"development/development/","title":"KubeRay Development Guide","text":"<p>This guide provides an overview of the different components in the KubeRay project and instructions for developing and testing each component. Most developers will be concerned with the KubeRay Operator; the other components are optional.</p>"},{"location":"development/development/#kuberay-operator","title":"KubeRay Operator","text":"<p>The KubeRay Operator is responsible for managing Ray clusters on Kubernetes. To learn more about developing and testing the KubeRay Operator, please refer to the Operator Development Guide.</p>"},{"location":"development/development/#kuberay-apiserver","title":"KubeRay APIServer","text":"<p>The KubeRay APIServer is a central component that exposes the KubeRay API for managing Ray clusters. For more information about developing and testing the KubeRay APIServer, please refer to the APIServer Development Guide.</p>"},{"location":"development/development/#kuberay-cli","title":"KubeRay CLI","text":"<p>The KubeRay CLI is a command-line interface for interacting with Ray clusters managed by KubeRay. For more information about developing and testing the KubeRay CLI, please refer to the CLI Development Guide.</p>"},{"location":"development/development/#proto-and-openapi","title":"Proto and OpenAPI","text":"<p>KubeRay uses Protocol Buffers (protobuf) and OpenAPI specifications to define the API and data structures. For more information about developing and testing proto files and OpenAPI specifications, please refer to the Proto and OpenAPI Development Guide.</p>"},{"location":"development/development/#deploying-documentation-locally","title":"Deploying Documentation Locally","text":"<p>To preview the KubeRay documentation locally, follow these steps:</p> <ul> <li>Make sure you have Docker installed on your machine.</li> <li>Open a terminal and navigate to the root directory of your KubeRay repository.</li> <li>Run the following command:</li> </ul> <pre><code>docker run --rm -it -p 8000:8000 -v ${PWD}:/docs squidfunk/mkdocs-material\n</code></pre> <ul> <li>Open your web browser and navigate to http://0.0.0.0:8000/kuberay/ to view the documentation.</li> </ul> <p>If you make any changes to the documentation files, the local preview will automatically update to reflect those changes.</p>"},{"location":"development/release/","title":"KubeRay Release Process","text":""},{"location":"development/release/#prerequisite","title":"Prerequisite","text":"<p>You need KubeRay GitHub write permissions to cut a release branch and create a release tag.</p>"},{"location":"development/release/#overview","title":"Overview","text":"<p>Each major release (e.g. <code>0.4</code>) is managed in its own GitHub branch. To release KubeRay, cut a release branch (e.g. <code>release-0.4</code>) from master and build commits on that branch until you reach a satisfactory final release commit.</p> <p>Immediately after cutting the release branch, create a commit for a release candidate (e.g. <code>0.4.0-rc.0</code>), and build the associated artifacts (images and charts). If further changes need to be made to the release, pick changes from the master branch into the release branch. Make as many release candidates as necessary until a stable final release commit is reached. Then build final release artifacts, publish release notes, and announce the release.</p>"},{"location":"development/release/#kuberay-release-schedule","title":"KubeRay release schedule","text":"<p>KubeRay release plans to synchronize with every two Ray releases. KubeRay v0.5.0 synchronizes with Ray 2.4.0, so v0.6.0 should synchronize with Ray 2.6.0.</p> <ul> <li>KubeRay feature freeze: Two weeks before the official Ray release.</li> <li>KubeRay release: One week before the official Ray release.</li> <li>Update KubeRay documentation in Ray repository: Finish before the official Ray release.</li> </ul>"},{"location":"development/release/#steps","title":"Steps","text":""},{"location":"development/release/#step-0-kuberay-feature-freeze","title":"Step 0. KubeRay feature freeze","text":"<p>Ensure the last master commit you want to release passes the Go-build-and-test workflow before feature freeze.</p>"},{"location":"development/release/#step-1-ensure-that-the-desired-master-commit-is-stable","title":"Step 1. Ensure that the desired master commit is stable","text":"<p>Ensure that the desired master commit is stable by verifying the following:</p> <ul> <li>The KubeRay documentation is up-to-date.</li> <li>All example configurations use the latest released version of Ray.</li> <li>The example configurations work.</li> </ul> <p>During the KubeRay <code>0.5.0</code> release, we used spreadsheets to track manual testing and documentation updates. Instead of using the latest stable release of KubeRay (i.e., v0.4.0 for the v0.5.0 release process), we should verify the master branch using the following:</p> <ul> <li>The nightly KubeRay operator Docker image: <code>kuberay/operator:nightly</code>.</li> <li>The local CRD / YAML / Helm charts.</li> </ul> <p>Open PRs to track the progress of manual testing for documentation, but avoid merging these PRs until the  Docker images and Helm charts for v0.5.0 are available  (example PRs: #997, #999, #1004, #1012). Bug fix pull requests to fix bugs which found in the documentation testing process can be merged (example PR: #1000).</p> <p>Manual testing can be time-consuming, and to relieve the workload, we plan to add more CI tests. The minimum requirements to move forward are:</p> <ul> <li>All example configurations can work with <code>kuberay/operator:nightly</code> and the latest release of Ray (i.e. 2.3.0 for KubeRay v0.5.0).</li> <li>Update all version strings in the documents.</li> </ul>"},{"location":"development/release/#step-2-create-a-new-branch-in-ray-projectkuberay-repository","title":"Step 2. Create a new branch in ray-project/kuberay repository","text":"<ul> <li>Depending on whether the release is for a major, minor, or patch version, take the following steps.</li> <li>Major or Minor version (e.g. <code>0.5.0</code> or <code>1.0.0</code>). Create a release branch named <code>release-X.Y</code>:     <pre><code>git checkout -b release-0.5\ngit push -u upstream release-0.5\n</code></pre></li> <li>Patch version (e.g. <code>0.5.1</code>). You don't need to cut a release branch for a patch version. Instead add commits to the release branch.</li> </ul>"},{"location":"development/release/#step-3-create-a-first-release-candidate-v050-rc0","title":"Step 3. Create a first release candidate (<code>v0.5.0-rc.0</code>).","text":"<ul> <li> <p>Merge a PR into the release branch updating Helm chart versions, Helm chart image tags, and kustomize manifest image tags. For <code>v0.5.0-rc0</code>, we did this in PR #1001</p> </li> <li> <p>Release <code>rc0</code> images using the release-image-build workflow on GitHub actions. You will prompted for a commit reference and an image tag. The commit reference should be the SHA of the tip of the release branch. The image tag should be <code>vX.Y.Z-rc.0</code>.</p> </li> <li> <p>Tag the tip of release branch with <code>vX.Y.Z-rc.0</code>.     <pre><code>git tag v0.5.0-rc.0\ngit push upstream v0.5.0-rc.0\n</code></pre></p> </li> <li> <p>Release rc0 Helm charts following the instructions.</p> </li> <li> <p>Open a PR into the Ray repo updating the operator version used in the autoscaler integration test. Make any adjustments necessary for the test to pass (example). Make sure the test labelled kubernetes-operator passes before merging.</p> </li> <li> <p>Announce the <code>rc0</code> release on the KubeRay slack, with deployment instructions (example).</p> </li> </ul>"},{"location":"development/release/#step-4-create-more-release-candidates-rc1-rc2-if-necessary","title":"Step 4. Create more release candidates (<code>rc1</code>, <code>rc2</code>, ...) if necessary","text":"<ul> <li>Resolve issues with the release branch by cherry picking master commits into the release branch.</li> <li>When cherry-picking changes, it is best to open a PR against the release branch -- don't push directly to the release branch.</li> <li>Follow step 3 to create new Docker images and Helm charts for the new release candidate.</li> </ul>"},{"location":"development/release/#step-5-create-a-final-release","title":"Step 5. Create a final release","text":"<ul> <li>Create a final release (i.e. v0.5.0) by repeating step 4 once more using the tag of the release (<code>vX.Y.Z</code>) with no <code>-rc</code> suffix.</li> </ul>"},{"location":"development/release/#step-6-merge-open-prs-in-step-1-and-post-release-prs","title":"Step 6. Merge open PRs in step 1 and post-release PRs","text":"<p>Now, we have the Docker images and Helm charts for v0.5.0. </p> <ul> <li> <p>Merge the pull requests in Step 1 (i.e. #997, #999, #1004, #1012)</p> </li> <li> <p>Merge post-release pull requests (example: #1010). See here to understand the definition of \"post-release\" and the compatibility philosophy for KubeRay.</p> </li> </ul>"},{"location":"development/release/#step-7-update-kuberay-documentation-in-ray-repository","title":"Step 7. Update KubeRay documentation in Ray repository.","text":"<ul> <li>Update KubeRay documentation in Ray repository with v0.5.0. Examples for v0.5.0:<ul> <li>https://github.com/ray-project/ray/pull/33339</li> <li>https://github.com/ray-project/ray/pull/34178</li> </ul> </li> </ul>"},{"location":"development/release/#step-8-generate-release","title":"Step 8. Generate release","text":"<ul> <li>Click \"Create release\" to create release for the tag v0.5.0 (link). </li> <li>Run <code>make release</code> in cli folder and generate <code>kuberay-$VERSION-darwin-amd64.zip</code> and <code>kuberay-$VERSION-linux-amd64.zip</code> files. Upload them to the GitHub release.</li> <li>Follow the instructions to generate release notes and add notes in the GitHub release.</li> </ul>"},{"location":"development/release/#step-9-announce-the-release-on-the-kuberay-slack","title":"Step 9. Announce the release on the KubeRay slack!","text":"<ul> <li>Announce the release on the KubeRay slack (example)!</li> </ul>"},{"location":"development/release/#step-10-update-changelog","title":"Step 10. Update CHANGELOG","text":"<ul> <li>Send a PR to add the release notes to CHANGELOG.md.</li> </ul>"},{"location":"development/release/#step-11-update-and-improve-this-release-document","title":"Step 11. Update and improve this release document!","text":"<ul> <li>Update this document and optimize the release process!</li> </ul>"},{"location":"guidance/FAQ/","title":"Frequently Asked Questions","text":"<p>Welcome to the Frequently Asked Questions page for KubeRay. This document addresses common inquiries. If you don't find an answer to your question here, please don't hesitate to connect with us via our community channels.</p>"},{"location":"guidance/FAQ/#contents","title":"Contents","text":"<ul> <li>Worker Init Container</li> <li>cluster domain</li> </ul>"},{"location":"guidance/FAQ/#worker-init-container","title":"Worker Init Container","text":"<p>When starting a RayCluster, the worker Pod needs to wait until the head Pod is started in order to connect to the head successfully. To achieve this, the KubeRay operator will automatically inject an init container into the worker Pod to wait for the head Pod to be ready before starting the worker container. The init container will continuously check if the head's GCS server is ready or not.</p> <p>Related questions: - Why are my worker Pods stuck in <code>Init:0/1</code> status, how can I troubleshoot the worker init container? - I do not want to use the default worker init container, how can I disable the auto-injection and add my own?</p>"},{"location":"guidance/FAQ/#cluster-domain","title":"Cluster Domain","text":"<p>Each Kubernetes cluster is assigned a unique cluster domain during installation. This domain helps differentiate between names local to the cluster and external names. The <code>cluster_domain</code> can be customized as outlined in the Kubernetes documentation. The default value for <code>cluster_domain</code> is <code>cluster.local</code>.</p> <p>The cluster domain plays a critical role in service discovery and inter-service communication within the cluster. It is part of the Fully Qualified Domain Name (FQDN) for services within the cluster. See here for examples. In the context of KubeRay, workers use the FQDN of the head service to establish a connection to the head.</p> <p>Related questions: - How can I set a custom cluster domain if mine is not <code>cluster.local</code>?</p>"},{"location":"guidance/FAQ/#questions","title":"Questions","text":""},{"location":"guidance/FAQ/#why-are-my-worker-pods-stuck-in-init01-status-how-can-i-troubleshoot-the-worker-init-container","title":"Why are my worker Pods stuck in <code>Init:0/1</code> status, how can I troubleshoot the worker init container?","text":"<p>Worker Pods might be stuck in <code>Init:0/1</code> status for several reasons. The default worker init container only progresses when the GCS server in the head Pod is ready. Here are some common causes for the issue: - The GCS server process failed in the head Pod. Inspect the head Pod logs for errors related to the GCS server. - Ray is not included in the <code>$PATH</code> in the worker init container. The init container uses <code>ray health-check</code> to check the GCS server status. - The cluster domain is not set correctly. See cluster-domain for more details. The init container uses the Fully Qualified Domain Name (FQDN) of the head service to connect to the GCS server. - The worker init container shares the same ImagePullPolicy, SecurityContext, Env, VolumeMounts, and Resources as the worker Pod template. Any setting requiring a sidecar container could lead to a deadlock. Refer to issue 1130 for additional details.</p> <p>If none of the above reasons apply, you can troubleshoot by disabling the default worker init container injection and adding your test init container to the worker Pod template.</p>"},{"location":"guidance/FAQ/#i-do-not-want-to-use-the-default-worker-init-container-how-can-i-disable-the-auto-injection-and-add-my-own","title":"I do not want to use the default worker init container, how can I disable the auto-injection and add my own?","text":"<p>The default worker init container is used to wait for the GCS server in the head Pod to be ready. It is defined here. To disable the injection, set the <code>ENABLE_INIT_CONTAINER_INJECTION</code> environment variable in the KubeRay operator to <code>false</code> (applicable only for versions after 0.5.0). Helm chart users can make this change here. Once disabled, you can add your custom init container to the worker Pod template. More details can be found in PR 1069.</p>"},{"location":"guidance/FAQ/#how-can-i-set-the-custom-cluster-domain-if-mine-is-not-clusterlocal","title":"How can I set the custom cluster domain if mine is not <code>cluster.local</code>?","text":"<p>To set a custom cluster domain, adjust the <code>CLUSTER_DOMAIN</code> environment variable in the KubeRay operator. Helm chart users can make this modification here.</p>"},{"location":"guidance/FAQ/#why-are-my-changes-to-rayclusterrayjob-cr-not-taking-effect","title":"Why are my changes to RayCluster/RayJob CR not taking effect?","text":"<p>Currently, only modifications to the <code>replicas</code> field in <code>RayCluster/RayJob</code> CR are supported. Changes to other fields may not take effect or could lead to unexpected results.</p>"},{"location":"guidance/autoscaler/","title":"Autoscaling","text":""},{"location":"guidance/autoscaler/#autoscaler-beta","title":"Autoscaler (beta)","text":"<p>Ray Autoscaler integration is beta since KubeRay 0.3.0 and Ray 2.0.0. While autoscaling functionality is stable, the details of autoscaler behavior and configuration may change in future releases.</p> <p>See the official Ray documentation for even more information about Ray autoscaling on Kubernetes.</p>"},{"location":"guidance/autoscaler/#prerequisite","title":"Prerequisite","text":"<ul> <li>Follow this document to install the latest stable KubeRay operator via Helm repository.</li> </ul>"},{"location":"guidance/autoscaler/#deploy-a-cluster-with-autoscaling-enabled","title":"Deploy a cluster with autoscaling enabled","text":"<p>Next, to deploy a sample autoscaling Ray cluster, run <pre><code>kubectl apply -f https://raw.githubusercontent.com/ray-project/kuberay/release-0.5/ray-operator/config/samples/ray-cluster.autoscaler.yaml\n</code></pre></p> <p>See the above config file for details on autoscaling configuration.</p> <p>Note</p> <p>Ray container resource requests and limits in the example configuration above are too small to be used in production. For typical use-cases, you should use large Ray pods. If possible, each Ray pod should be sized to take up its entire K8s node. We don't recommend allocating less than 8 gigabytes of memory for Ray containers running in production. For an autoscaling configuration more suitable for production, see ray-cluster.autoscaler.large.yaml.</p> <p>The output of <code>kubectl get pods</code> should indicate the presence of a Ray head pod with two containers, the Ray container and the autoscaler container. You should also see a Ray worker pod with a single Ray container.</p> <pre><code>$ kubectl get pods\nNAME                                             READY   STATUS    RESTARTS   AGE\nraycluster-autoscaler-head-mgwwk                 2/2     Running   0          4m41s\nraycluster-autoscaler-worker-small-group-fg4fv   1/1     Running   0          4m41s\n</code></pre> <p>Check the autoscaler container's logs to confirm that the autoscaler is healthy. Here's an example of logs from a healthy autoscaler. <pre><code>kubectl logs -f raycluster-autoscaler-head-mgwwk autoscaler\n\n2022-03-10 07:51:22,616 INFO monitor.py:226 -- Starting autoscaler metrics server on port 44217\n2022-03-10 07:51:22,621 INFO monitor.py:243 -- Monitor: Started\n2022-03-10 07:51:22,824 INFO node_provider.py:143 -- Creating KuberayNodeProvider.\n2022-03-10 07:51:22,825 INFO autoscaler.py:282 -- StandardAutoscaler: {'provider': {'type': 'kuberay', 'namespace': 'default', 'disable_node_updaters': True, 'disable_launch_config_check': True}, 'cluster_name': 'raycluster-autoscaler', 'head_node_type': 'head-group', 'available_node_types': {'head-group': {'min_workers': 0, 'max_workers': 0, 'node_config': {}, 'resources': {'CPU': 1}}, 'small-group': {'min_workers': 1, 'max_workers': 300, 'node_config': {}, 'resources': {'CPU': 1}}}, 'max_workers': 300, 'idle_timeout_minutes': 5, 'upscaling_speed': 1, 'file_mounts': {}, 'cluster_synced_files': [], 'file_mounts_sync_continuously': False, 'initialization_commands': [], 'setup_commands': [], 'head_setup_commands': [], 'worker_setup_commands': [], 'head_start_ray_commands': [], 'worker_start_ray_commands': [], 'auth': {}, 'head_node': {}, 'worker_nodes': {}}\n2022-03-10 07:51:23,027 INFO autoscaler.py:327 --\n======== Autoscaler status: 2022-03-10 07:51:23.027271 ========\nNode status\n---------------------------------------------------------------\nHealthy:\n 1 head-group\nPending:\n (no pending nodes)\nRecent failures:\n (no failures)\n\nResources\n---------------------------------------------------------------\nUsage:\n 0.0/1.0 CPU\n 0.00/0.931 GiB memory\n 0.00/0.200 GiB object_store_memory\n\nDemands:\n (no resource demands)\n</code></pre></p>"},{"location":"guidance/autoscaler/#notes","title":"Notes","text":"<ol> <li> <p>To enable autoscaling, set your RayCluster CR's <code>spec.enableInTreeAutoscaling</code> field to true.    The operator will then automatically inject a preconfigured autoscaler container to the head pod.    The service account, role, and role binding needed by the autoscaler will be created by the operator out-of-box.    The operator will also configure an empty-dir logging volume for the Ray head pod. The volume will be mounted into the Ray and    autoscaler containers; this is necessary to support the event logging introduced in Ray PR #13434.</p> <pre><code>spec:\n  enableInTreeAutoscaling: true\n</code></pre> </li> <li> <p>If your RayCluster CR's <code>spec.rayVersion</code> field is at least <code>2.0.0</code>, the autoscaler container will use the same image as the Ray container.    For Ray versions older than 2.0.0, the image <code>rayproject/ray:2.0.0</code> will be used to run the autoscaler.</p> </li> <li> <p>Autoscaling functionality is supported only with Ray versions at least as new as 1.11.0. Autoscaler support    is beta as of Ray 2.0.0 and KubeRay 0.3.0; while autoscaling functionality is stable, the details of autoscaler behavior and configuration may change in future releases.</p> </li> </ol>"},{"location":"guidance/autoscaler/#test-autoscaling","title":"Test autoscaling","text":"<p>Let's now try out the autoscaler. Run the following commands to scale up the cluster:</p> <pre><code>export HEAD_POD=$(kubectl get pods -o custom-columns=POD:metadata.name | grep raycluster-autoscaler-head)\nkubectl exec $HEAD_POD -it -c ray-head -- python -c \"import ray;ray.init();ray.autoscaler.sdk.request_resources(num_cpus=4)\"\n</code></pre> <p>You should then see two extra Ray nodes (pods) scale up to satisfy the 4 CPU demand.</p> <pre><code>$ kubectl get pods\nNAME                                             READY   STATUS    RESTARTS   AGE\nraycluster-autoscaler-head-mgwwk                 2/2     Running   0          4m41s\nraycluster-autoscaler-worker-small-group-4d255   1/1     Running   0          40s\nraycluster-autoscaler-worker-small-group-fg4fv   1/1     Running   0          4m41s\nraycluster-autoscaler-worker-small-group-qzhvg   1/1     Running   0          40s\n</code></pre>"},{"location":"guidance/aws-eks-gpu-cluster/","title":"Start Amazon EKS Cluster with GPUs for KubeRay","text":""},{"location":"guidance/aws-eks-gpu-cluster/#step-1-create-a-kubernetes-cluster-on-amazon-eks","title":"Step 1: Create a Kubernetes cluster on Amazon EKS","text":"<p>Follow the first two steps in this AWS documentation to: (1) create your Amazon EKS cluster and (2) configure your computer to communicate with your cluster.</p>"},{"location":"guidance/aws-eks-gpu-cluster/#step-2-create-node-groups-for-the-amazon-eks-cluster","title":"Step 2: Create node groups for the Amazon EKS cluster","text":"<p>Follow \"Step 3: Create nodes\" in this AWS documentation to create node groups. The following section provides more detailed information.</p>"},{"location":"guidance/aws-eks-gpu-cluster/#create-a-cpu-node-group","title":"Create a CPU node group","text":"<p>Typically, avoid running GPU workloads on the Ray head. Create a CPU node group for all Pods except Ray GPU  workers, such as the KubeRay operator, Ray head, and CoreDNS Pods.</p> <p>Here's a common configuration that works for most KubeRay examples in the docs:   * Instance type: m5.xlarge (4 vCPU; 16 GB RAM)   * Disk size: 256 GB   * Desired size: 1, Min size: 0, Max size: 1</p>"},{"location":"guidance/aws-eks-gpu-cluster/#create-a-gpu-node-group","title":"Create a GPU node group","text":"<p>Create a GPU node group for Ray GPU workers.</p> <ol> <li>Here's a common configuration that works for most KubeRay examples in the docs:</li> <li>AMI type: Bottlerocket NVIDIA (BOTTLEROCKET_x86_64_NVIDIA)</li> <li>Instance type: g5.xlarge (1 GPU; 24 GB GPU Memory; 4 vCPUs; 16 GB RAM)</li> <li>Disk size: 1024 GB</li> <li>Desired size: 1, Min size: 0, Max size: 1</li> </ol> <p>Note: If you encounter permission issues with <code>kubectl</code>, follow \"Step 2: Configure your computer to communicate with your cluster\" in the AWS documentation.</p> <ol> <li>Please install the NVIDIA device plugin.</li> <li>Install the DaemonSet for NVIDIA device plugin to run GPU enabled containers in your Amazon EKS cluster. You can refer to the Amazon EKS optimized accelerated Amazon Linux AMIs    or NVIDIA/k8s-device-plugin repository for more details.</li> <li>If the GPU nodes have taints, add <code>tolerations</code> to <code>nvidia-device-plugin.yml</code> to enable the DaemonSet to schedule Pods on the GPU nodes.</li> </ol> <pre><code># Install the DaemonSet\nkubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.9.0/nvidia-device-plugin.yml\n\n# Verify that your nodes have allocatable GPUs. If the GPU node fails to detect GPUs,\n# please verify whether the DaemonSet schedules the Pod on the GPU node.\nkubectl get nodes \"-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu\"\n\n# Example output:\n# NAME                                GPU\n# ip-....us-west-2.compute.internal   4\n# ip-....us-west-2.compute.internal   &lt;none&gt;\n</code></pre> <ol> <li>Add a Kubernetes taint to prevent scheduling CPU Pods on this GPU node group. For KubeRay examples, add the following taint to the GPU nodes: <code>Key: ray.io/node-type, Value: worker, Effect: NoSchedule</code>, and include the corresponding <code>tolerations</code> for GPU Ray worker Pods.</li> </ol> <p>Warning: GPU nodes are extremely expensive. Please remember to delete the cluster if you no longer need it.</p>"},{"location":"guidance/aws-eks-gpu-cluster/#step-3-verify-the-node-groups","title":"Step 3: Verify the node groups","text":"<p>Note: If you encounter permission issues with <code>eksctl</code>, navigate to your AWS account's webpage and copy the credential environment variables, including <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code>, and <code>AWS_SESSION_TOKEN</code>, from the \"Command line or programmatic access\" page.</p> <pre><code>eksctl get nodegroup --cluster ${YOUR_EKS_NAME}\n\n# CLUSTER         NODEGROUP       STATUS  CREATED                 MIN SIZE        MAX SIZE        DESIRED CAPACITY        INSTANCE TYPE   IMAGE ID                        ASG NAME                           TYPE\n# ${YOUR_EKS_NAME}     cpu-node-group  ACTIVE  2023-06-05T21:31:49Z    0               1               1                       m5.xlarge       AL2_x86_64                      eks-cpu-node-group-...     managed\n# ${YOUR_EKS_NAME}     gpu-node-group  ACTIVE  2023-06-05T22:01:44Z    0               1               1                       g5.12xlarge     BOTTLEROCKET_x86_64_NVIDIA      eks-gpu-node-group-...     managed\n</code></pre>"},{"location":"guidance/aws-eks-iam/","title":"IAM roles for service account on AWS EKS","text":"<p>Applications in a pod's containers can use an AWS SDK or the AWS CLI to make API requests to AWS services using AWS Identity and Access Management (IAM) permissions. Applications must sign their AWS API requests with AWS credentials. IAM roles for service accounts provide the ability to manage credentials for your applications. To achieve this, you can read the following articles:</p> <ul> <li>IAM roles for service accounts: This is the official AWS documentation that explains IAM roles for service accounts step-by-step.</li> <li>Understanding IAM roles for service accounts, IRSA, on AWS EKS.: Good article to provide an easy-to-understand explanation.</li> </ul>"},{"location":"guidance/aws-eks-iam/#pitfall","title":"Pitfall","text":"<ul> <li>It's worth noting that this pitfall occurs in Ray images prior to version 2.5.0.</li> </ul> <p>For example, users may want to download their files from their S3 bucket with AWS Python SDK (<code>boto3</code>) in Ray Pods. However, there is a pitfall in the Ray images. When you execute the boto3_example_1.py in a Ray Pod, you will get an error like <code>An error occurred (403) when calling the HeadObject operation: Forbidden</code> even if your pod is attached to a service account which has an IAM role that is able to access the S3 bucket.</p> <pre><code># boto3_example_1.py\nimport boto3\n\ns3 = boto3.client('s3')\nbucket_name = YOUR_BUCKET_NAME\nkey = YOUR_OBJECT_KEY\nfilename = YOUR_FILENAME\n\ns3.download_file(bucket_name, key, filename)\n</code></pre> <p>The root cause is that the version of <code>boto3</code> in the Ray image is too old. To elaborate, <code>rayproject/ray:2.3.0</code> provides boto3 version 1.4.8 (Nov. 21, 2017), a more recent version (1.26) is currently available as per https://pypi.org/project/boto3/#history. The <code>boto3</code> 1.4.8 does not support to initialize the security credentials automatically in some cases (e.g. <code>AssumeRoleWithWebIdentity</code>).</p> <pre><code># image: rayproject/ray:2.5.0\npip freeze | grep boto\n# boto3==1.4.8\n# botocore==1.8.50\n</code></pre> <p>Another issue that users may encounter is related to RayService. If the <code>working_dir</code> for RayService is set to a zip file located in a private S3 bucket, it can prevent the Ray Serve application from starting. Users can confirm this by executing <code>serve status</code> in the head Pod, which will return an error like <code>An error occurred (AccessDenied) when calling the GetObject operation: Access Denied</code>. In this case, users can build their custom images with upgrading the <code>boto3</code> package (i.e. Solution 2).</p>"},{"location":"guidance/aws-eks-iam/#workaround-solutions","title":"Workaround solutions","text":""},{"location":"guidance/aws-eks-iam/#solution-1-setup-the-credentials-explicitly","title":"Solution 1: Setup the credentials explicitly","text":"<pre><code># boto3_example_2.py\nimport os\nimport boto3\ndef assumed_role_session():\n    role_arn = os.getenv('AWS_ROLE_ARN')\n    with open(os.getenv(\"AWS_WEB_IDENTITY_TOKEN_FILE\"), 'r') as content_file:\n        web_identity_token = content_file.read()\n    role = boto3.client('sts').assume_role_with_web_identity(RoleArn=role_arn, RoleSessionName='assume-role',\n                                                             WebIdentityToken=web_identity_token)\n    credentials = role['Credentials']\n    aws_access_key_id = credentials['AccessKeyId']\n    aws_secret_access_key = credentials['SecretAccessKey']\n    aws_session_token = credentials['SessionToken']\n    return boto3.session.Session(aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key,\n                              aws_session_token=aws_session_token)\n\nsession = assumed_role_session()\ns3 = session.client(\"s3\")\nbucket_name = YOUR_BUCKET_NAME\nkey = YOUR_OBJECT_KEY\nfilename = YOUR_FILENAME\ns3.download_file(bucket_name, key, filename)\n</code></pre>"},{"location":"guidance/aws-eks-iam/#solution-2-upgrade-the-boto3-package","title":"Solution 2: Upgrade the boto3 package","text":"<pre><code>pip install --upgrade boto3\npython3 -m pip install -U pyOpenSSL cryptography\npython3 boto3_example_1.py # success\n</code></pre>"},{"location":"guidance/gcs-ft/","title":"Ray GCS Fault Tolerance","text":""},{"location":"guidance/gcs-ft/#ray-gcs-fault-tolerance-gcs-ft-beta-release","title":"Ray GCS Fault Tolerance (GCS FT) \uff08Beta release\uff09","text":"<p>Note: This feature is beta.</p> <p>Ray GCS FT enables GCS server to use external storage backend. As a result, Ray clusters can tolerant GCS failures and recover from failures without affecting important services such as detached Actors &amp; RayServe deployments.</p>"},{"location":"guidance/gcs-ft/#prerequisite","title":"Prerequisite","text":"<ul> <li>Ray 2.0 is required.</li> <li>You need to support external Redis server for Ray. (Redis HA cluster is highly recommended.)</li> </ul>"},{"location":"guidance/gcs-ft/#enable-ray-gcs-ft","title":"Enable Ray GCS FT","text":"<p>To enable Ray GCS FT in your newly KubeRay-managed Ray cluster, you need to enable it by adding an annotation to the RayCluster YAML file.</p> <p><pre><code>...\nkind: RayCluster\nmetadata:\nannotations:\nray.io/ft-enabled: \"true\" # &lt;- add this annotation enable GCS FT\nray.io/external-storage-namespace: \"my-raycluster-storage-namespace\" # &lt;- optional, to specify the external storage namespace\n...\n</code></pre> An example can be found at ray-cluster.external-redis.yaml</p> <p>When annotation <code>ray.io/ft-enabled</code> is added with a <code>true</code> value, KubeRay will enable Ray GCS FT feature. This feature contains several components:</p> <ol> <li>Newly created Ray cluster has <code>Readiness Probe</code> and <code>Liveness Probe</code> added to all the head/worker nodes.</li> <li>KubeRay Operator controller watches for <code>Event</code> object changes which can notify in case of readiness probe failures and mark them as <code>Unhealthy</code>.</li> <li>KubeRay Operator controller kills and recreate any <code>Unhealthy</code> Ray head/worker node.</li> </ol>"},{"location":"guidance/gcs-ft/#implementation-details","title":"Implementation Details","text":""},{"location":"guidance/gcs-ft/#readiness-probe-vs-liveness-probe","title":"Readiness Probe vs Liveness Probe","text":"<p>These are the two types of probes we used in Ray GCS FT. </p> <p>The readiness probe is used to notify KubeRay in case of failures in the corresponding Ray cluster. KubeRay can try its best to recover the Ray cluster. If KubeRay cannot recover the failed head/worker node, the liveness probe gets in, delete the old pod and create a new pod.</p> <p>By default, the liveness probe gets involved later than the readiness probe. The liveness probe is our last resort to recover the  Ray cluster. However, in our current implementation, for the readiness probe failures, we also kill &amp; recreate the corresponding pod that runs head/worker node.</p> <p>Currently, the readiness probe and the liveness probe are using the same command to do the work. In the future, we may run  different commands for the readiness probe and the liveness probe.</p> <p>On Ray head node, we access a local Ray dashboard http endpoint and a Raylet http endpoint to make sure this head node is in healthy state. Since Ray dashboard does not reside Ray worker node, we only check the local Raylet http endpoint to make sure the worker node is healthy.</p>"},{"location":"guidance/gcs-ft/#ray-gcs-ft-annotation","title":"Ray GCS FT Annotation","text":"<p>Our Ray GCS FT feature checks if an annotation called <code>ray.io/ft-enabled</code> is set to <code>true</code> in <code>RayCluster</code> YAML file. If so, KubeRay will also add such annotation to the pod whenever the head/worker node is created.</p>"},{"location":"guidance/gcs-ft/#use-external-redis-cluster","title":"Use External Redis Cluster","text":"<p>To use external Redis cluster as the backend storage(required by Ray GCS FT), you need to add <code>RAY_REDIS_ADDRESS</code> environment variable to the head node template.</p> <p>Also, you can specify a storage namespace for your Ray cluster by using an annotation <code>ray.io/external-storage-namespace</code></p> <p>An example can be found at ray-cluster.external-redis.yaml</p> <p>To use SSL/TLS in the connection, you add <code>rediss://</code> as the prefix of the redis address instead of the <code>redis://</code> prefix. This feature is only available in Ray 2.2 and above.</p> <p>You can also specify additional environment variables in the head pod to customize the SSL configuration:</p> <ul> <li><code>RAY_REDIS_CA_CERT</code> The location of the CA certificate (optional)</li> <li><code>RAY_REDIS_CA_PATH</code> Path of trusted certificates (optional)</li> <li><code>RAY_REDIS_CLIENT_CERT</code> File name of client certificate file (optional)</li> <li><code>RAY_REDIS_CLIENT_KEY</code> File name of client private key (optional)</li> <li><code>RAY_REDIS_SERVER_NAME</code> Server name to request (SNI) (optional)</li> </ul>"},{"location":"guidance/gcs-ft/#kuberay-operator-controller","title":"KubeRay Operator Controller","text":"<p>KubeRay Operator controller watches for new <code>Event</code> reconcile call. If this Event object is to notify the failed readiness probe, controller checks if this pod has <code>ray.io/ft-enabled</code> set to <code>true</code>. If this pod has this annotation set to true, that means this pod belongs to a Ray cluster that has Ray GCS FT enabled.</p> <p>After this, the controller will try to recover the failed pod. If controller cannot recover it, an annotation named  <code>ray.io/health-state</code> with a value <code>Unhealthy</code> is added to this pod.</p> <p>In every KubeRay Operator controller reconcile loop, it monitors any pod in Ray cluster that has <code>Unhealthy</code> value in annotation <code>ray.io/health-state</code>. If any pod is found, this pod is deleted and gets recreated.</p>"},{"location":"guidance/gcs-ft/#external-storage-namespace","title":"External Storage Namespace","text":"<p>External storage namespaces can be used to share a single storage backend among multiple Ray clusters. By default, <code>ray.io/external-storage-namespace</code> uses the RayCluster UID as its value when GCS FT is enabled. Or if the user wants to use customized external storage namespace, the user can add <code>ray.io/external-storage-namespace</code> annotation to RayCluster yaml file.</p> <p>Whenever <code>ray.io/external-storage-namespace</code> annotation is set, the head/worker node will have <code>RAY_external_storage_namespace</code> environment variable set which Ray can pick up later.</p>"},{"location":"guidance/gcs-ft/#known-issues-and-limitations","title":"Known issues and limitations","text":"<ol> <li>For now, Ray head/worker node that fails the readiness probe recovers itself by restarting itself. More fine-grained control and recovery mechanisms are expected in the future.</li> </ol>"},{"location":"guidance/gcs-ft/#test-ray-gcs-ft","title":"Test Ray GCS FT","text":"<p>Currently, two tests are responsible for ensuring Ray GCS FT is working correctly.</p> <ol> <li>Detached actor test</li> <li>RayServe test</li> </ol> <p>In detached actor test, a detached actor is created at first. Then, the head node is killed. KubeRay brings back another head node replacement pod. However, the detached actor is still expected to be available. (Note: the client that creates the detached actor does not exist and will retry in case of Ray cluster returns failure)</p> <p>In RayServe test, a simple RayServe app is deployed on the Ray cluster. In case of GCS server crash, the RayServe app continues to be accessible after the head node recovery.</p>"},{"location":"guidance/ingress/","title":"Ingress","text":""},{"location":"guidance/ingress/#ingress-usage","title":"Ingress Usage","text":"<p>Here we provide some examples to show how to use ingress to access your Ray cluster.</p> <ul> <li>Example: AWS Application Load Balancer (ALB) Ingress support on AWS EKS</li> <li>Example: Manually setting up NGINX Ingress on KinD</li> </ul>"},{"location":"guidance/ingress/#example-aws-application-load-balancer-alb-ingress-support-on-aws-eks","title":"Example: AWS Application Load Balancer (ALB) Ingress support on AWS EKS","text":""},{"location":"guidance/ingress/#prerequisite","title":"Prerequisite","text":"<ul> <li> <p>Follow the document Getting started with Amazon EKS \u2013 AWS Management Console and AWS CLI to create an EKS cluster.</p> </li> <li> <p>Follow the installation instructions to set up the AWS Load Balancer controller. Note that the repository maintains a webpage for each release. Please make sure you use the latest installation instructions.</p> </li> <li> <p>(Optional) Try echo server example in the aws-load-balancer-controller repository.</p> </li> <li> <p>(Optional) Read how-it-works.md to understand the mechanism of aws-load-balancer-controller.</p> </li> </ul>"},{"location":"guidance/ingress/#instructions","title":"Instructions","text":"<pre><code># Step 1: Install KubeRay operator and CRD\npushd helm-chart/kuberay-operator/\nhelm install kuberay-operator .\npopd\n\n# Step 2: Install a RayCluster\npushd helm-chart/ray-cluster\nhelm install ray-cluster .\npopd\n\n# Step 3: Edit the `ray-operator/config/samples/ray-cluster-alb-ingress.yaml`\n#\n# (1) Annotation `alb.ingress.kubernetes.io/subnets`\n#   1. Please include at least two subnets.\n#   2. One Availability Zone (ex: us-west-2a) can only have at most 1 subnet.\n#   3. In this example, you need to select public subnets (subnets that \"Auto-assign public IPv4 address\" is Yes on AWS dashboard)\n#\n# (2) Set the name of head pod service to `spec...backend.service.name`\neksctl get cluster ${YOUR_EKS_CLUSTER} # Check subnets on the EKS cluster\n\n# Step 4: Create an ALB ingress. When an ingress with proper annotations creates,\n#        AWS Load Balancer controller will reconcile a ALB (not in AWS EKS cluster).\n# For RayService, you can use ray-operator/config/samples/ray_v1alpha1_rayservice-alb-ingress.yaml\nkubectl apply -f ray-operator/config/samples/alb-ingress.yaml\n\n# Step 5: Check ingress created by Step 4.\nkubectl describe ingress ray-cluster-ingress\n\n# [Example]\n# Name:             ray-cluster-ingress\n# Labels:           &lt;none&gt;\n# Namespace:        default\n# Address:          k8s-default-rayclust-....${REGION_CODE}.elb.amazonaws.com\n# Default backend:  default-http-backend:80 (&lt;error: endpoints \"default-http-backend\" not found&gt;)\n# Rules:\n#  Host        Path  Backends\n#  ----        ----  --------\n#  *\n#              /   ray-cluster-kuberay-head-svc:8265 (192.168.185.157:8265)\n# Annotations: alb.ingress.kubernetes.io/scheme: internet-facing\n#              alb.ingress.kubernetes.io/subnets: ${SUBNET_1},${SUBNET_2}\n#              alb.ingress.kubernetes.io/tags: Environment=dev,Team=test\n#              alb.ingress.kubernetes.io/target-type: ip\n# Events:\n#   Type    Reason                  Age   From     Message\n#   ----    ------                  ----  ----     -------\n#   Normal  SuccessfullyReconciled  39m   ingress  Successfully reconciled\n\n# Step 6: Check ALB on AWS (EC2 -&gt; Load Balancing -&gt; Load Balancers)\n#        The name of the ALB should be like \"k8s-default-rayclust-......\".\n\n# Step 7: Check Ray Dashboard by ALB DNS Name. The name of the DNS Name should be like\n#        \"k8s-default-rayclust-.....us-west-2.elb.amazonaws.com\"\n\n# Step 8: Delete the ingress, and AWS Load Balancer controller will remove ALB.\n#        Check ALB on AWS to make sure it is removed.\nkubectl delete ingress ray-cluster-ingress\n</code></pre>"},{"location":"guidance/ingress/#example-manually-setting-up-nginx-ingress-on-kind","title":"Example: Manually setting up NGINX Ingress on KinD","text":"<pre><code># Step 1: Create a KinD cluster with `extraPortMappings` and `node-labels`\n# Reference for the setting up of kind cluster: https://kind.sigs.k8s.io/docs/user/ingress/\ncat &lt;&lt;EOF | kind create cluster --config=-\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  kubeadmConfigPatches:\n  - |\n    kind: InitConfiguration\n    nodeRegistration:\n      kubeletExtraArgs:\n        node-labels: \"ingress-ready=true\"\n  extraPortMappings:\n  - containerPort: 80\n    hostPort: 80\n    protocol: TCP\n  - containerPort: 443\n    hostPort: 443\n    protocol: TCP\nEOF\n\n# Step 2: Install NGINX ingress controller\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml\nsleep 10 # Wait for the Kubernetes API Server to create the related resources\nkubectl wait --namespace ingress-nginx \\\n--for=condition=ready pod \\\n--selector=app.kubernetes.io/component=controller \\\n--timeout=90s\n\n# Step 3: Install KubeRay operator\npushd helm-chart/kuberay-operator\nhelm install kuberay-operator .\npopd\n\n# Step 4: Install RayCluster and create an ingress separately. \n# If you want to change ingress settings, you can edit the ingress portion in \n# `ray-operator/config/samples/ray-cluster.separate-ingress.yaml`.\n# More information about change of setting was documented in https://github.com/ray-project/kuberay/pull/699 \n# and `ray-operator/config/samples/ray-cluster.separate-ingress.yaml`\nkubectl apply -f ray-operator/config/samples/ray-cluster.separate-ingress.yaml\n\n# Step 5: Check the ingress created in Step 4.\nkubectl describe ingress raycluster-ingress-head-ingress\n\n# [Example]\n# ...\n# Rules:\n# Host        Path  Backends\n# ----        ----  --------\n# *\n#             /raycluster-ingress/(.*)   raycluster-ingress-head-svc:8265 (10.244.0.11:8265)\n# Annotations:  nginx.ingress.kubernetes.io/rewrite-target: /$1\n\n# Step 6: Check `&lt;ip&gt;/raycluster-ingress/` on your browser. You will see the Ray Dashboard.\n#        [Note] The forward slash at the end of the address is necessary. `&lt;ip&gt;/raycluster-ingress`\n#               will report \"404 Not Found\".\n</code></pre>"},{"location":"guidance/kubeflow-integration/","title":"Kubeflow Integration","text":"<p>Credit: This manifest refers a lot to the engineering blog \"Building a Machine Learning Platform with Kubeflow and Ray on Google Kubernetes Engine\" from Google Cloud.</p>"},{"location":"guidance/kubeflow-integration/#kubeflow-an-interactive-development-solution","title":"Kubeflow: an interactive development solution","text":"<p>The Kubeflow project is dedicated to making deployments of machine learning (ML) workflows on Kubernetes simple, portable and scalable.</p>"},{"location":"guidance/kubeflow-integration/#requirements","title":"Requirements","text":"<ul> <li> <p>Dependencies</p> <ul> <li><code>kustomize</code>: v3.2.0 (Kubeflow manifest is sensitive to <code>kustomize</code> version.)</li> <li><code>Kubernetes</code>: v1.23</li> </ul> </li> <li> <p>Computing resources:</p> <ul> <li>16GB RAM</li> <li>8 CPUs</li> </ul> </li> </ul>"},{"location":"guidance/kubeflow-integration/#example-use-kubeflow-to-provide-an-interactive-development-envirzonment","title":"Example: Use Kubeflow to provide an interactive development envirzonment","text":""},{"location":"guidance/kubeflow-integration/#step-1-create-a-kubernetes-cluster-with-kind","title":"Step 1: Create a Kubernetes cluster with Kind.","text":"<pre><code># Kubeflow is sensitive to Kubernetes version and Kustomize version.\nkind create cluster --image=kindest/node:v1.23.0\nkustomize version --short\n# 3.2.0\n</code></pre>"},{"location":"guidance/kubeflow-integration/#step-2-install-kubeflow-v16-branch","title":"Step 2: Install Kubeflow v1.6-branch","text":"<ul> <li> <p>This example installs Kubeflow with the v1.6-branch.</p> </li> <li> <p>Install all Kubeflow official components and all common services using one command.</p> <ul> <li>If you do not want to install all components, you can comment out KNative, Katib, Tensorboards Controller, Tensorboard Web App, Training Operator, and KServe from example/kustomization.yaml.</li> </ul> </li> </ul>"},{"location":"guidance/kubeflow-integration/#step-3-install-kuberay-operator","title":"Step 3: Install KubeRay operator","text":"<ul> <li>Follow this document to install the latest stable KubeRay operator via Helm repository.</li> </ul>"},{"location":"guidance/kubeflow-integration/#step-4-install-raycluster","title":"Step 4: Install RayCluster","text":"<pre><code># Create a RayCluster CR, and the KubeRay operator will reconcile a Ray cluster\n# with 1 head Pod and 1 worker Pod.\nhelm install raycluster kuberay/ray-cluster --version 0.5.0 --set image.tag=2.2.0-py38-cpu\n\n# Check RayCluster\nkubectl get pod -l ray.io/cluster=raycluster-kuberay\n# NAME                                          READY   STATUS    RESTARTS   AGE\n# raycluster-kuberay-head-bz77b                 1/1     Running   0          64s\n# raycluster-kuberay-worker-workergroup-8gr5q   1/1     Running   0          63s\n</code></pre> <ul> <li>This step uses <code>rayproject/ray:2.2.0-py38-cpu</code> as its image. Ray is very sensitive to the Python versions and Ray versions between the server (RayCluster) and client (JupyterLab) sides. This image uses:<ul> <li>Python 3.8.13</li> <li>Ray 2.2.0</li> </ul> </li> </ul>"},{"location":"guidance/kubeflow-integration/#step-5-forward-the-port-of-istios-ingress-gateway","title":"Step 5: Forward the port of Istio's Ingress-Gateway","text":"<ul> <li>Follow the instructions to forward the port of Istio's Ingress-Gateway and log in to Kubeflow Central Dashboard.</li> </ul>"},{"location":"guidance/kubeflow-integration/#step-6-create-a-jupyterlab-via-kubeflow-central-dashboard","title":"Step 6: Create a JupyterLab via Kubeflow Central Dashboard","text":"<ul> <li>Click \"Notebooks\" icon in the left panel.</li> <li>Click \"New Notebook\"</li> <li>Select <code>kubeflownotebookswg/jupyter-scipy:v1.6.1</code> as OCI image.</li> <li>Click \"Launch\"</li> <li>Click \"CONNECT\" to connect into the JupyterLab instance.</li> </ul>"},{"location":"guidance/kubeflow-integration/#step-7-use-ray-client-in-the-jupyterlab-to-connect-to-the-raycluster","title":"Step 7: Use Ray client in the JupyterLab to connect to the RayCluster","text":"<p>Warning: Ray client has some known limitations and is not actively maintained.</p> <ul> <li>As mentioned in Step 4, Ray is very sensitive to the Python versions and Ray versions between the server (RayCluster) and client (JupyterLab) sides. Open a terminal in the JupyterLab:     <pre><code># Check Python version. The version's MAJOR and MINOR should match with RayCluster (i.e. Python 3.8)\npython --version # Python 3.8.10\n\n# Install Ray 2.2.0\npip install -U ray[default]==2.2.0\n</code></pre></li> <li>Connect to RayCluster via Ray client.     <pre><code># Open a new .ipynb page.\n\nimport ray\n# ray://${RAYCLUSTER_HEAD_SVC}.${NAMESPACE}.svc.cluster.local:${RAY_CLIENT_PORT}\nray.init(address=\"ray://raycluster-kuberay-head-svc.default.svc.cluster.local:10001\")\nprint(ray.cluster_resources())\n# {'node:10.244.0.41': 1.0, 'memory': 3000000000.0, 'node:10.244.0.40': 1.0, 'object_store_memory': 805386239.0, 'CPU': 2.0}\n\n# Try Ray task\n@ray.remote\ndef f(x):\n    return x * x\n\nfutures = [f.remote(i) for i in range(4)]\nprint(ray.get(futures)) # [0, 1, 4, 9]\n\n# Try Ray actor\n@ray.remote\nclass Counter(object):\n    def __init__(self):\n        self.n = 0\n\n    def increment(self):\n        self.n += 1\n\n    def read(self):\n        return self.n\n\ncounters = [Counter.remote() for i in range(4)]\n[c.increment.remote() for c in counters]\nfutures = [c.read.remote() for c in counters]\nprint(ray.get(futures)) # [1, 1, 1, 1]\n</code></pre></li> </ul>"},{"location":"guidance/kuberay-with-MCAD/","title":"KubeRay integration with MCAD (Multi-Cluster-App-Dispatcher)","text":"<p>The multi-cluster-app-dispatcher is a Kubernetes controller providing mechanisms for applications to manage batch jobs in a single or multi-cluster environment. For more details please refer here.</p>"},{"location":"guidance/kuberay-with-MCAD/#use-case","title":"Use case","text":"<p>MCAD allows you to deploy Ray cluster with a guarantee that sufficient resources are available in the cluster prior to actual pod creation in the Kubernetes cluster. It supports features such as:</p> <ul> <li>Integrates with upstream Kubernetes scheduling stack for features such co-scheduling, Packing on GPU dimension etc.</li> <li>Ability to wrap any Kubernetes objects.</li> <li>Increases control plane stability by JIT (Just-in Time) object creation.</li> <li>Queuing with policies.</li> <li>Quota management that goes across namespaces.</li> <li>Support for multiple Kubernetes clusters; dispatching jobs to any one of a number of Kubernetes clusters.</li> </ul> <p>In order to queue Ray cluster(s) and <code>gang dispatch</code> them when aggregated resources are available please refer to the setup KubeRay-MCAD integration with configuration files here.</p>"},{"location":"guidance/kuberay-with-MCAD/#submitting-kuberay-cluster-to-mcad","title":"Submitting KubeRay cluster to MCAD","text":"<p>Let's submit two Ray clusters on the same Kubernetes cluster.</p> <ul> <li>Assuming you have installed all the pre-requisites mentioned in the KubeRay-MCAD integration, we submit the first Ray cluster using command <code>kubectl create -f aw-raycluster.yaml</code> using config file here.</li> </ul> <pre><code>  Conditions:\n    Last Transition Micro Time:  2022-09-27T21:07:34.252275Z\n    Last Update Micro Time:      2022-09-27T21:07:34.252273Z\n    Status:                      True\n    Type:                        Init\n    Last Transition Micro Time:  2022-09-27T21:07:34.252535Z\n    Last Update Micro Time:      2022-09-27T21:07:34.252534Z\n    Reason:                      AwaitingHeadOfLine\n    Status:                      True\n    Type:                        Queueing\n    Last Transition Micro Time:  2022-09-27T21:07:34.261174Z\n    Last Update Micro Time:      2022-09-27T21:07:34.261174Z\n    Reason:                      FrontOfQueue.\n    Status:                      True\n    Type:                        HeadOfLine\n    Last Transition Micro Time:  2022-09-27T21:07:34.316208Z\n    Last Update Micro Time:      2022-09-27T21:07:34.316208Z\n    Reason:                      AppWrapperRunnable\n    Status:                      True\n    Type:                        Dispatched\n  Controllerfirsttimestamp:      2022-09-27T21:07:34.251877Z\n  Filterignore:                  true\n  Queuejobstate:                 Dispatched\n  Sender:                        before manageQueueJob - afterEtcdDispatching\n  State:                         Running\nEvents:                          &lt;none&gt;\n(base) asmalvan@mcad-dev:~/mcad-kuberay$ kubectl get pods\nNAME                                               READY   STATUS    RESTARTS   AGE\nraycluster-autoscaler-1-head-9s4x5                 2/2     Running   0          47s\nraycluster-autoscaler-1-worker-small-group-4s6jv   1/1     Running   0          47s\n</code></pre> <ul> <li> <p>As seen the cluster is dispatched and pods are running.</p> </li> <li> <p>Let's submit another Ray cluster and see it queued without creating pending pods using the command <code>kubectl create -f aw-raycluster.yaml</code>. To do this, change cluster name from <code>name: raycluster-autoscaler</code> to <code>name: raycluster-autoscaler-1</code> and re-submit</p> </li> </ul> <pre><code>Conditions:\n    Last Transition Micro Time:  2022-09-27T21:11:06.162080Z\n    Last Update Micro Time:      2022-09-27T21:11:06.162080Z\n    Status:                      True\n    Type:                        Init\n    Last Transition Micro Time:  2022-09-27T21:11:06.162401Z\n    Last Update Micro Time:      2022-09-27T21:11:06.162401Z\n    Reason:                      AwaitingHeadOfLine\n    Status:                      True\n    Type:                        Queueing\n    Last Transition Micro Time:  2022-09-27T21:11:06.171619Z\n    Last Update Micro Time:      2022-09-27T21:11:06.171618Z\n    Reason:                      FrontOfQueue.\n    Status:                      True\n    Type:                        HeadOfLine\n    Last Transition Micro Time:  2022-09-27T21:11:06.179694Z\n    Last Update Micro Time:      2022-09-27T21:11:06.179689Z\n    Message:                     Insufficient resources to dispatch AppWrapper.\n    Reason:                      AppWrapperNotRunnable.\n    Status:                      True\n    Type:                        Backoff\n  Controllerfirsttimestamp:      2022-09-27T21:11:06.161797Z\n  Filterignore:                  true\n  Queuejobstate:                 HeadOfLine\n  Sender:                        before ScheduleNext - setHOL\n  State:                         Pending\nEvents:                          &lt;none&gt;\n</code></pre> <ul> <li> <p>As seen the second Ray cluster is queued with no pending pods created. </p> </li> <li> <p>Dispatching policy out of the box is FIFO which can be augmented as per user needs. The second cluster will be dispatched when additional aggregated resources are available in the cluster or the first AppWrapper Ray cluster is deleted.</p> </li> </ul>"},{"location":"guidance/mobilenet-rayservice/","title":"Serve a MobileNet image classifier using RayService","text":"<p>Note: The Python files for the Ray Serve application and its client are in the repository ray-project/serve_config_examples.</p>"},{"location":"guidance/mobilenet-rayservice/#step-1-create-a-kubernetes-cluster-with-kind","title":"Step 1: Create a Kubernetes cluster with Kind.","text":"<pre><code>kind create cluster --image=kindest/node:v1.23.0\n</code></pre>"},{"location":"guidance/mobilenet-rayservice/#step-2-install-kuberay-operator","title":"Step 2: Install KubeRay operator","text":"<p>Follow this document to install the nightly KubeRay operator via  Helm. Note that the YAML file in Step 3 uses <code>serveConfigV2</code>, which is first supported by KubeRay v0.6.0.</p>"},{"location":"guidance/mobilenet-rayservice/#step-3-install-a-rayservice","title":"Step 3: Install a RayService","text":"<pre><code># path: ray-operator/config/samples/\nkubectl apply -f ray-service.mobilenet.yaml\n</code></pre> <ul> <li>The mobilenet.py file requires <code>tensorflow</code> as a dependency. Hence, the YAML file uses <code>rayproject/ray-ml:2.5.0</code> instead of <code>rayproject/ray:2.5.0</code>.</li> <li><code>python-multipart</code> is required for the request parsing function <code>starlette.requests.form()</code>, so the YAML file includes <code>python-multipart</code> in the runtime environment.</li> </ul>"},{"location":"guidance/mobilenet-rayservice/#step-4-forward-the-port-of-serve","title":"Step 4: Forward the port of Serve","text":"<pre><code>kubectl port-forward svc/rayservice-mobilenet-serve-svc 8000\n</code></pre> <p>Note that the Serve service will be created after the Serve applications are ready and running. This process may take approximately 1 minute after all Pods in the RayCluster are running.</p>"},{"location":"guidance/mobilenet-rayservice/#step-5-send-a-request-to-the-imageclassifier","title":"Step 5: Send a request to the ImageClassifier","text":"<ul> <li>Step 5.1: Prepare an image file.</li> <li>Step 5.2: Update <code>image_path</code> in mobilenet_req.py</li> <li>Step 5.3: Send a request to the <code>ImageClassifier</code>.   <pre><code>python mobilenet_req.py\n# sample output: {\"prediction\":[\"n02099601\",\"golden_retriever\",0.17944198846817017]}\n</code></pre></li> </ul>"},{"location":"guidance/observability/","title":"Observability","text":""},{"location":"guidance/observability/#raycluster-status","title":"RayCluster Status","text":""},{"location":"guidance/observability/#state","title":"State","text":"<p>In the RayCluster resource definition, we use <code>State</code> to represent the current status of the Ray cluster.</p> <p>For now, there are three types of the status exposed by the RayCluster's status.state: <code>ready</code>, <code>unhealthy</code> and <code>failed</code>.</p> State Description ready The Ray cluster is ready for use. unhealthy The <code>rayStartParams</code> are misconfigured and the Ray cluster may not function properly. failed A severe issue has prevented the head node or worker nodes from starting. <p>If you use the apiserver to retrieve the resource, you may find the state in the <code>clusterState</code> field.</p> <pre><code>curl --request GET '&lt;baseUrl&gt;/apis/v1alpha2/namespaces/&lt;namespace&gt;/clusters/&lt;raycluster-name&gt;'\n{\n\"name\": \"&lt;raycluster-name&gt;\",\n\"namespace\": \"&lt;namespace&gt;\",\n//...\n\"createdAt\": \"2022-08-10T10:31:25Z\",\n\"clusterState\": \"ready\",\n//...\n}\n</code></pre>"},{"location":"guidance/observability/#endpoint","title":"Endpoint","text":"<p>If you use the nodeport as service to expose the raycluster endpoint, like dashboard or redis, there are <code>endpoints</code> field in the status to record the service endpoints.</p> <p>you can directly use the ports in the <code>endpoints</code> to connect to the related service.</p> <p>Also, if you use apiserver to retrieve the resource, you can find the endpoints in the <code>serviceEndpoint</code> field.</p> <pre><code>curl --request GET '&lt;baseUrl&gt;/apis/v1alpha2/namespaces/&lt;namespace&gt;/clusters/&lt;raycluster-name&gt;'\n{\n\"name\": \"&lt;raycluster-name&gt;\",\n\"namespace\": \"&lt;namespace&gt;\",\n//...\n\"serviceEndpoint\": {\n\"dashboard\": \"30001\",\n\"head\": \"30002\",\n\"metrics\": \"30003\",\n\"redis\": \"30004\"\n},\n//...\n}\n</code></pre>"},{"location":"guidance/observability/#ray-cluster-monitoring-with-prometheus-grafana","title":"Ray Cluster: Monitoring with Prometheus &amp; Grafana","text":"<p>See prometheus-grafana.md for more details.</p>"},{"location":"guidance/pod-command/","title":"Specify container commands for Ray head/worker Pods","text":"<p>You can execute commands on the head/worker pods at two timings:</p> <ul> <li> <p>(1) Before <code>ray start</code>: As an example, you can set up some environment variables that will be used by <code>ray start</code>.</p> </li> <li> <p>(2) After <code>ray start</code> (RayCluster is ready): As an example, you can launch a Ray serve deployment when the RayCluster is ready.</p> </li> </ul>"},{"location":"guidance/pod-command/#current-kuberay-operator-behavior-for-container-commands","title":"Current KubeRay operator behavior for container commands","text":"<ul> <li>The current behavior for container commands is not finalized, and may be updated in the future.</li> <li>See code for more details.</li> </ul>"},{"location":"guidance/pod-command/#timing-1-before-ray-start","title":"Timing 1: Before <code>ray start</code>","text":"<p>Currently, for timing (1), we can set the container's <code>Command</code> and <code>Args</code> in RayCluster specification to reach the goal.</p> <pre><code># ray-operator/config/samples/ray-cluster.head-command.yaml\nrayStartParams:\n...\n#pod template\ntemplate:\nspec:\ncontainers:\n- name: ray-head\nimage: rayproject/ray:2.5.0\nresources:\n...\nports:\n...\n# `command` and `args` will become a part of `spec.containers.0.args` in the head Pod.\ncommand: [\"echo 123\"]\nargs: [\"456\"]\n</code></pre> <ul> <li> <p>Ray head Pod</p> <ul> <li><code>spec.containers.0.command</code> is hardcoded with <code>[\"/bin/bash\", \"-lc\", \"--\"]</code>.</li> <li><code>spec.containers.0.args</code> contains two parts:<ul> <li>(Part 1) user-specified command: A string concatenates <code>headGroupSpec.template.spec.containers.0.command</code> from RayCluster and <code>headGroupSpec.template.spec.containers.0.args</code> from RayCluster together.</li> <li>(Part 2) ray start command: The command is created based on <code>rayStartParams</code> specified in RayCluster. The command will look like <code>ulimit -n 65536; ray start ...</code>.</li> <li>To summarize, <code>spec.containers.0.args</code> will be <code>$(user-specified command) &amp;&amp; $(ray start command)</code>.</li> </ul> </li> </ul> </li> <li> <p>Example     <pre><code># Prerequisite: There is a KubeRay operator in the Kubernetes cluster.\n\n# Path: kuberay/\nkubectl apply -f ray-operator/config/samples/ray-cluster.head-command.yaml\n\n# Check ${RAYCLUSTER_HEAD_POD}\nkubectl get pod -l ray.io/node-type=head\n\n# Check `spec.containers.0.command` and `spec.containers.0.args`.\nkubectl describe pod ${RAYCLUSTER_HEAD_POD}\n\n# Command:\n#   /bin/bash\n#   -lc\n#   --\n# Args:\n#    echo 123  456  &amp;&amp; ulimit -n 65536; ray start --head  --dashboard-host=0.0.0.0  --num-cpus=1  --block  --metrics-export-port=8080  --memory=2147483648\n</code></pre></p> </li> </ul>"},{"location":"guidance/pod-command/#timing-2-after-ray-start-raycluster-is-ready","title":"Timing 2: After <code>ray start</code> (RayCluster is ready)","text":"<p>We have two solutions to execute commands after the RayCluster is ready. The main difference between these two solutions is users can check the logs via <code>kubectl logs</code> with Solution 1.</p>"},{"location":"guidance/pod-command/#solution-1-container-command-recommended","title":"Solution 1: Container command (Recommended)","text":"<p>As we mentioned in the section \"Timing 1: Before <code>ray start</code>\", user-specified command will be executed before the <code>ray start</code> command. Hence, we can execute the <code>ray_cluster_resources.sh</code> in background by updating <code>headGroupSpec.template.spec.containers.0.command</code> in <code>ray-cluster.head-command.yaml</code>.</p> <pre><code># ray-operator/config/samples/ray-cluster.head-command.yaml\n# Parentheses for the command is required.\ncommand: [\"(/home/ray/samples/ray_cluster_resources.sh&amp;)\"]\n\n# ray_cluster_resources.sh\napiVersion: v1\nkind: ConfigMap\nmetadata:\nname: ray-example\ndata:\nray_cluster_resources.sh: |\n#!/bin/bash\n\n# wait for ray cluster to finish initialization\nwhile true; do\nray health-check 2&gt;/dev/null\nif [ \"$?\" = \"0\" ]; then\nbreak\nelse\necho \"INFO: waiting for ray head to start\"\nsleep 1\nfi\ndone\n\n# Print the resources in the ray cluster after the cluster is ready.\npython -c \"import ray; ray.init(); print(ray.cluster_resources())\"\n\necho \"INFO: Print Ray cluster resources\"\n</code></pre> <ul> <li>Example     <pre><code># Path: kuberay/\n# (1) Update `command` to [\"(/home/ray/samples/ray_cluster_resources.sh&amp;)\"]\n# (2) Comment out `postStart` and `args`.\nkubectl apply -f ray-operator/config/samples/ray-cluster.head-command.yaml\n\n# Check ${RAYCLUSTER_HEAD_POD}\nkubectl get pod -l ray.io/node-type=head\n\n# Check the logs\nkubectl logs ${RAYCLUSTER_HEAD_POD}\n\n# INFO: waiting for ray head to start\n# .\n# . =&gt; Cluster initialization\n# .\n# 2023-02-16 18:44:43,724 INFO worker.py:1231 -- Using address 127.0.0.1:6379 set in the environment variable RAY_ADDRESS\n# 2023-02-16 18:44:43,724 INFO worker.py:1352 -- Connecting to existing Ray cluster at address: 10.244.0.26:6379...\n# 2023-02-16 18:44:43,735 INFO worker.py:1535 -- Connected to Ray cluster. View the dashboard at http://10.244.0.26:8265\n# {'object_store_memory': 539679129.0, 'node:10.244.0.26': 1.0, 'CPU': 1.0, 'memory': 2147483648.0}\n# INFO: Print Ray cluster resources\n</code></pre></li> </ul>"},{"location":"guidance/pod-command/#solution-2-poststart-hook","title":"Solution 2: postStart hook","text":"<pre><code># ray-operator/config/samples/ray-cluster.head-command.yaml\nlifecycle:\npostStart:\nexec:\ncommand: [\"/bin/sh\",\"-c\",\"/home/ray/samples/ray_cluster_resources.sh\"]\n</code></pre> <ul> <li> <p>We execute the script <code>ray_cluster_resources.sh</code> via the postStart hook. Based on this document, there is no guarantee that the hook will execute before the container ENTRYPOINT. Hence, we need to wait for RayCluster to finish initialization in <code>ray_cluster_resources.sh</code>.</p> </li> <li> <p>Example     <pre><code># Path: kuberay/\nkubectl apply -f ray-operator/config/samples/ray-cluster.head-command.yaml\n\n# Check ${RAYCLUSTER_HEAD_POD}\nkubectl get pod -l ray.io/node-type=head\n\n# Forward the port of Dashboard\nkubectl port-forward --address 0.0.0.0 ${RAYCLUSTER_HEAD_POD} 8265:8265\n\n# Open the browser and check the Dashboard (${YOUR_IP}:8265/#/job).\n# You shold see a SUCCEEDED job with the following Entrypoint:\n#\n# `python -c \"import ray; ray.init(); print(ray.cluster_resources())\"`\n</code></pre></p> </li> </ul>"},{"location":"guidance/pod-security/","title":"Pod Security","text":"<p>Kubernetes defines three different Pod Security Standards, including <code>privileged</code>, <code>baseline</code>, and <code>restricted</code>, to broadly cover the security spectrum. The <code>privileged</code> standard allows users to do known privilege escalations, and thus it is not  safe enough for security-critical applications.</p> <p>This document describes how to configure RayCluster YAML file to apply <code>restricted</code> Pod security standard. The following  references can help you understand this document better:</p> <ul> <li>Kubernetes - Pod Security Standards</li> <li>Kubernetes - Pod Security Admission</li> <li>Kubernetes - Auditing</li> <li>KinD - Auditing</li> </ul>"},{"location":"guidance/pod-security/#step-1-create-a-kind-cluster","title":"Step 1: Create a KinD cluster","text":"<p><pre><code># Path: kuberay/\nkind create cluster --config ray-operator/config/security/kind-config.yaml --image=kindest/node:v1.24.0\n</code></pre> The <code>kind-config.yaml</code> enables audit logging with the audit policy defined in <code>audit-policy.yaml</code>. The <code>audit-policy.yaml</code> defines an auditing policy to listen to the Pod events in the namespace <code>pod-security</code>. With this policy, we can check whether our Pods violate the policies in <code>restricted</code> standard or not.</p> <p>The feature Pod Security Admission is firstly  introduced in Kubernetes v1.22 (alpha) and becomes stable in Kubernetes v1.25. In addition, KubeRay currently supports  Kubernetes from v1.19 to v1.24. (At the time of writing, we have not tested KubeRay with Kubernetes v1.25). Hence, I use Kubernetes v1.24 in this step.</p>"},{"location":"guidance/pod-security/#step-2-check-the-audit-logs","title":"Step 2: Check the audit logs","text":"<p><pre><code>docker exec kind-control-plane cat /var/log/kubernetes/kube-apiserver-audit.log\n</code></pre> The log should be empty because the namespace <code>pod-security</code> does not exist.</p>"},{"location":"guidance/pod-security/#step-3-create-the-pod-security-namespace","title":"Step 3: Create the <code>pod-security</code> namespace","text":"<p><pre><code>kubectl create ns pod-security\nkubectl label --overwrite ns pod-security \\\npod-security.kubernetes.io/warn=restricted \\\npod-security.kubernetes.io/warn-version=latest \\\npod-security.kubernetes.io/audit=restricted \\\npod-security.kubernetes.io/audit-version=latest \\\npod-security.kubernetes.io/enforce=restricted \\\npod-security.kubernetes.io/enforce-version=latest\n</code></pre> With the <code>pod-security.kubernetes.io</code> labels, the built-in Kubernetes Pod security admission controller will apply the  <code>restricted</code> Pod security standard to all Pods in the namespace <code>pod-security</code>. The label <code>pod-security.kubernetes.io/enforce=restricted</code> means that the Pod will be rejected if it violate the policies defined in  <code>restricted</code> security standard. See Pod Security Admission for more details about the labels.</p>"},{"location":"guidance/pod-security/#step-4-install-the-kuberay-operator","title":"Step 4: Install the KubeRay operator","text":"<pre><code># Update the field securityContext in helm-chart/kuberay-operator/values.yaml\nsecurityContext:\n  allowPrivilegeEscalation: false\ncapabilities:\n    drop: [\"ALL\"]\nrunAsNonRoot: true\nseccompProfile:\n    type: RuntimeDefault\n\n# Path: kuberay/helm-chart/kuberay-operator\nhelm install -n pod-security kuberay-operator .\n</code></pre>"},{"location":"guidance/pod-security/#step-5-create-a-raycluster-choose-either-step-51-or-step-52","title":"Step 5: Create a RayCluster (Choose either Step 5.1 or Step 5.2)","text":"<ul> <li>If you choose Step 5.1, no Pod will be created in the namespace <code>pod-security</code>.</li> <li>If you choose Step 5.2, Pods can be created successfully.</li> </ul>"},{"location":"guidance/pod-security/#step-51-create-a-raycluster-without-proper-securitycontext-configurations","title":"Step 5.1: Create a RayCluster without proper <code>securityContext</code> configurations","text":"<p><pre><code># Path: kuberay/ray-operator/config/samples\nkubectl apply -n pod-security -f ray-cluster.complete.yaml\n\n# Wait 20 seconds and check audit logs for the error messages.\ndocker exec kind-control-plane cat /var/log/kubernetes/kube-apiserver-audit.log\n\n# Example error messagess\n# \"pods \\\"raycluster-complete-head-fkbf5\\\" is forbidden: violates PodSecurity \\\"restricted:latest\\\": allowPrivilegeEscalation != false (container \\\"ray-head\\\" must set securityContext.allowPrivilegeEscalation=false) ...\n\nkubectl get pod -n pod-security\n# NAME                               READY   STATUS    RESTARTS   AGE\n# kuberay-operator-8b6d55dbb-t8msf   1/1     Running   0          62s\n\n# Clean up the RayCluster\nkubectl delete rayclusters.ray.io -n pod-security raycluster-complete\n# raycluster.ray.io \"raycluster-complete\" deleted\n</code></pre> No Pod will be created in the namespace <code>pod-security</code>, and check audit logs for error messages.</p>"},{"location":"guidance/pod-security/#step-52-create-a-raycluster-with-proper-securitycontext-configurations","title":"Step 5.2: Create a RayCluster with proper <code>securityContext</code> configurations","text":"<p><pre><code># Path: kuberay/ray-operator/config/security\nkubectl apply -n pod-security -f ray-cluster.pod-security.yaml\n\n# Wait for the RayCluster convergence and check audit logs for the messages.\ndocker exec kind-control-plane cat /var/log/kubernetes/kube-apiserver-audit.log\n\n# Forward the dashboard port\nkubectl port-forward --address 0.0.0.0 svc/raycluster-pod-security-head-svc -n pod-security 8265:8265\n\n# Log in to the head Pod\nkubectl exec -it -n pod-security ${YOUR_HEAD_POD} -- bash\n\n# (Head Pod) Run a sample job in the Pod\npython3 samples/xgboost_example.py\n\n# Check the job status in the dashboard on your browser.\n# http://127.0.0.1:8265/#/job =&gt; The job status should be \"SUCCEEDED\".\n\n# (Head Pod) Make sure Python dependencies can be installed under `restricted` security standard \npip3 install jsonpatch\necho $? # Check the exit code of `pip3 install jsonpatch`. It should be 0.\n\n# Clean up the RayCluster\nkubectl delete -n pod-security -f ray-cluster.pod-security.yaml\n# raycluster.ray.io \"raycluster-pod-security\" deleted\n# configmap \"xgboost-example\" deleted\n</code></pre> One head Pod and one worker Pod will be created as specified in <code>ray-cluster.pod-security.yaml</code>. First, we log in to the head Pod, run a XGBoost example script, and check the job status in the dashboard. Next, we use <code>pip</code> to install a Python dependency (i.e. <code>jsonpatch</code>), and the exit code of the <code>pip</code> command should be 0.</p>"},{"location":"guidance/prometheus-grafana/","title":"Ray Cluster: Monitoring with Prometheus &amp; Grafana","text":"<p>This section will describe how to monitor Ray Clusters in Kubernetes using Prometheus &amp; Grafana.</p> <p>If you do not have any experience with Prometheus and Grafana on Kubernetes, I strongly recommend you watch this YouTube playlist.</p>"},{"location":"guidance/prometheus-grafana/#step-1-create-a-kubernetes-cluster-with-kind","title":"Step 1: Create a Kubernetes cluster with Kind.","text":"<pre><code>kind create cluster\n</code></pre>"},{"location":"guidance/prometheus-grafana/#step-2-install-kubernetes-prometheus-stack-via-helm-chart","title":"Step 2: Install Kubernetes Prometheus Stack via Helm chart","text":"<pre><code># Path: kuberay/\n./install/prometheus/install.sh\n\n# Check the installation\nkubectl get all -n prometheus-system\n\n# (part of the output)\n# NAME                                                  READY   UP-TO-DATE   AVAILABLE   AGE\n# deployment.apps/prometheus-grafana                    1/1     1            1           46s\n# deployment.apps/prometheus-kube-prometheus-operator   1/1     1            1           46s\n# deployment.apps/prometheus-kube-state-metrics         1/1     1            1           46s\n</code></pre> <ul> <li>KubeRay provides an install.sh script to install the kube-prometheus-stack chart and related custom resources, including ServiceMonitor, PodMonitor and PrometheusRule, in the namespace <code>prometheus-system</code> automatically.</li> </ul>"},{"location":"guidance/prometheus-grafana/#step-3-install-a-kuberay-operator","title":"Step 3: Install a KubeRay operator","text":"<ul> <li>Follow this document to install the latest stable KubeRay operator via Helm repository.</li> </ul>"},{"location":"guidance/prometheus-grafana/#step-4-install-a-raycluster","title":"Step 4: Install a RayCluster","text":"<pre><code>helm install raycluster kuberay/ray-cluster --version 0.5.0\n\n# Check ${RAYCLUSTER_HEAD_POD}\nkubectl get pod -l ray.io/node-type=head\n\n# Example output:\n# NAME                            READY   STATUS    RESTARTS   AGE\n# raycluster-kuberay-head-btwc2   1/1     Running   0          63s\n\n# Wait until all Ray Pods are running and forward the port of the Prometheus metrics endpoint in a new terminal.\nkubectl port-forward --address 0.0.0.0 ${RAYCLUSTER_HEAD_POD} 8080:8080\ncurl localhost:8080\n\n# Example output (Prometheus metrics format):\n# # HELP ray_spill_manager_request_total Number of {spill, restore} requests.\n# # TYPE ray_spill_manager_request_total gauge\n# ray_spill_manager_request_total{Component=\"raylet\",NodeAddress=\"10.244.0.13\",Type=\"Restored\",Version=\"2.0.0\"} 0.0\n\n# Ensure that the port (8080) for the metrics endpoint is also defined in the head's Kubernetes service.\nkubectl get service\n\n# NAME                          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                                         AGE\n# raycluster-kuberay-head-svc   ClusterIP   10.96.201.142   &lt;none&gt;        6379/TCP,8265/TCP,8080/TCP,8000/TCP,10001/TCP   106m\n</code></pre> <ul> <li>KubeRay will expose a Prometheus metrics endpoint in port 8080 via a built-in exporter by default. Hence, we do not need to install any external exporter.</li> <li>If you want to configure the metrics endpoint to a different port, see kuberay/#954 for more details.</li> <li>Prometheus metrics format:</li> <li><code># HELP</code>: Describe the meaning of this metric.</li> <li><code># TYPE</code>: See this document for more details.</li> </ul>"},{"location":"guidance/prometheus-grafana/#step-5-collect-head-node-metrics-with-a-servicemonitor","title":"Step 5: Collect Head Node metrics with a ServiceMonitor","text":"<pre><code>apiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\nname: ray-head-monitor\nnamespace: prometheus-system\nlabels:\n# `release: $HELM_RELEASE`: Prometheus can only detect ServiceMonitor with this label.\nrelease: prometheus\nspec:\njobLabel: ray-head\n# Only select Kubernetes Services in the \"default\" namespace.\nnamespaceSelector:\nmatchNames:\n- default\n# Only select Kubernetes Services with \"matchLabels\".\nselector:\nmatchLabels:\nray.io/node-type: head\n# A list of endpoints allowed as part of this ServiceMonitor.\nendpoints:\n- port: metrics\ntargetLabels:\n- ray.io/cluster\n</code></pre> <ul> <li>The YAML example above is serviceMonitor.yaml, and it is created by install.sh. Hence, no need to create anything here.</li> <li>See ServiceMonitor official document for more details about the configurations.</li> <li><code>release: $HELM_RELEASE</code>: Prometheus can only detect ServiceMonitor with this label.</li> </ul> <pre><code>helm ls -n prometheus-system\n# ($HELM_RELEASE is \"prometheus\".)\n# NAME            NAMESPACE               REVISION        UPDATED                                 STATUS          CHART                           APP VERSION\n# prometheus      prometheus-system       1               2023-02-06 06:27:05.530950815 +0000 UTC deployed        kube-prometheus-stack-44.3.1    v0.62.0\n\nkubectl get prometheuses.monitoring.coreos.com -n prometheus-system -oyaml\n# serviceMonitorSelector:\n#   matchLabels:\n#     release: prometheus\n# podMonitorSelector:\n#   matchLabels:\n#     release: prometheus\n# ruleSelector:\n#   matchLabels:\n#     release: prometheus\n</code></pre> <ul> <li> <p><code>namespaceSelector</code> and <code>seletor</code> are used to select exporter's Kubernetes service. Because Ray uses a built-in exporter, the ServiceMonitor selects Ray's head service which exposes the metrics endpoint (i.e. port 8080 here).   <pre><code>kubectl get service -n default -l ray.io/node-type=head\n# NAME                          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                                         AGE\n# raycluster-kuberay-head-svc   ClusterIP   10.96.201.142   &lt;none&gt;        6379/TCP,8265/TCP,8080/TCP,8000/TCP,10001/TCP   153m\n</code></pre></p> </li> <li> <p><code>targetLabels</code>: We added <code>spec.targetLabels[0].ray.io/cluster</code> because we want to include the name of the RayCluster in the metrics that will be generated by this ServiceMonitor. The <code>ray.io/cluster</code> label is part of the Ray head node service and it will be transformed into a <code>ray_io_cluster</code> metric label. That is, any metric that will be imported, will also contain the following label <code>ray_io_cluster=&lt;ray-cluster-name&gt;</code>. This may seem optional but it becomes mandatory if you deploy multiple RayClusters.</p> </li> </ul>"},{"location":"guidance/prometheus-grafana/#step-6-collect-worker-node-metrics-with-podmonitors","title":"Step 6: Collect Worker Node metrics with PodMonitors","text":"<p>KubeRay operator does not create a Kubernetes service for the Ray worker Pods, therefore we cannot use a Prometheus ServiceMonitor to scrape the metrics from the worker Pods. To collect worker metrics, we can use <code>Prometheus PodMonitors CRD</code> instead.</p> <p>Note: We could create a Kubernetes service with selectors a common label subset from our worker pods, however, this is not ideal because our workers are independent from each other, that is, they are not a collection of replicas spawned by replicaset controller. Due to that, we should avoid using a Kubernetes service for grouping them together.</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: PodMonitor\nmetadata:\nname: ray-workers-monitor\nnamespace: prometheus-system\nlabels:\n# `release: $HELM_RELEASE`: Prometheus can only detect PodMonitor with this label.\nrelease: prometheus\nray.io/cluster: raycluster-kuberay # $RAY_CLUSTER_NAME: \"kubectl get rayclusters.ray.io\"\nspec:\njobLabel: ray-workers\n# Only select Kubernetes Pods in the \"default\" namespace.\nnamespaceSelector:\nmatchNames:\n- default\n# Only select Kubernetes Pods with \"matchLabels\".\nselector:\nmatchLabels:\nray.io/node-type: worker\n# A list of endpoints allowed as part of this PodMonitor.\npodMetricsEndpoints:\n- port: metrics\n</code></pre> <ul> <li> <p><code>release: $HELM_RELEASE</code>: Prometheus can only detect PodMonitor with this label. See here for more details.</p> </li> <li> <p>PodMonitor in <code>namespaceSelector</code> and <code>selector</code> are used to select Kubernetes Pods.   <pre><code>kubectl get pod -n default -l ray.io/node-type=worker\n# NAME                                          READY   STATUS    RESTARTS   AGE\n# raycluster-kuberay-worker-workergroup-5stpm   1/1     Running   0          3h16m\n</code></pre></p> </li> <li> <p><code>ray.io/cluster: $RAY_CLUSTER_NAME</code>: We also define <code>metadata.labels</code> by manually adding <code>ray.io/cluster: &lt;ray-cluster-name&gt;</code> and then instructing the PodMonitors resource to add that label in the scraped metrics via <code>spec.podTargetLabels[0].ray.io/cluster</code>.</p> </li> </ul>"},{"location":"guidance/prometheus-grafana/#step-7-collect-custom-metrics-with-recording-rules","title":"Step 7: Collect custom metrics with Recording Rules","text":"<p>Recording Rules allow us to precompute frequently needed or computationally expensive PromQL expressions and save their result as custom metrics. Note this is different from Custom Application-level Metrics which aim for the visibility of ray applications.</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\nname: ray-cluster-gcs-rules\nnamespace: prometheus-system\nlabels:\n# `release: $HELM_RELEASE`: Prometheus can only detect Recording Rules with this label.\nrelease: prometheus\nspec:\ngroups:\n- #  Rules within a group are run periodically with the same evaluation interval(30s in this example).\nname: ray-cluster-main-staging-gcs.rules\n# How often rules in the group are evaluated.\ninterval: 30s\nrules:\n- # The name of the custom metric.\n# Also see best practices for naming metrics created by recording rules:\n# https://prometheus.io/docs/practices/rules/#recording-rules\nrecord: ray_gcs_availability_30d\n# PromQL expression.\nexpr: |\n(\n100 * (\nsum(rate(ray_gcs_update_resource_usage_time_bucket{container=\"ray-head\", le=\"20.0\"}[30d]))\n/\nsum(rate(ray_gcs_update_resource_usage_time_count{container=\"ray-head\"}[30d]))\n)\n)\n</code></pre> <ul> <li> <p>The PromQL expression above is:  $$\\frac{ number of update resource usage RPCs that have RTT smaller then 20ms in last 30 days }{total number of update resource usage RPCs in last 30 days }   \\times 100 $$  </p> </li> <li> <p>The recording rule above is one of rules defined in prometheusRules.yaml, and it is created by install.sh. Hence, no need to create anything here.</p> </li> <li> <p>See PrometheusRule official document for more details about the configurations.</p> </li> <li> <p><code>release: $HELM_RELEASE</code>: Prometheus can only detect PrometheusRule with this label. See here for more details.</p> </li> <li> <p>PrometheusRule can be reloaded at runtime. Use <code>kubectl apply {modified prometheusRules.yaml}</code> to reconfigure the rules if needed.</p> </li> </ul>"},{"location":"guidance/prometheus-grafana/#step-8-define-alert-conditions-with-alerting-rules","title":"Step 8: Define Alert Conditions with Alerting Rules","text":"<p>Alerting rules allow us to define alert conditions based on PromQL expressions and to send notifications about firing alerts to Alertmanager which adds summarization, notification rate limiting, silencing and alert dependencies on top of the simple alert definitions.</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\nname: ray-cluster-gcs-rules\nnamespace: prometheus-system\nlabels:\n# `release: $HELM_RELEASE`: Prometheus can only detect Alerting Rules with this label.\nrelease: prometheus\nspec:\ngroups:\n- name: ray-cluster-main-staging-gcs.rules\n# How often rules in the group are evaluated.\ninterval: 30s\nrules:\n- alert: MissingMetricRayGlobalControlStore\n# A set of informational labels. Annotations can be used to store longer additional information compared to rules.0.labels.\nannotations:\ndescription: Ray GCS is not emitting any metrics for Resource Update requests\nsummary: Ray GCS is not emitting metrics anymore\n# PromQL expression.\nexpr: |\n(\nabsent(ray_gcs_update_resource_usage_time_bucket) == 1\n)\n# Time that Prometheus will wait and check if the alert continues to be active during each evaluation before firing the alert.\n# firing alerts may be due to false positives or noise if the setting value is too small.\n# On the other hand, if the value is too big, the alerts may not be handled in time.\nfor: 5m\n# A set of additional labels to be attached to the alert.\n# It is possible to overwrite the labels in metadata.labels, so make sure one of the labels match the label in ruleSelector.matchLabels.\nlabels:\nseverity: critical\n</code></pre> <ul> <li> <p>The PromQL expression above checks if there is no time series exist for <code>ray_gcs_update_resource_usage_time_bucket</code> metric. See absent() for more detail.</p> </li> <li> <p>The alerting rule above is one of rules defined in prometheusRules.yaml, and it is created by install.sh. Hence, no need to create anything here.</p> </li> <li> <p>Alerting rules are configured in the same way as recording rules.</p> </li> </ul>"},{"location":"guidance/prometheus-grafana/#step-9-access-prometheus-web-ui","title":"Step 9: Access Prometheus Web UI","text":"<pre><code># Forward the port of Prometheus Web UI in the Prometheus server Pod.\nkubectl port-forward --address 0.0.0.0 prometheus-prometheus-kube-prometheus-prometheus-0 -n prometheus-system 9090:9090\n</code></pre> <ul> <li>Go to <code>${YOUR_IP}:9090/targets</code> (e.g. <code>127.0.0.1:9090/targets</code>). You should be able to see:</li> <li><code>podMonitor/prometheus-system/ray-workers-monitor/0 (1/1 up)</code></li> <li> <p><code>serviceMonitor/prometheus-system/ray-head-monitor/0 (1/1 up)</code> </p> </li> <li> <p>Go to <code>${YOUR_IP}:9090/graph</code>. You should be able to query:</p> </li> <li>System Metrics</li> <li>Application Level Metrics</li> <li> <p>Custom Metrics defined in Recording Rules (e.g. <code>ray_gcs_availability_30d</code>)</p> </li> <li> <p>Go to <code>${YOUR_IP}:9090/alerts</code>. You should be able to see:</p> </li> <li>Alerting Rules (e.g. <code>MissingMetricRayGlobalControlStore</code>).</li> </ul>"},{"location":"guidance/prometheus-grafana/#step-10-access-grafana","title":"Step 10: Access Grafana","text":"<pre><code># Forward the port of Grafana\nkubectl port-forward --address 0.0.0.0 deployment/prometheus-grafana -n prometheus-system 3000:3000\n\n# Check ${YOUR_IP}:3000 for the Grafana login page (e.g. 127.0.0.1:3000).\n# The default username is \"admin\" and the password is \"prom-operator\".\n</code></pre> <ul> <li> <p>The default password is defined by <code>grafana.adminPassword</code> in the values.yaml of the kube-prometheus-stack chart.</p> </li> <li> <p>After logging in to Grafana successfully, we can import Ray Dashboard into Grafana via dashboard_default.json.</p> </li> <li>Click \"Dashboards\" icon in the left panel.</li> <li>Click \"Import\".</li> <li>Click \"Upload JSON file\".</li> <li>Choose config/grafana/dashboard_default.json.</li> <li>Click \"Import\".</li> </ul> <p></p>"},{"location":"guidance/rayStartParams/","title":"rayStartParams","text":""},{"location":"guidance/rayStartParams/#default-ray-start-parameters-for-kuberay","title":"Default Ray Start Parameters for KubeRay","text":"<p>This document outlines the default settings for <code>rayStartParams</code> in KubeRay.</p>"},{"location":"guidance/rayStartParams/#options-exclusive-to-the-head-pod","title":"Options Exclusive to the Head Pod","text":"<ul> <li> <p><code>--dashboard-host</code>: Host for the dashboard server, either <code>localhost</code> (127.0.0.1) or <code>0.0.0.0</code>. The latter setting exposes the Ray dashboard outside the Ray cluster, which is required when ingress is utilized for Ray cluster access. The default value for both Ray and KubeRay 0.5.0 is <code>localhost</code>. Please note that this will change for versions of KubeRay later than 0.5.0, where the default setting will be <code>0.0.0.0</code>.</p> </li> <li> <p><code>--no-monitor</code> (Modification is not recommended):</p> </li> <li>Ray autoscaler supports various node providers such as AWS, GCP, Azure, and Kubernetes. However, the default autoscaler is not compatible with Kubernetes. Therefore, when KubeRay autoscaling is enabled (i.e. <code>EnableInTreeAutoscaling</code> is true), KubeRay disables the monitor process via setting <code>--no-monitor</code> to true and injects a sidecar container for KubeRay autoscaler. See PR #13505 for more details.</li> <li> <p>Please note that the monitor process serves not only for autoscaling but also for observability, such as Prometheus metrics. Considering this, it is reasonable to disable the Kubernetes-incompatible autoscaler regardless of the value of <code>EnableInTreeAutoscaling</code>. To achieve this, we can launch the monitor process without autoscaling functionality by setting the autoscaler to READONLY mode. If <code>autoscaling-option</code> is not set, the autoscaler will default to READONLY mode.</p> </li> <li> <p><code>--port</code>: Port for the GCS server. The port is set to <code>6379</code> by default. Please ensure that this value matches the <code>gcs-server</code> container port in Ray head container.</p> </li> <li> <p><code>--redis-password</code>: Redis password for an external Redis, necessary when fault tolerance is enabled.  The default value is <code>\"\"</code> after Ray 2.3.0. See #929 for more details. </p> </li> </ul>"},{"location":"guidance/rayStartParams/#options-exclusive-to-the-worker-pods","title":"Options Exclusive to the worker Pods","text":"<ul> <li><code>--address</code>: Address of the GCS server. Worker pods utilize this address to establish a connection with the Ray cluster. By default, this address takes the form <code>&lt;FQDN&gt;:&lt;GCS_PORT&gt;</code>. The <code>GCS_PORT</code> corresponds to the value set in the <code>--port</code> option. For more insights on Fully Qualified Domain Name (FQDN), refer to PR #938 and PR #951.</li> </ul>"},{"location":"guidance/rayStartParams/#options-applicable-to-both-head-and-worker-pods","title":"Options Applicable to Both Head and Worker Pods","text":"<ul> <li> <p><code>--block</code>: This option blocks the ray start command indefinitely. It will be automatically set by KubeRay. See PR #675 for more details. Modification is not recommended.</p> </li> <li> <p><code>--memory</code>: Amount of memory on this Ray node. Default is determined by Ray container resource limits. Modify Ray container resource limits instead of this option. See PR #170.</p> </li> <li> <p><code>--metrics-export-port</code>: Port for exposing Ray metrics through a Prometheus endpoint. The port is set to <code>8080</code> by default. Please ensure that this value matches the <code>metrics</code> container port if you need to customize it. See PR #954 and prometheus-grafana doc for more details.</p> </li> <li> <p><code>--num-cpus</code>: Number of logical CPUs on this Ray node. Default is determined by Ray container resource limits. Modify Ray container resource limits instead of this option. See PR #170. However, it is sometimes useful to override this autodetected value. For example, setting <code>num-cpus:\"0\"</code> for the Ray head pod will prevent Ray workloads with non-zero CPU requirements from being scheduled on the head.</p> </li> <li> <p><code>--num-gpus</code>: Number of GPUs on this Ray node. Default is determined by Ray container resource limits. Modify Ray container resource limits instead of this option. See PR #170.</p> </li> </ul>"},{"location":"guidance/rayclient-nginx-ingress/","title":"Connect to Rayclient with NGINX Ingress","text":"<p>Warning: Ray client has some known limitations and is not actively maintained.</p> <p>This document provides an example for connecting Ray client to a Raycluster via NGINX Ingress on Kind. Although this is a Kind example, the steps applies to any Kubernetes Cluster that runs the NGINX Ingress Controller.</p>"},{"location":"guidance/rayclient-nginx-ingress/#requirements","title":"Requirements","text":"<ul> <li> <p>Environment:</p> <ul> <li><code>Ubuntu</code></li> <li><code>Kind</code></li> </ul> </li> <li> <p>Computing resources:</p> <ul> <li>16GB RAM</li> <li>8 CPUs</li> </ul> </li> </ul>"},{"location":"guidance/rayclient-nginx-ingress/#step-1-create-a-kind-cluster","title":"Step 1: Create a Kind cluster","text":"<p>The extra arg prepares the Kind cluster for deploying the ingress controller <pre><code>cat &lt;&lt;EOF | kind create cluster --config=-\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  kubeadmConfigPatches:\n  - |\n    kind: InitConfiguration\n    nodeRegistration:\n      kubeletExtraArgs:\n        node-labels: \"ingress-ready=true\"\n  extraPortMappings:\n  - containerPort: 80\n    hostPort: 80\n    protocol: TCP\n  - containerPort: 443\n    hostPort: 443\n    protocol: TCP\nEOF\n</code></pre></p>"},{"location":"guidance/rayclient-nginx-ingress/#step-2-deploy-nginx-ingress-controller","title":"Step 2: Deploy NGINX Ingress Controller","text":"<p>The SSL Passthrough feature is required to pass on the encryption to the backend service directly. <pre><code># Deploy the NGINX Ingress Controller\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml\n\n# Turn on SSL Passthrough\nkubectl patch deploy --type json --patch '[{\"op\":\"add\",\"path\": \"/spec/template/spec/containers/0/args/-\",\"value\":\"--enable-ssl-passthrough\"}]' ingress-nginx-controller -n ingress-nginx\n\n# Verify log has Starting TLS proxy for SSL Passthrough\nkubectl logs deploy/ingress-nginx-controller -n ingress-nginx\n</code></pre></p>"},{"location":"guidance/rayclient-nginx-ingress/#step-3-install-kuberay-operator","title":"Step 3: Install KubeRay operator","text":"<p>Follow this document to install the latest stable KubeRay operator via Helm repository.</p>"},{"location":"guidance/rayclient-nginx-ingress/#step-4-create-a-raycluster-with-tls-enabled","title":"Step 4: Create a Raycluster with TLS enabled","text":"<p>The Ray client server is a GRPC service. The NGINX Ingress Controller supports GRPC backend service which uses http/2 and requires secured connection. The command below creates a Raycluster with TLS enabled: <pre><code>kubectl apply -f https://raw.githubusercontent.com/ray-project/kuberay/master/ray-operator/config/samples/ray-cluster.tls.yaml\n</code></pre> Refer to the TLS document for more detail.</p>"},{"location":"guidance/rayclient-nginx-ingress/#step-5-create-an-ingress-for-the-ray-client-service","title":"Step 5: Create an ingress for the Ray client service","text":"<p>With the Raycluster running, create an ingress for the Ray client backend service using the rayclient-ingress example below: <pre><code>cat &lt;&lt; EOF | kubectl apply -f -\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: rayclient-ingress\n  annotations:\n    kubernetes.io/ingress.class: nginx\n    nginx.ingress.kubernetes.io/ssl-passthrough: \"true\"\n    nginx.ingress.kubernetes.io/backend-protocol: \"GRPC\"\nspec:\n  rules:\n    - host: \"localhost\"\n      http:\n        paths:\n        - path: /\n          pathType: Prefix\n          backend:\n            service:\n              name: raycluster-tls-head-svc\n              port:\n                number: 10001\nEOF\n</code></pre> The annotation, <code>nginx.ingress.kubernetes.io/backend-protocol: \"GRPC\"</code> sets up the appropriate NGINX configuration to route http/2 traffic to a GRPC backend service. The <code>nginx.ingress.kubernetes.io/ssl-passthrough: \"true\"</code> annotation tells the ingress to forward the encrypted traffic to the backend service to be handled inside the Raycluster itself.</p>"},{"location":"guidance/rayclient-nginx-ingress/#step-6-connecting-to-ray-client-service-via-the-ingress","title":"Step 6: Connecting to Ray client service via the ingress","text":"<p>Since the Raycluster uses TLS, the local Ray client would require a set of certificates to connect to Raycluster.</p> <p>Warning: Ray client has some known limitations and is not actively maintained. <pre><code># Download the ca key pair and create a cert signing request (CSR)\nkubectl get secret ca-tls -o template='{{index .data \"ca.key\"}}'|base64 -d &gt; ./ca.key\nkubectl get secret ca-tls -o template='{{index .data \"ca.crt\"}}'|base64 -d &gt; ./ca.crt\nopenssl req -nodes -newkey rsa:2048 -keyout ./tls.key -out ./tls.csr -subj '/CN=local'\ncat &lt;&lt;EOF &gt;./cert.conf\nauthorityKeyIdentifier=keyid,issuer\nbasicConstraints=CA:FALSE\nsubjectAltName = @alt_names\n[alt_names]\nDNS.1 = localhost\nIP.1 = 127.0.0.1\nEOF\n# Sign and create a tls cert\nopenssl x509 -req -CA ./ca.crt -CAkey ./ca.key -in ./tls.csr -out ./tls.crt -days 365 -CAcreateserial -extfile ./cert.conf\n\n# Connect Ray client to the Raycluster using the tls keypair and the ca cert\npython -c '\nimport os\nimport ray\nos.environ[\"RAY_USE_TLS\"] = \"1\"\nos.environ[\"RAY_TLS_SERVER_CERT\"] = os.path.join(\"./\", \"tls.crt\")\nos.environ[\"RAY_TLS_SERVER_KEY\"] = os.path.join(\"./\", \"tls.key\")\nos.environ[\"RAY_TLS_CA_CERT\"] = os.path.join(\"./\", \"ca.crt\")\nray.init(address=\"ray://localhost\", logging_level=\"DEBUG\")'\n</code></pre></p> <p>The output should be similar to: <pre><code>2023-04-25 16:33:32,452 INFO client_builder.py:253 -- Passing the following kwargs to ray.init() on the server: logging_level\n2023-04-25 16:33:32,460 DEBUG worker.py:378 -- client gRPC channel state change: ChannelConnectivity.IDLE\n2023-04-25 16:33:32,664 DEBUG worker.py:378 -- client gRPC channel state change: ChannelConnectivity.CONNECTING\n2023-04-25 16:33:32,671 DEBUG worker.py:378 -- client gRPC channel state change: ChannelConnectivity.READY\n</code></pre></p>"},{"location":"guidance/rayjob/","title":"RayJob","text":""},{"location":"guidance/rayjob/#ray-job-alpha","title":"Ray Job (alpha)","text":"<p>Note: This is the alpha version of Ray Job Support in KubeRay. There will be ongoing improvements for Ray Job in the future releases.</p>"},{"location":"guidance/rayjob/#prerequisites","title":"Prerequisites","text":"<ul> <li>Ray 1.10 or higher</li> <li>KubeRay v0.3.0+. (v0.5.0+ is recommended)</li> </ul>"},{"location":"guidance/rayjob/#what-is-a-rayjob","title":"What is a RayJob?","text":"<p>A RayJob manages 2 things: * Ray Cluster: Manages resources in a Kubernetes cluster. * Job: Manages jobs in a Ray Cluster.</p>"},{"location":"guidance/rayjob/#what-does-the-rayjob-provide","title":"What does the RayJob provide?","text":"<ul> <li>Kubernetes-native support for Ray clusters and Ray Jobs. You can use a Kubernetes config to define a Ray cluster and job, and use <code>kubectl</code> to create them. The cluster can be deleted automatically once the job is finished.</li> </ul>"},{"location":"guidance/rayjob/#deploy-kuberay","title":"Deploy KubeRay","text":"<p>Make sure your KubeRay operator version is at least v0.3.0. The latest released KubeRay version is recommended. For installation instructions, please follow the documentation.</p>"},{"location":"guidance/rayjob/#run-an-example-job","title":"Run an example Job","text":"<p>There is one example config file to deploy a RayJob included here: ray_v1alpha1_rayjob.yaml</p> <pre><code># Create a RayJob.\n$ kubectl apply -f config/samples/ray_v1alpha1_rayjob.yaml\n</code></pre> <pre><code># List running RayJobs.\n$ kubectl get rayjob\nNAME            AGE\nrayjob-sample   7s\n</code></pre> <pre><code># RayJob sample will also create a raycluster.\n# raycluster will create few resources including pods and services. You can use the following commands to check them:\n$ kubectl get rayclusters\n$ kubectl get pod\n</code></pre>"},{"location":"guidance/rayjob/#rayjob-configuration","title":"RayJob Configuration","text":"<ul> <li><code>entrypoint</code> - The shell command to run for this job. job_id.</li> <li><code>jobId</code> - (Optional) Job ID to specify for the job. If not provided, one will be generated.</li> <li><code>metadata</code> - Arbitrary user-provided metadata for the job.</li> <li><code>runtimeEnv</code> - base64 string of the runtime json string.</li> <li><code>shutdownAfterJobFinishes</code> - whether to recycle the cluster after job finishes.</li> <li><code>ttlSecondsAfterFinished</code> - TTL to clean up the cluster. This only works if <code>shutdownAfterJobFinishes</code> is set.</li> </ul>"},{"location":"guidance/rayjob/#rayjob-observability","title":"RayJob Observability","text":"<p>You can use <code>kubectl logs</code> to check the operator logs or the head/worker nodes logs. You can also use <code>kubectl describe rayjobs rayjob-sample</code> to check the states and event logs of your RayJob instance:</p> <pre><code>Status:\n  Dashboard URL:          rayjob-sample-raycluster-vnl8w-head-svc.ray-system.svc.cluster.local:8265\n  End Time:               2022-07-24T02:04:56Z\n  Job Deployment Status:  Complete\n  Job Id:                 test-hehe\n  Job Status:             SUCCEEDED\n  Message:                Job finished successfully.\n  Ray Cluster Name:       rayjob-sample-raycluster-vnl8w\n  Ray Cluster Status:\n    Available Worker Replicas:  1\n    Endpoints:\n      Client:          32572\n      Dashboard:       32276\n      Gcs - Server:    30679\n    Last Update Time:  2022-07-24T02:04:43Z\n    State:             ready\n  Start Time:          2022-07-24T02:04:49Z\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Created    90s   rayjob-controller  Created cluster rayjob-sample-raycluster-vnl8w\n  Normal  Submitted  82s   rayjob-controller  Submit Job test-hehe\n  Normal  Deleted    15s   rayjob-controller  Deleted cluster rayjob-sample-raycluster-vnl8w\n</code></pre> <p>If the job doesn't run successfully, the above <code>describe</code> command will provide information about that too: <pre><code>Status:\n  Dashboard URL:          rayjob-sample-raycluster-nrdm8-head-svc.ray-system.svc.cluster.local:8265\n  End Time:               2022-07-24T02:01:39Z\n  Job Deployment Status:  Complete\n  Job Id:                 test-hehe\n  Job Status:             FAILED\n  Message:                Job failed due to an application error, last available logs:\npython: can't open file '/tmp/code/script.ppy': [Errno 2] No such file or directory\n\n  Ray Cluster Name:  rayjob-sample-raycluster-nrdm8\n  Ray Cluster Status:\n    Available Worker Replicas:  1\n    Endpoints:\n      Client:          31852\n      Dashboard:       32606\n      Gcs - Server:    32436\n    Last Update Time:  2022-07-24T02:01:30Z\n    State:             ready\n  Start Time:          2022-07-24T02:01:38Z\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Created    2m9s  rayjob-controller  Created cluster rayjob-sample-raycluster-nrdm8\n  Normal  Submitted  2m    rayjob-controller  Submit Job test-hehe\n  Normal  Deleted    58s   rayjob-controller  Deleted cluster rayjob-sample-raycluster-nrdm8\n</code></pre></p>"},{"location":"guidance/rayjob/#delete-the-rayjob-instance","title":"Delete the RayJob instance","text":"<pre><code>$ kubectl delete -f config/samples/ray_v1alpha1_rayjob.yaml\n</code></pre>"},{"location":"guidance/rayservice/","title":"RayService","text":""},{"location":"guidance/rayservice/#ray-services-alpha","title":"Ray Services (alpha)","text":"<p>Note: This is the alpha version of Ray Services. There will be ongoing improvements for Ray Services in the future releases.</p>"},{"location":"guidance/rayservice/#prerequisites","title":"Prerequisites","text":"<ul> <li>Ray 2.0 or newer.</li> <li>KubeRay 0.6.0 or newer.</li> </ul>"},{"location":"guidance/rayservice/#what-is-a-rayservice","title":"What is a RayService?","text":"<p>A RayService manages 2 things:</p> <ul> <li>Ray Cluster: Manages resources in a Kubernetes cluster.</li> <li>Ray Serve Applications: Manages users' applications.</li> </ul>"},{"location":"guidance/rayservice/#what-does-the-rayservice-provide","title":"What does the RayService provide?","text":"<ul> <li>Kubernetes-native support for Ray clusters and Ray Serve applications: After using a Kubernetes config to define a Ray cluster and its Ray Serve applications, you can use <code>kubectl</code> to create the cluster and its applications.</li> <li>In-place update for Ray Serve applications: Users can update the Ray Serve config in the RayService CR config and use <code>kubectl apply</code> to update the applications.</li> <li>Zero downtime upgrade for Ray clusters: Users can update the Ray cluster config in the RayService CR config and use <code>kubectl apply</code> to update the cluster. RayService will temporarily create a pending cluster and wait for it to be ready, then switch traffic to the new cluster and terminate the old one.</li> <li>Services HA: RayService will monitor the Ray cluster and Serve deployments' health statuses. If RayService detects an unhealthy status for a period of time, RayService will try to create a new Ray cluster and switch traffic to the new cluster when it is ready.</li> </ul>"},{"location":"guidance/rayservice/#deploy-the-operator","title":"Deploy the Operator","text":"<p>Follow this document to install the nightly KubeRay operator via  Helm. Note that sample RayService in this guide uses <code>serveConfigV2</code> to specify a multi-application Serve config. This will be first supported in Kuberay 0.6.0, and is currently supported only on the nightly KubeRay operator.</p> <p>Check that the controller is running.</p> <pre><code>$ kubectl get deployments -n ray-system\nNAME           READY   UP-TO-DATE   AVAILABLE   AGE\nray-operator   1/1     1            1           40s\n\n$ kubectl get pods -n ray-system\nNAME                            READY   STATUS    RESTARTS   AGE\nray-operator-75dbbf8587-5lrvn   1/1     Running   0          31s\n</code></pre>"},{"location":"guidance/rayservice/#run-an-example-cluster","title":"Run an Example Cluster","text":"<p>In this guide we will be working with the sample RayService defined by ray_v1alpha1_rayservice.yaml.</p> <p>Let's first take a look at the Ray Serve config embedded in the RayService yaml. At a high level (without digging too deep into the details about the deployments in each application), there are two applications: a fruit stand app and a calculator app. The fruit stand application is imported from the <code>fruit.py</code> file in the test_dag repo, and it's hosted at the route prefix <code>/fruit</code>. Similarly, the calculator app is imported from the <code>conditional_dag.py</code> file in the same repo, and it's hosted at the route prefix <code>/calc</code>. <pre><code>serveConfigV2: |\napplications:\n- name: fruit_app\nimport_path: fruit.deployment_graph\nroute_prefix: /fruit\nruntime_env:\nworking_dir: \"https://github.com/ray-project/test_dag/archive/41d09119cbdf8450599f993f51318e9e27c59098.zip\"\ndeployments: ...\n- name: math_app\nimport_path: conditional_dag.serve_dag\nroute_prefix: /calc\nruntime_env:\nworking_dir: \"https://github.com/ray-project/test_dag/archive/41d09119cbdf8450599f993f51318e9e27c59098.zip\"\ndeployments: ...\n</code></pre></p> <p>To start a RayService and deploy these two applications, run the following command: <pre><code>$ kubectl apply -f config/samples/ray_v1alpha1_rayservice.yaml\n</code></pre></p> <p>List running RayServices to check on your new service <code>rayservice-sample</code>. <pre><code>$ kubectl get rayservice\nNAME                AGE\nrayservice-sample   7s\n</code></pre></p> <p>The created RayService should include a head pod, a worker pod, and four services. <pre><code>$ kubectl get pods\nNAME                                                      READY   STATUS    RESTARTS   AGE\nervice-sample-raycluster-qd2vl-worker-small-group-bxpp6   1/1     Running   0          24m\nrayservice-sample-raycluster-qd2vl-head-45hj4             1/1     Running   0          24m\n\n$ kubectl get services\nNAME                                               TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                          AGE\nkubernetes                                         ClusterIP   10.100.0.1       &lt;none&gt;        443/TCP                                          62d\n# A head node service maintained by the RayService.\nrayservice-sample-head-svc                         ClusterIP   10.100.34.24     &lt;none&gt;        6379/TCP,8265/TCP,10001/TCP,8000/TCP,52365/TCP   24m\n# A dashboard agent service maintained by the RayCluster.\nrayservice-sample-raycluster-qd2vl-dashboard-svc   ClusterIP   10.100.109.177   &lt;none&gt;        52365/TCP                                        24m\n# A head node service maintained by the RayCluster.\nrayservice-sample-raycluster-qd2vl-head-svc        ClusterIP   10.100.180.221   &lt;none&gt;        6379/TCP,8265/TCP,10001/TCP,8000/TCP,52365/TCP   24m\n# A serve service maintained by the RayService.\nrayservice-sample-serve-svc                        ClusterIP   10.100.39.92     &lt;none&gt;        8000/TCP                                         24m\n</code></pre></p> <p>Note: Default ports and their definitions. </p> Port Definition 6379 Ray GCS 8265 Ray Dashboard 10001 Ray Client 8000 Ray Serve 52365 Ray Dashboard Agent <p>Get information about the RayService using its name. <pre><code>$ kubectl describe rayservices rayservice-sample\n</code></pre></p>"},{"location":"guidance/rayservice/#access-user-services","title":"Access User Services","text":"<p>The users' traffic can go through the <code>serve</code> service (e.g. <code>rayservice-sample-serve-svc</code>).</p>"},{"location":"guidance/rayservice/#run-a-curl-pod","title":"Run a Curl Pod","text":"<pre><code>$ kubectl run curl --image=radial/busyboxplus:curl -i --tty\n</code></pre> <p>Or if you already have a curl pod running, you can login using <code>kubectl exec -it curl sh</code>.</p> <p>You can query the two applications in your service at their respective endpoints. Query the fruit stand app with the route prefix <code>/fruit</code>: <pre><code>[ root@curl:/ ]$ curl -X POST -H 'Content-Type: application/json' rayservice-sample-serve-svc.default.svc.cluster.local:8000/fruit/ -d '[\"MANGO\", 2]'\n&gt; 6\n</code></pre> Query the calculator app with the route prefix <code>/calc</code>: <pre><code>[ root@curl:/ ]$ curl -X POST -H 'Content-Type: application/json' rayservice-sample-serve-svc.default.svc.cluster.local:8000/calc/ -d '[\"MUL\", 3]'\n&gt; 15 pizzas please!\n</code></pre> You should get the responses <code>6</code> and <code>15 pizzas please!</code>.</p>"},{"location":"guidance/rayservice/#use-port-forwarding","title":"Use Port Forwarding","text":"<p>Set up Kubernetes port forwarding. <pre><code>$ kubectl port-forward service/rayservice-sample-serve-svc 8000\n</code></pre> For the fruit example deployment, you can try the following requests: <pre><code>[ root@curl:/ ]$ curl -X POST -H 'Content-Type: application/json' localhost:8000/fruit/ -d '[\"MANGO\", 2]'\n&gt; 6\n[ root@curl:/ ]$ curl -X POST -H 'Content-Type: application/json' localhost:8000/calc/ -d '[\"MUL\", 3]'\n&gt; 15 pizzas please!\n</code></pre></p> <p>Note: <code>serve-svc</code> is HA in general. It will do traffic routing among all the workers which have serve deployments and will always try to point to the healthy cluster, even during upgrading or failing cases.  You can set <code>serviceUnhealthySecondThreshold</code> to define the threshold of seconds that the serve deployments fail. You can also set <code>deploymentUnhealthySecondThreshold</code> to define the threshold of seconds that Ray fails to deploy any serve deployments.</p>"},{"location":"guidance/rayservice/#access-ray-dashboard","title":"Access Ray Dashboard","text":"<p>Set up Kubernetes port forwarding for the dashboard. <pre><code>$ kubectl port-forward service/rayservice-sample-head-svc 8265\n</code></pre> Access the dashboard using a web browser at <code>localhost:8265</code>.</p>"},{"location":"guidance/rayservice/#update-ray-serve-config","title":"Update Ray Serve config","text":"<p>You can update the applications in the <code>serveConfigV2</code> in your RayService config file. For example, update the price of mangos from <code>3</code> to <code>4</code> for the fruit app in ray_v1alpha1_rayservice.yaml: <pre><code>- name: MangoStand\n  num_replicas: 1\nuser_config:\n    price: 4\n</code></pre></p> <p>Use <code>kubectl apply</code> to update your RayService and <code>kubectl describe  rayservices rayservice-sample</code> to take a look at the RayService's information. It should look similar to: <pre><code>serveDeploymentStatuses:\n- healthLastUpdateTime: \"2022-07-18T21:51:37Z\"\nlastUpdateTime: \"2022-07-18T21:51:41Z\"\nname: MangoStand\n  status: UPDATING\n</code></pre></p> <p>After it finishes deployment, let's send a request again. In the curl pod from earlier, run: <pre><code>[ root@curl:/ ]$ curl -X POST -H 'Content-Type: application/json' rayservice-sample-serve-svc.default.svc.cluster.local:8000/fruit/ -d '[\"MANGO\", 2]'\n&gt; 8\n</code></pre> Or if using port forwarding: <pre><code>curl -X POST -H 'Content-Type: application/json' localhost:8000/fruit/ -d '[\"MANGO\", 2]'\n&gt; 8\n</code></pre> You should now get <code>8</code> as a result.</p>"},{"location":"guidance/rayservice/#upgrade-rayservice-raycluster-config","title":"Upgrade RayService RayCluster Config","text":"<p>You can update the <code>rayClusterConfig</code> in your RayService config file. For example, you can increase the number of workers to 2: <pre><code>workerGroupSpecs:\n  # the pod replicas in this group typed worker\n- replicas: 2\n</code></pre></p> <p>Use <code>kubectl apply</code> to update your RayService and <code>kubectl describe  rayservices rayservice-sample</code> to take a look at the RayService's information. It should look similar to: <pre><code>  pendingServiceStatus:\n    appStatus: {}\ndashboardStatus:\n      healthLastUpdateTime: \"2022-07-18T21:54:53Z\"\nlastUpdateTime: \"2022-07-18T21:54:54Z\"\nrayClusterName: rayservice-sample-raycluster-bshfr\n    rayClusterStatus: {}\n</code></pre> You can see the RayService is preparing a pending cluster. Once the pending cluster is healthy, the RayService will make it the active cluster and terminate the previous one.</p>"},{"location":"guidance/rayservice/#rayservice-observability","title":"RayService Observability","text":"<p>You can use <code>kubectl logs</code> to check the operator logs or the head/worker nodes logs. You can also use <code>kubectl describe rayservices rayservice-sample</code> to check the states and event logs of your RayService instance.</p> <p>For Ray Serve monitoring, you can refer to the Ray observability documentation. To run Ray state APIs, log in to the head pod by running <code>kubectl exec -it &lt;head-node-pod&gt; bash</code> and use the Ray CLI or you can run commands locally using <code>kubectl exec -it &lt;head-node-pod&gt; -- &lt;ray state api&gt;</code>.</p> <p>For example, <code>kubectl exec -it &lt;head-node-pod&gt; -- ray summary tasks</code> outputs the following: <pre><code>======== Tasks Summary: 2022-07-28 15:10:24.801670 ========\nStats:\n------------------------------------\ntotal_actor_scheduled: 17\ntotal_actor_tasks: 5\ntotal_tasks: 0\n\n\nTable (group by func_name):\n------------------------------------\n    FUNC_OR_CLASS_NAME                 STATE_COUNTS    TYPE\n0   ServeController.listen_for_change  RUNNING: 5      ACTOR_TASK\n1   ServeReplica:MangoStand.__init__   FINISHED: 3     ACTOR_CREATION_TASK\n2   HTTPProxyActor.__init__            FINISHED: 2     ACTOR_CREATION_TASK\n3   ServeReplica:PearStand.__init__    FINISHED: 3     ACTOR_CREATION_TASK\n4   ServeReplica:OrangeStand.__init__  FINISHED: 3     ACTOR_CREATION_TASK\n5   ServeReplica:FruitMarket.__init__  FINISHED: 3     ACTOR_CREATION_TASK\n6   ServeReplica:DAGDriver.__init__    FINISHED: 2     ACTOR_CREATION_TASK\n7   ServeController.__init__           FINISHED: 1     ACTOR_CREATION_TASK\n</code></pre></p>"},{"location":"guidance/rayservice/#delete-the-rayservice-instance","title":"Delete the RayService Instance","text":"<pre><code>$ kubectl delete -f config/samples/ray_v1alpha1_rayservice.yaml\n</code></pre>"},{"location":"guidance/rayservice/#delete-the-operator","title":"Delete the Operator","text":"<pre><code>$ kubectl delete -k \"github.com/ray-project/kuberay/ray-operator/config/default\"\n</code></pre>"},{"location":"guidance/stable-diffusion-rayservice/","title":"Serve a StableDiffusion text-to-image model using RayService","text":"<p>Note: The Python files for the Ray Serve application and its client are in the ray-project/serve_config_examples repo  and the Ray documentation.</p>"},{"location":"guidance/stable-diffusion-rayservice/#step-1-create-a-kubernetes-cluster-with-gpus","title":"Step 1: Create a Kubernetes cluster with GPUs","text":"<p>Follow aws-eks-gpu-cluster.md to create an AWS EKS cluster with 1 CPU (<code>m5.xlarge</code>) node and 1 GPU (<code>g5.xlarge</code>) node.</p>"},{"location":"guidance/stable-diffusion-rayservice/#step-2-install-the-nightly-kuberay-operator","title":"Step 2: Install the nightly KubeRay operator","text":"<p>Follow this document to install the nightly KubeRay operator via  Helm. We're installing the nightly release here since this example's YAML file uses <code>serveConfigV2</code>, which uses features that will be released in KubeRay v0.6.0.</p>"},{"location":"guidance/stable-diffusion-rayservice/#step-3-install-a-rayservice","title":"Step 3: Install a RayService","text":"<pre><code># path: ray-operator/config/samples/\nkubectl apply -f ray-service.stable-diffusion.yaml\n</code></pre> <p>This RayService configuration contains some important settings:</p> <ul> <li>Its <code>tolerations</code> for workers match the taints on the GPU node group. Without the tolerations, worker Pods won't be scheduled on GPU nodes.     <pre><code># Please add the following taints to the GPU node.\ntolerations:\n- key: \"ray.io/node-type\"\noperator: \"Equal\"\nvalue: \"worker\"\neffect: \"NoSchedule\"\n</code></pre></li> <li>It includes <code>diffusers</code> in <code>runtime_env</code> since this package is not included by default in the <code>ray-ml</code> image.</li> </ul>"},{"location":"guidance/stable-diffusion-rayservice/#step-4-forward-the-port-of-serve","title":"Step 4: Forward the port of Serve","text":"<pre><code>kubectl port-forward svc/stable-diffusion-serve-svc 8000\n</code></pre> <p>Note that the RayService's Kubernetes service will be created after the Serve applications are ready and running. This process may take approximately 1 minute after all Pods in the RayCluster are running.</p>"},{"location":"guidance/stable-diffusion-rayservice/#step-5-send-a-request-to-the-text-to-image-model","title":"Step 5: Send a request to the text-to-image model","text":"<pre><code># Step 5.1: Download `stable_diffusion_req.py` \ncurl -LO https://raw.githubusercontent.com/ray-project/serve_config_examples/master/stable_diffusion/stable_diffusion_req.py\n\n# Step 5.2: Set your `prompt` in `stable_diffusion_req.py`.\n\n# Step 5.3: Send a request to the Stable Diffusion model.\npython stable_diffusion_req.py\n# Check output.png\n</code></pre>"},{"location":"guidance/tls/","title":"TLS Authentication","text":"<p>Ray can be configured to use TLS on its gRPC channels. This means that connecting to the Ray head will require an appropriate set of credentials and also that data exchanged between various processes (client, head, workers) will be encrypted (Ray's document).</p> <p>This document provides detailed instructions for generating a public-private key pair and CA certificate for configuring KubeRay.</p> <p>Warning: Enabling TLS will cause a performance hit due to the extra overhead of mutual authentication and encryption. Testing has shown that this overhead is large for small workloads and becomes relatively smaller for large workloads. The exact overhead will depend on the nature of your workload.</p>"},{"location":"guidance/tls/#prerequisites","title":"Prerequisites","text":"<p>To fully understand this document, it's highly recommended that you have a solid understanding of the following concepts:</p> <ul> <li>private/public key</li> <li>CA (certificate authority)</li> <li>CSR (certificate signing request)</li> <li>self-signed certificate</li> </ul> <p>This YouTube video is a good start.</p>"},{"location":"guidance/tls/#tldr","title":"TL;DR","text":"<p>Please note that this document is designed to support KubeRay version 0.5.0 or later. If you are using an older version of KubeRay, some of the instructions or configurations may not apply or may require additional modifications.</p> <p>Warning: Please note that the <code>ray-cluster.tls.yaml</code> file is intended for demo purposes only. It is crucial that you do not store your CA private key in a Kubernetes Secret in your production environment.</p> <pre><code># Install v0.5.0 KubeRay operator\n# `ray-cluster.tls.yaml` will cover from Step 1 to Step 3 (path: kuberay/)\nkubectl apply -f ray-operator/config/samples/ray-cluster.tls.yaml\n\n# Jump to Step 4 \"Verify TLS authentication\" to verify the connection.\n</code></pre> <p><code>ray-cluster.tls.yaml</code> will create:</p> <ul> <li>A Kubernetes Secret containing the CA's private key (<code>ca.key</code>) and self-signed certificate (<code>ca.crt</code>) (Step 1)</li> <li>A Kubernetes ConfigMap containing the scripts <code>gencert_head.sh</code> and <code>gencert_worker.sh</code>, which allow Ray Pods to generate private keys (<code>tls.key</code>) and self-signed certificates (<code>tls.crt</code>) (Step 2)</li> <li>A RayCluster with proper TLS environment variables configurations (Step 3)</li> </ul> <p>The certificate (<code>tls.crt</code>) for a Ray Pod is encrypted using the CA's private key (<code>ca.key</code>). Additionally, all Ray Pods have the CA's public key included in <code>ca.crt</code>, which allows them to decrypt certificates from other Ray Pods.</p>"},{"location":"guidance/tls/#step-1-generate-a-private-key-and-self-signed-certificate-for-ca","title":"Step 1: Generate a private key and self-signed certificate for CA","text":"<p>In this document, a self-signed certificate is used, but users also have the option to choose a publicly trusted certificate authority (CA) for their TLS authentication.</p> <pre><code># Step 1-1: Generate a self-signed certificate and a new private key file for CA.\nopenssl req -x509 \\\n-sha256 -days 3650 \\\n-nodes \\\n-newkey rsa:2048 \\\n-subj \"/CN=*.kuberay.com/C=US/L=San Francisco\" \\\n-keyout ca.key -out ca.crt\n\n# Step 1-2: Check the CA's public key from the self-signed certificate.\nopenssl x509 -in ca.crt -noout -text\n\n# Step 1-3\n# Method 1: Use `cat $FILENAME | base64` to encode `ca.key` and `ca.crt`.\n#           Then, paste the encoding strings to the Kubernetes Secret in `ray-cluster.tls.yaml`.\n\n# Method 2: Use kubectl to encode the certifcate as Kubernetes Secret automatically.\n#           (Note: You should comment out the Kubernetes Secret in `ray-cluster.tls.yaml`.)\nkubectl create secret generic ca-tls --from-file=ca.key --from-file=ca.crt\n</code></pre> <ul> <li><code>ca.key</code>: CA's private key</li> <li><code>ca.crt</code>: CA's self-signed certificate</li> </ul> <p>This step is optional because the <code>ca.key</code> and <code>ca.crt</code> files have already been included in the Kubernetes Secret specified in ray-cluster.tls.yaml.</p>"},{"location":"guidance/tls/#step-2-create-separate-private-key-and-self-signed-certificate-for-ray-pods","title":"Step 2: Create separate private key and self-signed certificate for Ray Pods","text":"<p>In ray-cluster.tls.yaml, each Ray Pod (both head and workers) generates its own private key file (<code>tls.key</code>) and self-signed certificate file (<code>tls.crt</code>) in its init container. We generate separate files for each Pod because worker Pods do not have deterministic DNS names, and we cannot use the same certificate across different Pods.</p> <p>In the YAML file, you'll find a ConfigMap named <code>tls</code> that contains two shell scripts: <code>gencert_head.sh</code> and <code>gencert_worker.sh</code>. These scripts are used to generate the private key and self-signed certificate files (<code>tls.key</code> and <code>tls.crt</code>) for the Ray head and worker Pods. An alternative approach for users is to prebake the shell scripts directly into the docker image that's utilized by the init containers, rather than relying on a ConfigMap.</p> <p>Please find below a brief explanation of what happens in each of these scripts: 1. A 2048-bit RSA private key is generated and saved as <code>/etc/ray/tls/tls.key</code>. 2. A Certificate Signing Request (CSR) is generated using the private key file (<code>tls.key</code>) and the <code>csr.conf</code> configuration file. 3. A self-signed certificate (<code>tls.crt</code>) is generated using the private key of the Certificate Authority (<code>ca.key</code>) and the previously generated CSR.</p> <p>The only difference between <code>gencert_head.sh</code> and <code>gencert_worker.sh</code> is the <code>[ alt_names ]</code> section in <code>csr.conf</code> and <code>cert.conf</code>. The worker Pods use the fully qualified domain name (FQDN) of the head Kubernetes Service to establish a connection with the head Pod. Therefore, the <code>[alt_names]</code> section for the head Pod needs to include the FQDN of the head Kubernetes Service. By the way, the head Pod uses <code>$POD_IP</code> to communicate with worker Pods.</p> <pre><code># gencert_head.sh\n[alt_names]\nDNS.1 = localhost\nDNS.2 = $FQ_RAY_IP\nIP.1 = 127.0.0.1\nIP.2 = $POD_IP\n\n# gencert_worker.sh\n[alt_names]\nDNS.1 = localhost\nIP.1 = 127.0.0.1\nIP.2 = $POD_IP\n</code></pre> <p>In Kubernetes networking model, the IP that a Pod sees itself as is the same IP that others see it as. That's why Ray Pods can self-register for the certificates.</p>"},{"location":"guidance/tls/#step-3-configure-environment-variables-for-ray-tls-authentication","title":"Step 3: Configure environment variables for Ray TLS authentication","text":"<p>To enable TLS authentication in your Ray cluster, set the following environment variables:</p> <ul> <li><code>RAY_USE_TLS</code>: Either 1 or 0 to use/not-use TLS. If this is set to 1 then all of the environment variables below must be set. Default: 0.</li> <li><code>RAY_TLS_SERVER_CERT</code>: Location of a certificate file which is presented to other endpoints so as to achieve mutual authentication (i.e. <code>tls.crt</code>).</li> <li><code>RAY_TLS_SERVER_KEY</code>: Location of a private key file which is the cryptographic means to prove to other endpoints that you are the authorized user of a given certificate (i.e. <code>tls.key</code>).</li> <li><code>RAY_TLS_CA_CERT</code>: Location of a CA certificate file which allows TLS to decide whether an endpoint\u2019s certificate has been signed by the correct authority (i.e. <code>ca.crt</code>).</li> </ul> <p>For more information on how to configure Ray with TLS authentication, please refer to Ray's document.</p>"},{"location":"guidance/tls/#step-4-verify-tls-authentication","title":"Step 4: Verify TLS authentication","text":"<pre><code># Log in to the worker Pod\nkubectl exec -it ${WORKER_POD} -- bash\n\n# Since the head Pod has the certificate of $FQ_RAY_IP, the connection to the worker Pods\n# will be established successfully, and the exit code of the ray health-check command\n# should be 0.\nray health-check --address $FQ_RAY_IP:6379\necho $? # 0\n\n# Since the head Pod has the certificate of $RAY_IP, the connection will fail and an error\n# message similar to the following will be displayed: \"Peer name raycluster-tls-head-svc is\n# not in peer certificate\".\nray health-check --address $RAY_IP:6379\n\n# If you add `DNS.3 = $RAY_IP` to the [alt_names] section in `gencert_head.sh`,\n# the head Pod will generate the certificate of $RAY_IP.\n#\n# For KubeRay versions prior to 0.5.0, this step is necessary because Ray workers in earlier\n# versions use $RAY_IP to connect with Ray head.\n</code></pre>"},{"location":"guidance/tls/#step-5-connect-to-the-cluster-with-ray-client-using-tls-for-interactive-development","title":"Step 5: Connect to the cluster with Ray client using TLS for interactive development","text":"<p>To learn more, please check interactive development and TLS authentication for more detail.</p> <p>For instructions on connecting the Ray cluster from a Pod: <pre><code># Create a client pod and connect to cluster\nkubectl apply -f ray-operator/config/samples/ray-pod.tls.yaml\nkubectl logs ray-client-tls\n</code></pre> Verify the output similar to: <pre><code>{'CPU': 2.0, 'node:10.254.20.20': 1.0, 'object_store_memory': 771128524.0, 'memory': 3000000000.0, 'node:10.254.16.25': 1.0}\n</code></pre></p>"},{"location":"guidance/volcano-integration/","title":"KubeRay integration with Volcano","text":"<p>Volcano is a batch scheduling system built on Kubernetes. It provides a suite of mechanisms (gang scheduling, job queues, fair scheduling policies) currently missing from Kubernetes that are commonly required by many classes of batch and elastic workloads. KubeRay's Volcano integration enables more efficient scheduling of Ray pods in multi-tenant Kubernetes environments.</p> <p>Note that this is a new feature. Feedback and contributions welcome.</p>"},{"location":"guidance/volcano-integration/#setup","title":"Setup","text":""},{"location":"guidance/volcano-integration/#install-volcano","title":"Install Volcano","text":"<p>Volcano needs to be successfully installed in your Kubernetes cluster before enabling Volcano integration with KubeRay. Refer to the Quick Start Guide for Volcano installation instructions.</p>"},{"location":"guidance/volcano-integration/#install-kuberay-operator-with-batch-scheduling","title":"Install KubeRay Operator with Batch Scheduling","text":"<p>Deploy the KubeRay Operator with the <code>--enable-batch-scheduler</code> flag to enable Volcano batch scheduling support.</p> <p>When installing via Helm, you can set the following in your <code>values.yaml</code> file:</p> <pre><code>batchScheduler:\n    enabled: true\n</code></pre> <p>Or on the command line:</p> <pre><code># helm install kuberay-operator --set batchScheduler.enabled=true\n</code></pre>"},{"location":"guidance/volcano-integration/#run-ray-cluster-with-volcano-scheduler","title":"Run Ray Cluster with Volcano scheduler","text":"<p>Add the <code>ray.io/scheduler-name: volcano</code> label to your RayCluster CR to submit the cluster pods to Volcano for scheduling.</p> <p>Example:</p> <pre><code>apiVersion: ray.io/v1alpha1\nkind: RayCluster\nmetadata:\n  name: test-cluster\n  labels:\n    ray.io/scheduler-name: volcano\n    volcano.sh/queue-name: kuberay-test-queue\nspec:\n  rayVersion: '2.5.0'\n  headGroupSpec:\n    rayStartParams: {}\n    replicas: 1\n    template:\n      spec:\n        containers:\n        - name: ray-head\n          image: rayproject/ray:2.5.0\n          resources:\n            limits:\n              cpu: \"1\"\n              memory: \"2Gi\"\n            requests:\n              cpu: \"1\"\n              memory: \"2Gi\"\n  workerGroupSpecs: []\n</code></pre> <p>The following labels can also be provided in the RayCluster metadata:</p> <ul> <li><code>ray.io/priority-class-name</code>: the cluster priority class as defined by Kubernetes here.</li> <li><code>volcano.sh/queue-name</code>: the Volcano queue name the cluster will be submitted to.</li> </ul> <p>If autoscaling is enabled, <code>minReplicas</code> will be used for gang scheduling, otherwise the desired <code>replicas</code> will be used.</p>"},{"location":"guidance/volcano-integration/#example-gang-scheduling","title":"Example: Gang scheduling","text":"<p>In this example, we'll walk through how gang scheduling works with Volcano and KubeRay.</p> <p>First, let's create a queue with a capacity of 4 CPUs and 6Gi of RAM:</p> <pre><code>$ kubectl create -f - &lt;&lt;EOF\napiVersion: scheduling.volcano.sh/v1beta1\nkind: Queue\nmetadata:\n  name: kuberay-test-queue\nspec:\n  weight: 1\n  capability:\n    cpu: 4\n    memory: 6Gi\nEOF\n</code></pre> <p>The weight in the definition above indicates the relative weight of a queue in cluster resource division. This is useful in cases where the total capability of all the queues in your cluster exceeds the total available resources, forcing the queues to share among themselves. Queues with higher weight will be allocated a proportionally larger share of the total resources.</p> <p>The capability is a hard constraint on the maximum resources the queue will support at any given time. It can be updated as needed to allow more or fewer workloads to run at a time.</p> <p>Next we'll create a RayCluster with a head node (1 CPU + 2Gi of RAM) and two workers (1 CPU + 1Gi of RAM each), for a total of 3 CPU and 4Gi of RAM:</p> <pre><code>$ kubectl create -f - &lt;&lt;EOF\napiVersion: ray.io/v1alpha1\nkind: RayCluster\nmetadata:\n  name: test-cluster-0\n  labels:\n    ray.io/scheduler-name: volcano\n    volcano.sh/queue-name: kuberay-test-queue\nspec:\n  rayVersion: '2.5.0'\n  headGroupSpec:\n    rayStartParams: {}\n    replicas: 1\n    template:\n      spec:\n        containers:\n        - name: ray-head\n          image: rayproject/ray:2.5.0\n          resources:\n            limits:\n              cpu: \"1\"\n              memory: \"2Gi\"\n            requests:\n              cpu: \"1\"\n              memory: \"2Gi\"\n  workerGroupSpecs:\n    - groupName: worker\n      rayStartParams: {}\n      replicas: 2\n      minReplicas: 2\n      maxReplicas: 2\n      template:\n        spec:\n          containers:\n          - name: ray-head\n            image: rayproject/ray:2.5.0\n            resources:\n              limits:\n                cpu: \"1\"\n                memory: \"1Gi\"\n              requests:\n                cpu: \"1\"\n                memory: \"1Gi\"\nEOF\n</code></pre> <p>Because our queue has a capacity of 4 CPU and 6Gi of RAM, this resource should schedule successfully without any issues. We can verify this by checking the status of our cluster's Volcano PodGroup to see that the phase is <code>Running</code> and the last status is <code>Scheduled</code>:</p> <pre><code>$ kubectl get podgroup ray-test-cluster-0-pg -o yaml\n\napiVersion: scheduling.volcano.sh/v1beta1\nkind: PodGroup\nmetadata:\n  creationTimestamp: \"2022-12-01T04:43:30Z\"\n  generation: 2\n  name: ray-test-cluster-0-pg\n  namespace: test\n  ownerReferences:\n  - apiVersion: ray.io/v1alpha1\n    blockOwnerDeletion: true\n    controller: true\n    kind: RayCluster\n    name: test-cluster-0\n    uid: 7979b169-f0b0-42b7-8031-daef522d25cf\n  resourceVersion: \"4427347\"\n  uid: 78902d3d-b490-47eb-ba12-d6f8b721a579\nspec:\n  minMember: 3\n  minResources:\n    cpu: \"3\"\n    memory: 4Gi\n  queue: kuberay-test-queue\nstatus:\n  conditions:\n  - lastTransitionTime: \"2022-12-01T04:43:31Z\"\n    reason: tasks in gang are ready to be scheduled\n    status: \"True\"\n    transitionID: f89f3062-ebd7-486b-8763-18ccdba1d585\n    type: Scheduled\n  phase: Running\n</code></pre> <p>And checking the status of our queue to see that we have 1 running job:</p> <pre><code>$ kubectl get queue kuberay-test-queue -o yaml\n\napiVersion: scheduling.volcano.sh/v1beta1\nkind: Queue\nmetadata:\n  creationTimestamp: \"2022-12-01T04:43:21Z\"\n  generation: 1\n  name: kuberay-test-queue\n  resourceVersion: \"4427348\"\n  uid: a6c4f9df-d58c-4da8-8a58-e01c93eca45a\nspec:\n  capability:\n    cpu: 4\n    memory: 6Gi\n  reclaimable: true\n  weight: 1\nstatus:\n  reservation: {}\n  running: 1\n  state: Open\n</code></pre> <p>Next we'll add an additional RayCluster with the same configuration of head / worker nodes, but a different name:</p> <pre><code>$ kubectl create -f - &lt;&lt;EOF\napiVersion: ray.io/v1alpha1\nkind: RayCluster\nmetadata:\n  name: test-cluster-1\n  labels:\n    ray.io/scheduler-name: volcano\n    volcano.sh/queue-name: kuberay-test-queue\nspec:\n  rayVersion: '2.5.0'\n  headGroupSpec:\n    rayStartParams: {}\n    replicas: 1\n    template:\n      spec:\n        containers:\n        - name: ray-head\n          image: rayproject/ray:2.5.0\n          resources:\n            limits:\n              cpu: \"1\"\n              memory: \"2Gi\"\n            requests:\n              cpu: \"1\"\n              memory: \"2Gi\"\n  workerGroupSpecs:\n    - groupName: worker\n      rayStartParams: {}\n      replicas: 2\n      minReplicas: 2\n      maxReplicas: 2\n      template:\n        spec:\n          containers:\n          - name: ray-head\n            image: rayproject/ray:2.5.0\n            resources:\n              limits:\n                cpu: \"1\"\n                memory: \"1Gi\"\n              requests:\n                cpu: \"1\"\n                memory: \"1Gi\"\nEOF\n</code></pre> <p>Now check the status of its PodGroup to see that its phase is <code>Pending</code> and the last status is <code>Unschedulable</code>:</p> <pre><code>$ kubectl get podgroup ray-test-cluster-1-pg -o yaml\n\napiVersion: scheduling.volcano.sh/v1beta1\nkind: PodGroup\nmetadata:\n  creationTimestamp: \"2022-12-01T04:48:18Z\"\n  generation: 2\n  name: ray-test-cluster-1-pg\n  namespace: test\n  ownerReferences:\n  - apiVersion: ray.io/v1alpha1\n    blockOwnerDeletion: true\n    controller: true\n    kind: RayCluster\n    name: test-cluster-1\n    uid: b3cf83dc-ef3a-4bb1-9c42-7d2a39c53358\n  resourceVersion: \"4427976\"\n  uid: 9087dd08-8f48-4592-a62e-21e9345b0872\nspec:\n  minMember: 3\n  minResources:\n    cpu: \"3\"\n    memory: 4Gi\n  queue: kuberay-test-queue\nstatus:\n  conditions:\n  - lastTransitionTime: \"2022-12-01T04:48:19Z\"\n    message: '3/3 tasks in gang unschedulable: pod group is not ready, 3 Pending,\n      3 minAvailable; Pending: 3 Undetermined'\n    reason: NotEnoughResources\n    status: \"True\"\n    transitionID: 3956b64f-fc52-4779-831e-d379648eecfc\n    type: Unschedulable\n  phase: Pending\n</code></pre> <p>Because our new cluster requires more CPU and RAM than our queue will allow, even though we could fit one of the pods with the remaining 1 CPU and 2Gi of RAM, none of the cluster's pods will be placed until there is enough room for all the pods. Without using Volcano for gang scheduling in this way, one of the pods would ordinarily be placed, leading to the cluster being partially allocated, and some jobs (like Horovod training) getting stuck waiting for resources to become available.</p> <p>We can see the effect this has on scheduling the pods for our new RayCluster, which are listed as <code>Pending</code>:</p> <pre><code>$ kubectl get pods\n\nNAME                                            READY   STATUS         RESTARTS   AGE\ntest-cluster-0-worker-worker-ddfbz              1/1     Running        0          7m\ntest-cluster-0-head-vst5j                       1/1     Running        0          7m\ntest-cluster-0-worker-worker-57pc7              1/1     Running        0          6m59s\ntest-cluster-1-worker-worker-6tzf7              0/1     Pending        0          2m12s\ntest-cluster-1-head-6668q                       0/1     Pending        0          2m12s\ntest-cluster-1-worker-worker-n5g8k              0/1     Pending        0          2m12s\n</code></pre> <p>If we dig into the pod details, we'll see that this is indeed because Volcano cannot schedule the gang:</p> <pre><code>$ kubectl describe pod test-cluster-1-head-6668q | tail -n 3\n\n  Type     Reason            Age   From     Message\n  ----     ------            ----  ----     -------\n  Warning  FailedScheduling  4m5s  volcano  3/3 tasks in gang unschedulable: pod group is not ready, 3 Pending, 3 minAvailable; Pending: 3 Undetermined\n</code></pre> <p>Let's go ahead and delete the first RayCluster to clear up space in the queue:</p> <pre><code>$ kubectl delete raycluster test-cluster-0\n</code></pre> <p>The PodGroup for the second cluster has moved to the <code>Running</code> state, as there are now enough resources available to schedule the entire set of pods:</p> <pre><code>$ kubectl get podgroup ray-test-cluster-1-pg -o yaml\n\napiVersion: scheduling.volcano.sh/v1beta1\nkind: PodGroup\nmetadata:\n  creationTimestamp: \"2022-12-01T04:48:18Z\"\n  generation: 9\n  name: ray-test-cluster-1-pg\n  namespace: test\n  ownerReferences:\n  - apiVersion: ray.io/v1alpha1\n    blockOwnerDeletion: true\n    controller: true\n    kind: RayCluster\n    name: test-cluster-1\n    uid: b3cf83dc-ef3a-4bb1-9c42-7d2a39c53358\n  resourceVersion: \"4428864\"\n  uid: 9087dd08-8f48-4592-a62e-21e9345b0872\nspec:\n  minMember: 3\n  minResources:\n    cpu: \"3\"\n    memory: 4Gi\n  queue: kuberay-test-queue\nstatus:\n  conditions:\n  - lastTransitionTime: \"2022-12-01T04:54:04Z\"\n    message: '3/3 tasks in gang unschedulable: pod group is not ready, 3 Pending,\n      3 minAvailable; Pending: 3 Undetermined'\n    reason: NotEnoughResources\n    status: \"True\"\n    transitionID: db90bbf0-6845-441b-8992-d0e85f78db77\n    type: Unschedulable\n  - lastTransitionTime: \"2022-12-01T04:55:10Z\"\n    reason: tasks in gang are ready to be scheduled\n    status: \"True\"\n    transitionID: 72bbf1b3-d501-4528-a59d-479504f3eaf5\n    type: Scheduled\n  phase: Running\n  running: 2\n</code></pre> <p>Checking the pods again, we see that the second cluster is now up and running:</p> <pre><code>$ kubectl get pods\n\nNAME                                            READY   STATUS         RESTARTS   AGE\ntest-cluster-1-worker-worker-n5g8k              1/1     Running        0          9m4s\ntest-cluster-1-head-6668q                       1/1     Running        0          9m4s\ntest-cluster-1-worker-worker-6tzf7              1/1     Running        0          9m4s\n</code></pre> <p>Finally, we'll cleanup the remaining cluster and queue:</p> <pre><code>$ kubectl delete raycluster test-cluster-1\n$ kubectl delete queue kuberay-test-queue\n</code></pre>"},{"location":"guidance/volcano-integration/#questions","title":"Questions","text":"<p>Reach out to @tgaddair for questions regarding usage of this integration.</p>"},{"location":"release/changelog/","title":"Generate the changelog for a release","text":""},{"location":"release/changelog/#prerequisite","title":"Prerequisite","text":"<ol> <li> <p>Prepare your Github Token</p> </li> <li> <p>Install the Github python dependencies needed to generate the changelog.     <pre><code>pip install PyGithub\n</code></pre></p> </li> </ol>"},{"location":"release/changelog/#generate-release-notes","title":"Generate release notes","text":"<ol> <li> <p>Run the following command and fetch oneline git commits from the last release (v0.3.0) to current release (v0.4.0).</p> <pre><code>git log v0.3.0..v0.4.0 --oneline\n</code></pre> <p>You may need to run the following command first:</p> <pre><code>git fetch --tags\n</code></pre> </li> <li> <p>Copy the above commit history to <code>scripts/changelog-generator.py</code> and replace <code>&lt;your_github_token&gt;</code> with your Github token. Run the script to generate changelogs.</p> <pre><code>from github import Github\nimport re\n\n\nclass ChangelogGenerator:\n    def __init__(self, github_repo):\n        # Replace &lt;your_github_token&gt; with your Github Token\n        self._github = Github('&lt;your_github_token&gt;')\n        self._github_repo = self._github.get_repo(github_repo)\n\n    def generate(self, pr_id):\n        pr = self._github_repo.get_pull(pr_id)\n\n        return \"{title} ([#{pr_id}]({pr_link}), @{user})\".format(\n            title=pr.title,\n            pr_id=pr_id,\n            pr_link=pr.html_url,\n            user=pr.user.login\n        )\n\n\n# generated by `git log &lt;oldTag&gt;..&lt;newTag&gt; --oneline`\npayload = '''\n7374e2c [RayService] Skip update events without change (#811) (#825)\n7f83353 Switch to 0.4.0 and eliminate Chart app versions. (#810)\n86b0af2 Remove ingress.enabled from KubeRay operator chart (#812) (#816)\nc1cbaed Update chart versions for 0.4.0-rc.0 (#804)\n84a70f1 Update image tags. (#784)\nd760b9c [helm] Add memory limits and resource documentation. (#789) (#798)\n16905df [Feature] Improve the observability of integration tests (#775) (#796)\n83aab82 [CI] Pin go version in CRD consistency check (#794) (#797)\n....\n'''\n\ng = ChangelogGenerator(\"ray-project/kuberay\")\nfor pr_match in re.finditer(r\"#(\\d+)\", payload):\n    pr_id = int(pr_match.group(1))\n    print(\"* {}\".format(g.generate(pr_id)))\n</code></pre> </li> <li> <p>To create the release notes, save the output of the script. Modify the script's output as follows.</p> <ul> <li>Remove extraneous data, such as commits with tag information or links to other PRs, e.g. <pre><code>- c1cbaed (tag: v0.4.0-rc.0) Update chart versions for 0.4.0-rc.0 (#804) -&gt; c1cbaed Update chart versions for 0.4.0-rc.0 (#804)\n- 86b0af2 Remove ingress.enabled from KubeRay operator chart (#812) (#816) -&gt; 86b0af2 Remove ingress.enabled from KubeRay operator chart (#816)\n</code></pre></li> <li>Group commits by category e.g. <code>KubeRay Operator</code>, <code>Documentation</code>, etc. (The choice of categories is at the release manager's discretion.)</li> <li>Add a section summarizing important changes.</li> <li>Add a section listing individuals who contributed to the release.</li> </ul> </li> <li> <p>Cut the release from tags and add the release notes from the last step. For an example, see the v0.3.0 release notes.</p> </li> <li> <p>Send a PR to update CHANGELOG.md. The changelog should be updated by prepending the new release notes.</p> </li> </ol>"},{"location":"release/helm-chart/","title":"Helm charts release","text":"<p>We host all Helm charts on kuberay-helm. This document describes the process for release managers to release Helm charts.</p>"},{"location":"release/helm-chart/#the-end-to-end-workflow","title":"The end-to-end workflow","text":""},{"location":"release/helm-chart/#step-1-update-versions-in-chartyaml-and-valuesyaml-files","title":"Step 1: Update versions in Chart.yaml and values.yaml files","text":"<p>Please update the value of <code>version</code> in ray-cluster/Chart.yaml, kuberay-operator/Chart.yaml, and kuberay-apiserver/Chart.yaml to the new release version (e.g. 0.4.0).</p> <p>Also make sure <code>image.tag</code> has been updated in kuberay-operator/values.yaml and kuberay-apiserver/values.yaml.</p>"},{"location":"release/helm-chart/#step-2-copy-the-helm-chart-directory-from-kuberay-to-kuberay-helm","title":"Step 2: Copy the helm-chart directory from kuberay to kuberay-helm","text":"<p>In kuberay-helm CI, <code>helm/chart-releaser-action</code> will create releases for all charts in the directory <code>helm-chart</code> and update <code>index.yaml</code> in the gh-pages branch when the PR is merged into <code>main</code>. Note that <code>index.yaml</code> is necessary when you run the command <code>helm repo add</code>. I recommend removing the <code>helm-chart</code> directory in the kuberay-helm repository and creating a new one by copying from the kuberay repository.</p>"},{"location":"release/helm-chart/#step-3-validate-the-charts","title":"Step 3: Validate the charts","text":"<p>When the PR is merged into <code>main</code>, the releases and <code>index.yaml</code> will be generated. You can validate the charts as follows:</p> <ul> <li>Confirm that the releases are created as expected.</li> <li>Confirm that index.yaml exists.</li> <li>Confirm that index.yaml has the metadata of all releases, including old versions.</li> <li> <p>Check the creation/update time of all releases and <code>index.yaml</code> to ensure they are updated.</p> </li> <li> <p>Install charts from Helm repository.     <pre><code>helm repo add kuberay https://ray-project.github.io/kuberay-helm/\nhelm repo update\n\n# List all charts\nhelm search repo kuberay\n\n# Install charts\nhelm install kuberay-operator kuberay/kuberay-operator\nhelm install kuberay-apiserver kuberay/kuberay-apiserver\nhelm install ray-cluster kuberay/ray-cluster\n</code></pre></p> </li> </ul>"},{"location":"release/helm-chart/#delete-the-existing-releases","title":"Delete the existing releases","text":"<p><code>helm/chart-releaser-action</code> does not encourage users to delete existing releases; thus, <code>index.yaml</code> will not be updated automatically after the deletion. If you really need to do that, please read this section carefully before you do that.</p> <ul> <li>Delete the releases</li> <li> <p>Remove the related tags using the the following command. If tags are not properly removed, you may run into the problem described in ray-project/kuberay/#561.</p> <p><pre><code># git remote -v\n# upstream        git@github.com:ray-project/kuberay-helm.git (fetch)\n# upstream        git@github.com:ray-project/kuberay-helm.git (push)\n\n# The following command deletes the tag \"ray-cluster-0.4.0\".\ngit push --delete upstream ray-cluster-0.4.0\n</code></pre> * Remove <code>index.yaml</code> * Trigger kuberay-helm CI again to create new releases and a new index.yaml. * Follow \"Step 3: Validate the charts\" to test it.</p> </li> </ul>"}]}